{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn_classification_v19 = _v17 + Enveloped Dataset Curating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T01:46:25.081690Z",
     "start_time": "2019-06-22T01:46:25.078908Z"
    }
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:10.779497Z",
     "start_time": "2020-01-24T07:50:10.772681Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:12.126772Z",
     "start_time": "2020-01-24T07:50:10.780869Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.14.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # !!! \"=-1\" Forces system to not use GPU (This can allow for simultaneous training via GPU and testing via CPU)\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    " \n",
    "import tensorflow as tf\n",
    "# Logging Verbosity\n",
    "#   Level | Level for Humans | Level Description                  \n",
    "#  -------|------------------|------------------------------------ \n",
    "#   0     | DEBUG            | [Default] Print all messages       \n",
    "#   1     | INFO             | Filter out INFO messages           \n",
    "#   2     | WARNING          | Filter out INFO & WARNING messages \n",
    "#   3     | ERROR            | Filter out all messages      \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.autograph.set_verbosity(3)\n",
    "tf.get_logger().setLevel('ERROR') # Filter out INFO messages\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK, STATUS_FAIL\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import channels\n",
    "\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from time import sleep\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "# import pixiedust # Visual Debugger\n",
    "\n",
    "import data_utils_wlan_v03 as data_utils\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "    \n",
    "pickle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:12.133030Z",
     "start_time": "2020-01-24T07:50:12.127842Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FOLDER_PATH = '/mnt/2ndSSD/802-11_datasets/wlan_enveloped'\n",
    "DATA_PATH = FOLDER_PATH + '/data'\n",
    "PROJECT_NAME = 'nn_classification'\n",
    "VERSION = '_v19'\n",
    "\n",
    "from datetime import datetime\n",
    "NOW = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "ROOT_LOG_DIR = FOLDER_PATH + \"/logs_\" + PROJECT_NAME + VERSION\n",
    "if not os.path.exists(ROOT_LOG_DIR):\n",
    "    os.mkdir(ROOT_LOG_DIR)\n",
    "\n",
    "# Tensorboard folders\n",
    "LOGS_DIR = \"{}/run-{}\".format(ROOT_LOG_DIR,NOW)\n",
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.mkdir(LOGS_DIR)\n",
    "\n",
    "if not os.path.exists(LOGS_DIR + '/train'):\n",
    "    os.mkdir(LOGS_DIR + '/train')\n",
    "    \n",
    "if not os.path.exists(LOGS_DIR + '/valid'):\n",
    "    os.mkdir(LOGS_DIR + '/valid')\n",
    "\n",
    "# Checkpoints folder\n",
    "CHECKPOINTS_DIR = FOLDER_PATH + \"/checkpoints_\" + PROJECT_NAME + VERSION\n",
    "if not os.path.exists(CHECKPOINTS_DIR):\n",
    "    os.mkdir(CHECKPOINTS_DIR)\n",
    "    \n",
    "MODEL_FINAL_WEIGHTS_SAVE_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION + '_final_weights.h5'\n",
    "MODEL_FINAL_SAVE_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION + '_final_model.h5'\n",
    "MODEL_TFLITE_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION +'.tflite'\n",
    "MODEL_TFLITE_QUANT_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION +'quantized.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Dataset Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:12.142846Z",
     "start_time": "2020-01-24T07:50:12.133998Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fnmatch import fnmatch\n",
    "import scipy.io as sio\n",
    "\n",
    "def calculate_dataset_limits():\n",
    "# Determining Maximum n_mat_samples_per_packet and Minimum n_mat_packets\n",
    "#    so that the data tensor is large enough to hold even the longest signal\n",
    "#    but no larger in height than the shortest packet count \n",
    "\n",
    "    n_packets = 2**16\n",
    "    n_adc_samples = 0  \n",
    "    padding_max = 0\n",
    "    for filename in tqdm_notebook(sorted(listdir(DATA_PATH)),desc='Find Max #\\\n",
    "        ADC Samples and Min # Packet Samples Loop', leave=False):\n",
    "        filepath = join(DATA_PATH, filename)\n",
    "        if fnmatch(filepath, '*.mat'):\n",
    "            dataDict = sio.loadmat(filepath, squeeze_me = True, struct_as_record=False)\n",
    "            dataStruct = dataDict['Dataset']\n",
    "            sig_list_ = np.transpose(dataStruct.signal)\n",
    "            n_adc_samples_per_packet = np.size(sig_list_,1)\n",
    "            padding = dataStruct.padding\n",
    "            if n_adc_samples_per_packet > n_adc_samples:\n",
    "                n_adc_samples = n_adc_samples_per_packet\n",
    "            n_packet_samples = np.size(sig_list_,0)\n",
    "            if n_packet_samples < n_packets and n_packet_samples != 6:\n",
    "                n_packets = n_packet_samples\n",
    "            if padding_max < padding:\n",
    "                padding_max = padding\n",
    "                print('Signal Padding: {}'.format(padding))\n",
    "    return n_packets, n_adc_samples, padding_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:24.969270Z",
     "start_time": "2020-01-24T07:50:12.143826Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Find Max #        ADC Samples and Min # Packet Samples Loâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal Padding: 60\n",
      "User Settings:\n",
      "  Maximum # of ADC Samples in Dataset = 7952\n",
      "  Minimum # of Packets per Packet Type = 1876\n",
      "  # of Packets per Block = 625\n",
      "  Classification: protocolAndPacket| # of Classifications: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_matlab_protocols = 3 # 2 = g, n ; 3 = b, g, n\n",
    "n_matlab_data_packet_types = 3\n",
    "n_matlab_ctrl_packet_types = 6\n",
    "n_matlab_packet_types = n_matlab_data_packet_types+n_matlab_ctrl_packet_types\n",
    "n_matlab_center_freqs = 1\n",
    "n_matlab_snrs = 1\n",
    "n_matlab_downsamples = 3\n",
    "n_matlab_mcss = 8\n",
    "n_matlab_hts = 2\n",
    "\n",
    "n_matlab_packets_min, n_matlab_adc_samples_per_packet_max, padding_max = calculate_dataset_limits()\n",
    "\n",
    "# n_matlab_adc_samples_per_packet_max = 8032 # !!! Hard-coded for speed\n",
    "# n_matlab_packets_min = 1998 #2000 # !!! Hard-coded for speed\n",
    "\n",
    "N_PROTOCOLS = n_matlab_protocols \n",
    "RF_SIGNAL_LIST =  ['x', 'b', 'g', 'n'] # ['x', 'g', 'n']  # !!! Hard-coded for speeding up Debugging\n",
    "N_RF_SIGNALS = len(RF_SIGNAL_LIST)\n",
    "\n",
    "N_SNRS = n_matlab_snrs\n",
    "PADDING = padding_max\n",
    "\n",
    "N_DATA_PACKET_TYPES = n_matlab_data_packet_types\n",
    "N_CTRL_PACKET_TYPES = n_matlab_ctrl_packet_types\n",
    "N_PACKET_TYPES = N_DATA_PACKET_TYPES + N_CTRL_PACKET_TYPES\n",
    "PACKET_TYPE_LIST = range(N_PACKET_TYPES)\n",
    "N_PACKETS_PER_PACKET_TYPE = n_matlab_packets_min # 2000\n",
    "\n",
    "N_DATASET_BLOCKS = 3\n",
    "N_PACKETS_PER_BLOCK = int(N_PACKETS_PER_PACKET_TYPE/N_DATASET_BLOCKS)\n",
    "\n",
    "N_ADC_SAMPLES = n_matlab_adc_samples_per_packet_max\n",
    "\n",
    "N_CENTER_FREQS = n_matlab_center_freqs\n",
    "N_SNRS = n_matlab_snrs\n",
    "N_DOWNSAMPLES = n_matlab_downsamples\n",
    "N_MCSS = n_matlab_mcss\n",
    "N_HTS = n_matlab_hts\n",
    "\n",
    "print(\"User Settings:\")\n",
    "print(\"  Maximum # of ADC Samples in Dataset = {}\".format(N_ADC_SAMPLES))\n",
    "print(\"  Minimum # of Packets per Packet Type = {}\".format(N_PACKETS_PER_PACKET_TYPE))\n",
    "print(\"  # of Packets per Block = {}\".format(N_PACKETS_PER_BLOCK))\n",
    "\n",
    "# Classification Type:\n",
    "CLASSIFICATION_TYPE = 'protocolAndPacket' # 'protocol' #'address' #'protocolAndPacket'\n",
    "\n",
    "if CLASSIFICATION_TYPE == 'protocol':\n",
    "    N_CLASSIFICATIONS = N_RF_SIGNALS # Protocol Classification\n",
    "elif CLASSIFICATION_TYPE == 'packet':\n",
    "    N_CLASSIFICATIONS = N_RF_SIGNALS # Packet Type Classification\n",
    "elif CLASSIFICATION_TYPE == 'protocolAndPacket':\n",
    "    N_CLASSIFICATIONS = N_PROTOCOLS*N_PACKET_TYPES + 1 # Address Classification\n",
    "elif CLASSIFICATION_TYPE == 'address':\n",
    "    N_CLASSIFICATIONS = 99 # Address Classification\n",
    "    \n",
    "print('  Classification: {}| # of Classifications: {}\\n'.format(CLASSIFICATION_TYPE, N_CLASSIFICATIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Plot Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:24.975370Z",
     "start_time": "2020-01-24T07:50:24.970113Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def plot_envelope(data, class_idx=2**8-1, data_rate=0, data_len=2048,\\\n",
    "                  signal_name='NA', descriptor = '',\\\n",
    "                  classification_type ='address', y_axis_label='v',\\\n",
    "                  y_max=2.0, x_max=7832):    \n",
    "\n",
    "    if signal_name == 'NA':                   \n",
    "        if classification_type == 'protocol':\n",
    "            signal_name = data_utils.convert_idx_to_rf_signal_name(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'packet':\n",
    "            signal_name = data_utils.convert_idx_to_packet_name(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'protocolAndPacket':\n",
    "            signal_name = data_utils.convert_idx_to_protocol_and_packet_name(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'address':\n",
    "            signal_name = data_utils.convert_idx_to_address(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        else:\n",
    "           raise Exception('[{}] is not a valid classification_type'\\\n",
    "                           .format(classification_type))\n",
    "    \n",
    "    if y_axis_label == 'v':\n",
    "        y_label = 'Amplitude (V)'\n",
    "    elif y_axis_label == 'adc':\n",
    "        y_label = 'Amplitude (ADC Counts)'\n",
    "    else:\n",
    "       raise Exception('[{}] is not a valid Y Label'.format(y_axis_label))\n",
    "    \n",
    "    \n",
    "    plt.plot(data)\n",
    "    plt.title('{} - {}\\nClass: {}| MCS: {}| Length: {} adc_samples'.\\\n",
    "              format(descriptor, signal_name, class_idx, data_rate, int(data_len)))\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.ylim((-0.01*y_max, y_max*1.01))\n",
    "    plt.xlim((-0.01*x_max, x_max*1.01))\n",
    "    #     plt.xlim((-0.01*x_max, x_max*.01))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *.mat to *.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T21:17:14.392603Z",
     "start_time": "2019-11-25T21:07:02.010138Z"
    },
    "cell_style": "center",
    "code_folding": [
     156
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: This is compatible with data produced after 2019-10-25\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fnmatch import fnmatch\n",
    "import scipy.io as sio\n",
    "import heapq\n",
    "\n",
    "# N_PACKETS_PER_PACKET_TYPE, N_ADC_SAMPLES, PADDING_MAX = calculate_dataset_limits()\n",
    "N_PACKETS_PER_BLOCK = int(N_PACKETS_PER_PACKET_TYPE/N_DATASET_BLOCKS)\n",
    "print(\"Maximum # of ADC Samples in Dataset = {}\".format(N_ADC_SAMPLES))\n",
    "print(\"Minimum # of Packets per Packet Type = {}\".format(N_PACKETS_PER_PACKET_TYPE))\n",
    "print(\"# of Packets per Block = {}\".format(N_PACKETS_PER_BLOCK))\n",
    "\n",
    "sig_avg_max_list = []\n",
    "\n",
    "for block in tnrange(N_DATASET_BLOCKS):\n",
    "    for downsample_idx in tnrange(N_DOWNSAMPLES, desc='Downsample Loop', leave=False):\n",
    "        # Looping through each of the downsample ratios (1x, 2x, 4x)\n",
    "        n_downsamples_per_pickle = 1\n",
    "        data = (np.ones([N_PROTOCOLS, N_PACKET_TYPES, N_CENTER_FREQS,\\\n",
    "                         n_downsamples_per_pickle, N_SNRS, N_MCSS,\\\n",
    "                         N_PACKETS_PER_BLOCK, N_ADC_SAMPLES],\\\n",
    "                         dtype=np.uint16)*int(2**16-1)).astype(np.int16)\n",
    "        print(\"Data Tensor Shape = {}\".format(data.shape))\n",
    "        data_features = (np.ones([N_PROTOCOLS, N_PACKET_TYPES,\\\n",
    "                                  N_CENTER_FREQS, n_downsamples_per_pickle,\\\n",
    "                                  N_SNRS, N_MCSS, 11], dtype=np.uint16)*\\\n",
    "                                  int(2**16-1)).astype(np.uint16)\n",
    "             # [center_freq_, snr_, downsample_ratio_, mcs_, data_rate_, ht_, n_samples_per_packet_temp]\n",
    "\n",
    "        for filename in tqdm_notebook(sorted(listdir(DATA_PATH)),desc='File Open Loop', leave=False):\n",
    "            filepath = join(DATA_PATH, filename)\n",
    "            if fnmatch(filepath, '*.mat'):        \n",
    "                # Opening the *.mat\n",
    "                dataDict = sio.loadmat(filepath, squeeze_me = True, struct_as_record=False)\n",
    "                dataStruct = dataDict['Dataset']        \n",
    "\n",
    "                sig_list = np.transpose(dataStruct.signal)\n",
    "                if np.size(sig_list,0) < N_PACKETS_PER_BLOCK:\n",
    "                    n_packets = np.size(sig_list,0)\n",
    "                else:\n",
    "                    n_packets = N_PACKETS_PER_BLOCK # np.size(sig_list,0) # dataStruct.num_packets\n",
    "                    \n",
    "                packet_type_list = dataStruct.packetType\n",
    "                snr_list = dataStruct.snr\n",
    "                n_samples_list = dataStruct.numSamples\n",
    "                downsample_ratio_list = dataStruct.downsampleRate\n",
    "                packet_n_list = dataStruct.packetNo\n",
    "                std_802p11_list = np.tile(dataStruct.standard, N_PACKETS_PER_PACKET_TYPE)\n",
    "                center_freq_list = np.tile(dataStruct.center_freq, N_PACKETS_PER_PACKET_TYPE)\n",
    "                mcs_list = np.tile((dataStruct.mcs).split('S')[1], N_PACKETS_PER_PACKET_TYPE)\n",
    "                mcs_list2 = np.tile(filename.split('_')[9][-1], N_PACKETS_PER_PACKET_TYPE) # mcs_list = dataStruct.mcs\n",
    "                if mcs_list[0] != mcs_list2[0]:\n",
    "                    set_trace()\n",
    "                \n",
    "                ht_list = np.tile(dataStruct.ht, N_PACKETS_PER_PACKET_TYPE)\n",
    "                data_rate_list = np.tile(dataStruct.dataRate, N_PACKETS_PER_PACKET_TYPE)\n",
    "                padding_list = np.tile(dataStruct.padding, N_PACKETS_PER_PACKET_TYPE)\n",
    "                if dataStruct.padding != 60:\n",
    "                    print(filename)\n",
    "                    print('Padding: {}'.format(dataStruct.padding))\n",
    "                    set_trace()\n",
    "                address_list = np.tile(2**8-1, N_PACKETS_PER_PACKET_TYPE) #!!! Replace eventually\n",
    "                if data_utils.convert_downsample_to_idx(\\\n",
    "                                     downsample_ratio_list[0]) != downsample_idx:\n",
    "                    continue # !!! Note: this is inefficient but the data set isn't so large that it\n",
    "                             #       matters at the moment\n",
    "\n",
    "                print(\"\\n\" + filename)\n",
    "\n",
    "                # Scaling and quantizing the signal\n",
    "                n_largest = 440\n",
    "#                 for sig_idx in range(np.size(sig_list,0)):\n",
    "#                     # Determining the Avg max Vpeak\n",
    "#                     avg_max = np.mean(heapq.nlargest(n_largest, sig_list[sig_idx,:]))\n",
    "#                     sig_avg_max_list.append(avg_max)\n",
    "                scaling_target_v = 1.0\n",
    "                adc_resolution = 14\n",
    "                adc_v_ref = 2.0\n",
    "                env_sig = data_utils.scale_and_quantize_sig(sig_list, n_largest,\\\n",
    "                    scaling_target_v, adc_resolution, adc_v_ref)\n",
    "\n",
    "                # Parsing out Packet Type\n",
    "                packet_type_of_4 = filename.split('_')[12]            \n",
    "                for packet_per_block_idx in tnrange(n_packets, desc='Packet Sample Loop'):\n",
    "                    if packet_type_of_4 == 'Control':\n",
    "                        packet_idx = packet_per_block_idx\n",
    "                    else:\n",
    "                        packet_idx_offset = int(block*n_packets)\n",
    "                        packet_idx = packet_idx_offset + packet_per_block_idx\n",
    "                        \n",
    "                    packet_type_idx = data_utils.convert_packet_type_to_idx(\\\n",
    "                        packet_type_list[packet_idx])\n",
    "                    packet_length = n_samples_list[packet_idx]\\\n",
    "                        + 2*padding_list[packet_idx]\n",
    "                    std_idx = data_utils.convert_std_to_idx(\\\n",
    "                        std_802p11_list[packet_idx])-1 # Not pickling Noise\n",
    "                    center_freq_idx = data_utils.convert_center_freq_to_idx(\\\n",
    "                        center_freq_list[packet_idx])\n",
    "                    ds_ratio_idx = 0\n",
    "                        #data_utils.convert_downsample_to_idx(downsample_ratio_list[packet_idx])\n",
    "                    snr_idx = data_utils.convert_snr_to_idx(\\\n",
    "                        snr_list[packet_idx])\n",
    "                    mcs_idx = data_utils.convert_mcs_and_data_rate_to_idx(\\\n",
    "                        mcs_list[packet_idx], data_rate_list[packet_idx])\n",
    "\n",
    "                    if packet_type_of_4 == 'Control':                        \n",
    "                        # If packet is of Control type, then np.tile the results\\\n",
    "                        #   to fill the full the N_PACKETS_PER_BLOCK \n",
    "                        data[std_idx, packet_type_idx, center_freq_idx, ds_ratio_idx,\\\n",
    "                             snr_idx, mcs_idx, :, 0:packet_length] =\\\n",
    "                            np.tile(env_sig[packet_idx, 0:packet_length],\\\n",
    "                                    (N_PACKETS_PER_BLOCK,1))\n",
    "                        if std_802p11_list[packet_idx] == 'b':\n",
    "                            # There are only 4 802.11b data_rates, as compared to\\\n",
    "                            #   8 MCS Levels, so we duplicate the 802.11b packets\n",
    "                            data[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                                 ds_ratio_idx, snr_idx, mcs_idx+4, :, :] =\\\n",
    "                                data[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                                     ds_ratio_idx, snr_idx, mcs_idx, :, :]\n",
    "                    else:\n",
    "                        # There are enough packets recorded to fill the n_packets\n",
    "                        data[std_idx, packet_type_idx, center_freq_idx, ds_ratio_idx,\\\n",
    "                             snr_idx, mcs_idx,0:n_packets, 0:packet_length] =\\\n",
    "                                env_sig[packet_idx_offset:packet_idx_offset+n_packets, 0:packet_length]\n",
    "                        if std_802p11_list[packet_idx] == 'b':\n",
    "                            # There are only 4 802.11b data_rates, as compared to\\\n",
    "                            #   8 MCS Levels, so we duplicate the 802.11b packets\n",
    "                            data[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                                 ds_ratio_idx, snr_idx, mcs_idx+4, :, :] =\\\n",
    "                                data[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                                     ds_ratio_idx, snr_idx, mcs_idx, :, :]\n",
    "                    std_0 = std_802p11_list[packet_idx]\n",
    "                    std_idx_including_noise = data_utils.convert_std_to_idx(std_0)\n",
    "                    packet_type_0 = packet_type_list[packet_idx]\n",
    "                    std_and_packet_type_0 = data_utils.\\\n",
    "                        convert_std_and_packet_type_to_idx(RF_SIGNAL_LIST,\\\n",
    "                           std_idx_including_noise, packet_type_0)\n",
    "                    center_freq_0 = center_freq_list[packet_idx]\n",
    "                    snr_0 = snr_list[packet_idx]\n",
    "                    ds_ratio_0 = downsample_ratio_list[packet_idx]\n",
    "                    data_rate_0 = data_rate_list[packet_idx]      \n",
    "                    mcs_0 = data_utils.convert_mcs_and_data_rate_to_idx(\\\n",
    "                           mcs_list[packet_idx],data_rate_0) \n",
    "                    ht_0 = ht_list[packet_idx]\n",
    "                    address_0 = address_list[packet_idx]\n",
    "\n",
    "                    data_features[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                                  ds_ratio_idx, snr_idx, mcs_idx,:] =\\\n",
    "                        [std_idx_including_noise, packet_type_idx, std_and_packet_type_0,\\\n",
    "                         center_freq_0, snr_0, ds_ratio_0, mcs_0, data_rate_0, ht_0,\\\n",
    "                         address_0, n_samples_list[packet_idx]]\n",
    "                    if std_0 == 'b':\n",
    "                        # There are only 4 802.11b data_rates, as compared to\\\n",
    "                            #   8 MCS Levels, so we duplicate the 802.11b packets\n",
    "                        data_features[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                                  ds_ratio_idx, snr_idx, mcs_idx+4,:] =\\\n",
    "                        [std_idx_including_noise, packet_type_idx, std_and_packet_type_0,\\\n",
    "                         center_freq_0, snr_0, ds_ratio_0, mcs_0, data_rate_0, ht_0,\\\n",
    "                         address_0, n_samples_list[packet_idx]]\n",
    "\n",
    "        # Pickling each downsample_ratio data tensor\n",
    "\n",
    "        if pickle is True:\n",
    "            pickle_data_filename = '/' + 'wlan_enveloped_data_downsample_ratio_'+\\\n",
    "                str(data_utils.convert_downsample_idx_to_n(downsample_idx)) +\\\n",
    "                '_block_'+ str(block)+'.p'\n",
    "            pickle_data_features_filename = '/' + 'wlan_enveloped_features_downsample_ratio_'+\\\n",
    "                str(data_utils.convert_downsample_idx_to_n(downsample_idx)) +\\\n",
    "                '_block_'+ str(block)+ '.p'\n",
    "            data_utils.pickle_tensor(data, DATA_PATH + pickle_data_filename)\n",
    "            data_utils.pickle_tensor(data_features, DATA_PATH + pickle_data_features_filename)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Preprocess\n",
    "Data = [Protocol Type]x[PacketType]x[Center Freq]x[Downsample Ratio]x[SNR]x[MCS]x[# of Packets Per Type]x[Eveloped Signal]\n",
    "\n",
    "Feature List = [Protocol Type]x[PacketType]x[Protocol Type, PacketType, Protocol + Packet Type, Center Freq, SNR, Downsample Ratio, MCS, Data Rate, HT, Address, # of ADC Samples]\n",
    "\n",
    "| Protocol Type |  Label  |\n",
    "|------------|---------|\n",
    "|   x   |    0    |\n",
    "|   b   |    1    |\n",
    "|   g   |    2    |\n",
    "|   n   |    3    |\n",
    "\n",
    "| PacketType |  Label  |\n",
    "|------------|---------|\n",
    "|   Beacon   |    0    |\n",
    "|   Data     |    1    |\n",
    "|  QoS Data  |    2    |\n",
    "|    ACK     |    3    |\n",
    "|    RTS     |    4    |\n",
    "|    CTS     |    5    |\n",
    "|    Null    |    6    |\n",
    "|  QoS Null  |    7    |\n",
    "|  Blk ACK   |    8    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *.p to Enveloped data_tensor[ ][ ][ ][ ][ ][ ][ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:24.993905Z",
     "start_time": "2020-01-24T07:50:24.976245Z"
    },
    "code_folding": [
     98
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fnmatch import fnmatch\n",
    "# del data, data_features\n",
    "\n",
    "# Adding Noise to the full tensor\n",
    "def create_tensors(offset, rf_signal_list, n_packet_types,\\\n",
    "                   n_center_freq, n_downsamples, n_snrs, n_mcss,\\\n",
    "                   n_packets_per_packet_type, n_adc_samples, ds_ratio,\\\n",
    "                   block, classification_type, verbose_load, print_each_plot):\n",
    "    n_rf_signal = np.size(rf_signal_list)\n",
    "    \n",
    "    x_tensor_size = (n_rf_signal, n_packet_types, n_center_freq,\\\n",
    "                     n_downsamples, n_snrs, n_mcss,\\\n",
    "                     n_packets_per_packet_type, n_adc_samples)\n",
    "    x_tensor = np.zeros(x_tensor_size, dtype=np.uint16)\n",
    "    noise_tensor_size = (1, 1, n_center_freq,\\\n",
    "             n_downsamples, n_snrs, n_mcss,\\\n",
    "             n_packets_per_packet_type, n_adc_samples)\n",
    "    mu, std_dev = 0.0, 0.05\n",
    "    noise_scaling = 2.0\n",
    "    for packet_types_idx in tnrange(n_packet_types):\n",
    "        noise_tensor = np.abs(np.random.normal(mu, std_dev, size =\\\n",
    "            noise_tensor_size)*noise_scaling) \n",
    "        # Creating a tensor with random.normal len(N_ADC_SAMPLES)\n",
    "        noise_tensor_quantized = data_utils.quantize_sig(noise_tensor,\\\n",
    "                                             adc_resolution=14, adc_v_ref=2.0)\n",
    "        del noise_tensor    \n",
    "        x_tensor[0,packet_types_idx,:,:,:,:,:,:] = noise_tensor_quantized\n",
    "        del noise_tensor_quantized\n",
    "    \n",
    "    features_tensor = np.zeros((N_PROTOCOLS, n_packet_types, n_center_freq,\\\n",
    "                     n_downsamples, n_snrs, n_mcss, 11))\n",
    "        #[std, packet_type, std_and_packet_type, center_freq, snr, \n",
    "        #  downsample_ratio, mcs, data_rate, ht, n_samples_per_packet_temp]\n",
    "    if classification_type == 'address' or classification_type =='data_rate':\n",
    "        y_tensor = np.zeros((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                            n_packets_per_packet_type), dtype=np.uint16)\n",
    "    else:\n",
    "        y_tensor = np.zeros((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                            n_packets_per_packet_type), dtype=np.uint8)\n",
    "        \n",
    "    data_len_tensor = (np.ones((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                        n_packets_per_packet_type), dtype=np.uint16)\\\n",
    "                        *int(2**16-1)).astype(np.uint16)\n",
    "    data_rate_tensor = (np.ones((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                        n_packets_per_packet_type), dtype=np.uint16)\\\n",
    "                        *int(2**8-1)).astype(np.uint8)\n",
    "    print('x_tensor = {}| features_tensor = {}\\n'.format(x_tensor.shape,\\\n",
    "                                                     features_tensor.shape))\n",
    "    print('y_tensor = {}| data_len_tensor = {}| data_rate_tensor = {}\\n'.\\\n",
    "          format(y_tensor.shape, data_len_tensor.shape, data_rate_tensor.shape))\n",
    "    for filename in sorted(listdir(DATA_PATH)):\n",
    "        filepath_data = join(DATA_PATH, filename)\n",
    "        if fnmatch(filepath_data, '*.p') and fnmatch(filepath_data, '*_data_*')\\\n",
    "            and fnmatch(filepath_data, '*_ratio_'+str(ds_ratio)+'*') and\\\n",
    "            fnmatch(filepath_data, '*_block_'+str(block)+'*'):\n",
    "            start_adc_sample = 0\n",
    "            end_adc_sample = n_adc_samples\n",
    "        \n",
    "            x_tensor[1:N_PROTOCOLS+1,:,:,:,:,:,start_adc_sample:end_adc_sample]=\\\n",
    "                data_utils.unpickle_file(filepath_data) # 0th rf_signal_idx=Noise\n",
    "            \n",
    "            filename_features = filename.split('data')[0]+'features'+\\\n",
    "                                                    filename.split('data')[1]\n",
    "            filepath_features = join(DATA_PATH, filename_features)\n",
    "            features_tensor = data_utils.unpickle_file(filepath_features)\n",
    "\n",
    "            data_len_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                np.repeat(features_tensor[:,:,0,0,0,:,10,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3)\n",
    "            data_rate_tensor[1:N_PROTOCOLS+1,:,:] =\\\n",
    "                np.repeat(features_tensor[:,:,0,0,0,:,6,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3)\n",
    "            if classification_type == 'protocol':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,0,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Protocol Classification\n",
    "            elif classification_type == 'packet':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,1,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Packet Type Classification\n",
    "            elif classification_type == 'protocolAndPacket':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,2,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Protolcol and Packet \n",
    "            elif classification_type == 'address':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,9,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Address Classification\n",
    "            \n",
    "            # Matlab Dataset Labeling Error\n",
    "            if 2**8-1 in y_tensor or 2**16-1 in y_tensor:\n",
    "                # No portion of the y_tensor should be unlabeled (i.e. features_tensor[:,:,0,0,0,:,#] == 2**8-1)\n",
    "                print(\"Error in y_tensor labeling: {}\".format(classification_type))\n",
    "                set_trace()\n",
    "                \n",
    "            if print_each_plot is True:\n",
    "                for rf_signal_idx in tnrange(n_rf_signal):\n",
    "                    print(\"RF Signal Index: {}\".format(rf_signal_idx))\n",
    "                    packet_type_idx, c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                        packet_idx = 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "                    plt.figure(figsize=(25, 2))\n",
    "                    plt.subplot(1, 4,rf_signal_idx+ 1) \n",
    "\n",
    "                    data = x_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                                    packet_idx,:]\n",
    "                    class_target = y_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                    data_len = data_len_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                    data_rate = data_rate_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                    data_set_name = data_utils.convert_idx_to_rf_signal_name(\\\n",
    "                        rf_signal_list, rf_signal_idx)\n",
    "                    plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', data_set_name, classification_type, 'adc', y_max = 16384,\\\n",
    "                      x_max=data_len)\n",
    "\n",
    "    return x_tensor, y_tensor, data_len_tensor, data_rate_tensor\n",
    "\n",
    "# offset_0 = 0\n",
    "\n",
    "# n_downsamples_0 = 1 # N_DOWNSAMPLES\n",
    "# n_packets_per_packet_type_0 = N_PACKETS_PER_BLOCK\n",
    "# ds_ratio_sel = 4\n",
    "# block_sel = 0\n",
    "# verbose_load = True\n",
    "# show_plot = True\n",
    "# x_tensor, y_tensor, data_len_tensor, data_rate_tensor =\\\n",
    "#     create_tensors(offset_0, RF_SIGNAL_LIST, N_PACKET_TYPES,\\\n",
    "#                    N_CENTER_FREQS, n_downsamples_0, N_SNRS, N_MCSS,\\\n",
    "#                    n_packets_per_packet_type_0, N_ADC_SAMPLES, ds_ratio_sel,\\\n",
    "#                    block_sel, CLASSIFICATION_TYPE, verbose_load, show_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:25.005339Z",
     "start_time": "2020-01-24T07:50:24.995360Z"
    },
    "code_folding": [],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_tensor(x_tensor, y_tensor, data_len_tensor, data_rate_tensor, valid_frac, test_frac, show_plot = False):\n",
    "    x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "    x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\\\n",
    "        = data_utils.split_tensor_without_shuffle(x_tensor, y_tensor,\\\n",
    "                                              data_len_tensor, data_rate_tensor,\\\n",
    "                                              validation_fraction = test_frac) # Segmenting to train and test\n",
    "    # Memory clearing\n",
    "    del x_tensor, y_tensor, data_len_tensor, data_rate_tensor\n",
    "\n",
    "    x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "    x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor\\\n",
    "        = data_utils.split_tensor_without_shuffle(x_train_tensor, y_train_tensor,\\\n",
    "                                                data_len_train_tensor, data_rate_train_tensor,\\\n",
    "                                                validation_fraction = valid_frac) # Sub-segmenting training into train and valid\n",
    "    print('x_train_tensor shape = {}\\nx_valid_tensor shape = {}\\nx_test_tensor shape = {}'\\\n",
    "          .format(x_train_tensor.shape, x_valid_tensor.shape, x_test_tensor.shape))\n",
    "    print('y_train_tensor shape = {}| y_valid_tensor shape = {}| y_test_tensor shape = {}'\\\n",
    "          .format(y_train_tensor.shape, y_valid_tensor.shape, y_test_tensor.shape))\n",
    "    \n",
    "    if show_plot == True:\n",
    "        rf_signal_idx = np.random.randint(0, np.size(x_valid_tensor,0))\n",
    "        print(\"Random RF Signal Index: {}\".format(rf_signal_idx))\n",
    "        packet_type_idx, c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "            packet_idx = 0, 0, 0, 0, 0, 0\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        for ii in range(3):\n",
    "            plt.subplot(1, 3,ii+ 1)\n",
    "            if ii == 0:\n",
    "                data = x_train_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_train_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Train'     \n",
    "            elif ii == 1:\n",
    "                data = x_valid_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_valid_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Valid'     \n",
    "            else:\n",
    "                data = x_test_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_test_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Test'        \n",
    "\n",
    "            plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "                      y_max = 2**14-1, x_max=data_len)\n",
    "    return x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "            x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "            x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\n",
    "\n",
    "# valid_frac = 0.1\n",
    "# test_frac =  0.2\n",
    "# show_plot = True\n",
    "# x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "# x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "# x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\\\n",
    "#     = split_tensor(x_tensor, y_tensor, data_len_tensor, data_rate_tensor, valid_frac, test_frac, show_plot)\n",
    "\n",
    "# # Memory clearing\n",
    "# del x_tensor, y_tensor, data_len_tensor, data_rate_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data - Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:25.014219Z",
     "start_time": "2020-01-24T07:50:25.006449Z"
    },
    "code_folding": [],
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def min_max_scale_x_data(x_train_tensor, x_valid_tensor, x_test_tensor, show_plot = False):\n",
    "    # Min-Max Scaling x_train\n",
    "    scaling_range_min = 0.0\n",
    "    scaling_range_max = 1.0 #2**14-1\n",
    "    scaling_range = (scaling_range_min, scaling_range_max)\n",
    "    x_train_tensor = np.float32(data_utils.min_max_scale(x_train_tensor, scaling_range))\n",
    "    x_valid_tensor = np.float32(data_utils.min_max_scale(x_valid_tensor, scaling_range))\n",
    "    x_test_tensor = np.float32(data_utils.min_max_scale(x_test_tensor, scaling_range))\n",
    "    \n",
    "    if show_plot is True:\n",
    "        rf_signal_idx = np.random.randint(0, np.size(x_valid_tensor,0))\n",
    "        print(\"Random RF Signal Index: {}\".format(rf_signal_idx))\n",
    "        packet_type_idx, c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "            packet_idx = 0, 0, 0, 0, 0, 0\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        for ii in range(3):\n",
    "            plt.subplot(1, 3,ii+ 1)\n",
    "            if ii == 0:\n",
    "                data = x_train_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_train_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Train'     \n",
    "            elif ii == 1:\n",
    "                data = x_valid_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_valid_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Valid'     \n",
    "            else:\n",
    "                data = x_test_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_test_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Test'        \n",
    "\n",
    "            plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "                      y_max = scaling_range_max, x_max=data_len)\n",
    "    return x_train_tensor, x_valid_tensor, x_test_tensor\n",
    "\n",
    "# show_plot = True\n",
    "# x_train_tensor_norm, x_valid_tensor_norm, x_test_tensor_norm\\\n",
    "#     = min_max_scale_x_data(x_train_tensor, x_valid_tensor, x_test_tensor, show_plot)\n",
    "\n",
    "# del x_train_tensor, x_valid_tensor, x_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape to Unshuffled Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:25.026863Z",
     "start_time": "2020-01-24T07:50:25.015143Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def reshape_to_unshuffled_matrix(x_train_tensor_norm, y_train_tensor,\\\n",
    "    data_len_train_tensor, data_rate_train_tensor,x_valid_tensor_norm,\\\n",
    "    y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "    x_test_tensor_norm, y_test_tensor, data_len_test_tensor,\\\n",
    "    data_rate_test_tensor, show_plot = False):\n",
    "    \n",
    "#     data = [N_RF_SIGNALS, N_PACKET_TYPES, N_CENTER_FREQS,\\\n",
    "#         n_downsamples_per_pickle, N_SNRS, N_MCSS,\\\n",
    "#         N_PACKETS_PER_BLOCK, N_ADC_SAMPLES]\n",
    "\n",
    "#     data_features = [N_RF_SIGNALS, N_PACKET_TYPES,\\\n",
    "#         N_CENTER_FREQS, n_downsamples_per_pickle,\\\n",
    "#         N_SNRS, N_MCSS, 11]\n",
    "    \n",
    "    n_rf_signal = np.size(x_valid_tensor_norm, 0)\n",
    "    n_packet_types = np.size(x_valid_tensor_norm, 1)\n",
    "    n_center_freqs = np.size(x_valid_tensor_norm, 2)\n",
    "    n_downsamples_per_pickle = np.size(x_valid_tensor_norm, 3)\n",
    "    n_snrs = np.size(x_valid_tensor_norm, 4)\n",
    "    n_mcss = np.size(x_valid_tensor_norm, 5)\n",
    "    n_adc_samples = np.size(x_valid_tensor_norm, 7)\n",
    "    \n",
    "    # Reshaping the tensors into a [Combined Samples]x[ADC Samples] Matrix\n",
    "    n_packet_samples_train = np.size(x_train_tensor_norm, 6)\n",
    "    batches_in_train = n_packet_samples_train*n_rf_signal*n_packet_types\\\n",
    "        *n_center_freqs*n_downsamples_per_pickle*n_snrs*n_mcss\n",
    "    x_train_unshuffled = x_train_tensor_norm.reshape(\\\n",
    "        (batches_in_train, n_adc_samples)).astype(np.float32)\n",
    "    y_train_unshuffled = np.squeeze(y_train_tensor.reshape(\\\n",
    "        (batches_in_train, 1))).astype(np.uint8)\n",
    "    data_len_train_unshuffled = np.squeeze(data_len_train_tensor.reshape(\\\n",
    "        (batches_in_train, 1))).astype(np.uint16)\n",
    "    data_rate_train_unshuffled = np.squeeze(data_rate_train_tensor.reshape(\\\n",
    "        (batches_in_train, 1))).astype(np.uint16)\n",
    "\n",
    "    n_packet_samples_valid = np.size(x_valid_tensor_norm, 6)\n",
    "    batches_in_valid = n_packet_samples_valid*n_rf_signal*n_packet_types\\\n",
    "        *n_center_freqs*n_downsamples_per_pickle*n_snrs*n_mcss\n",
    "    x_valid_unshuffled = x_valid_tensor_norm.reshape(\\\n",
    "        (batches_in_valid, n_adc_samples)).astype(np.float32)\n",
    "    y_valid_unshuffled = np.squeeze(y_valid_tensor.reshape(\\\n",
    "        (batches_in_valid, 1))).astype(np.uint8)\n",
    "    data_len_valid_unshuffled = np.squeeze(data_len_valid_tensor.reshape(\\\n",
    "        (batches_in_valid, 1))).astype(np.uint16)\n",
    "    data_rate_valid_unshuffled = np.squeeze(data_rate_valid_tensor.reshape(\\\n",
    "        (batches_in_valid, 1))).astype(np.uint16)\n",
    "\n",
    "    n_packet_samples_test = np.size(x_test_tensor_norm, 6)\n",
    "    batches_in_test = n_packet_samples_test*n_rf_signal*n_packet_types\\\n",
    "        *n_center_freqs*n_downsamples_per_pickle*n_snrs*n_mcss\n",
    "    x_test_unshuffled = x_test_tensor_norm.reshape(\\\n",
    "        (batches_in_test, n_adc_samples)).astype(np.float32)\n",
    "    y_test_unshuffled = np.squeeze(y_test_tensor.reshape(\\\n",
    "        (batches_in_test, 1))).astype(np.uint8)\n",
    "    data_len_test_unshuffled = np.squeeze(data_len_test_tensor.reshape(\\\n",
    "        (batches_in_test, 1))).astype(np.uint16)\n",
    "    data_rate_test_unshuffled = np.squeeze(data_rate_test_tensor.reshape(\\\n",
    "        (batches_in_test, 1))).astype(np.uint16)\n",
    "\n",
    "    if show_plot is True:\n",
    "        batch_idx = np.random.randint(0, np.size(x_valid_unshuffled,0))\n",
    "        print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        for ii in range(3):\n",
    "            plt.subplot(1, 3,ii+ 1)\n",
    "            if ii == 0:\n",
    "                data = x_train_unshuffled[batch_idx,:]\n",
    "                class_target = y_train_unshuffled[batch_idx]\n",
    "                data_len = data_len_train_unshuffled[batch_idx]\n",
    "                data_rate = data_rate_train_unshuffled[batch_idx]\n",
    "                dataset_name = 'Train'     \n",
    "            elif ii == 1:\n",
    "                data = x_valid_unshuffled[batch_idx, :]\n",
    "                class_target = y_valid_unshuffled[batch_idx]\n",
    "                data_len = data_len_valid_unshuffled[batch_idx]\n",
    "                data_rate = data_rate_valid_unshuffled[batch_idx]\n",
    "                dataset_name = 'Valid'\n",
    "            else:\n",
    "                data = x_test_unshuffled[batch_idx,:]\n",
    "                class_target = y_test_unshuffled[batch_idx]\n",
    "                data_len = data_len_test_unshuffled[batch_idx]\n",
    "                data_rate = data_rate_test_unshuffled[batch_idx]\n",
    "                dataset_name = 'Test'        \n",
    "\n",
    "            plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "                      y_max = 1.0, x_max=data_len)\n",
    "\n",
    "    return x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled,\\\n",
    "        x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled, data_rate_valid_unshuffled,\\\n",
    "        x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled\n",
    "\n",
    "# print(\"# of Protocols in Dataset (not including 'Noise' classification): {}\".format(N_CLASSIFICATIONS-1))\n",
    "\n",
    "# show_plot = True\n",
    "# x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled,\\\n",
    "#     data_rate_train_unshuffled, x_valid_unshuffled, y_valid_unshuffled,\\\n",
    "#     data_len_valid_unshuffled, data_rate_valid_unshuffled, x_test_unshuffled,\\\n",
    "#     y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled\\\n",
    "#     = reshape_to_unshuffled_matrix(x_train_tensor_norm, y_train_tensor,\\\n",
    "#         data_len_train_tensor, data_rate_train_tensor, x_valid_tensor_norm,\\\n",
    "#         y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "#         x_test_tensor_norm, y_test_tensor, data_len_test_tensor,\\\n",
    "#         data_rate_test_tensor, show_plot)\n",
    "\n",
    "# # Memory clearing\n",
    "# del x_test_tensor_norm, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\n",
    "# del x_valid_tensor_norm, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor\n",
    "# del x_train_tensor_norm, y_train_tensor, data_len_train_tensor, data_rate_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_mat[ ][ ] to *.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:25.034820Z",
     "start_time": "2020-01-24T07:50:25.027745Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def write_unshuffled_matrix_to_p(data_type, classification_type,\\\n",
    "    downsample_rate, block, data_rate_unshuffled = None, \\\n",
    "    data_len_unshuffled = None, y_unshuffled = None, x_unshuffled = None,\\\n",
    "    is_verbose = True):\n",
    "    \n",
    "    if data_rate_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_data_rate_'\\\n",
    "            + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(data_rate_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "        \n",
    "    if data_len_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_data_len_'\\\n",
    "            + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(data_len_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "    \n",
    "    if y_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_y_' + data_type\\\n",
    "            + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(y_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "\n",
    "    # Note: x_unshuffled run alone in order to free up as much memory as possible before writing\n",
    "    if x_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_x_' + data_type\\\n",
    "            + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(x_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "\n",
    "# ds_rate_0 = 1\n",
    "# block_0 = 0\n",
    "# is_verbose = True\n",
    "# data_type_0 = 'valid'\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "#     y_valid_unshuffled, x_valid_unshuffled, is_verbose)\n",
    "# del x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "#     data_rate_valid_unshuffled\n",
    "\n",
    "# data_type_0 = 'test'\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_test_unshuffled, data_len_test_unshuffled,\\\n",
    "#     y_test_unshuffled, x_test_unshuffled, is_verbose)\n",
    "# del x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled,\\\n",
    "#     data_rate_test_unshuffled\n",
    "\n",
    "# data_type_0 = 'train'\n",
    "# x_unshuffled_dummy = None\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_train_unshuffled, data_len_train_unshuffled,\\\n",
    "#     y_train_unshuffled, x_unshuffled_dummy, is_verbose)\n",
    "# del y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled\n",
    "\n",
    "# data_rate_unshuffled_dummy = None\n",
    "# data_len_unshuffled_dummy = None\n",
    "# y_unshuffled_dummy = None\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_unshuffled_dummy, data_len_unshuffled_dummy,\\\n",
    "#     y_unshuffled_dummy, x_train_unshuffled, is_verbose)\n",
    "# del x_train_unshuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T09:56:49.040708Z",
     "start_time": "2019-06-27T09:56:49.037885Z"
    }
   },
   "source": [
    "## Load and Preprocess All Data Blocks To *.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:09:31.908433Z",
     "start_time": "2019-11-19T02:08:16.870257Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification: {}| # of Classifications: {}\\n'.format(CLASSIFICATION_TYPE, N_CLASSIFICATIONS))\n",
    "\n",
    "N_PACKETS_PER_PACKET_TYPE, N_ADC_SAMPLES = calculate_dataset_limits()\n",
    "N_PACKETS_PER_BLOCK = int(N_PACKETS_PER_PACKET_TYPE/N_DATASET_BLOCKS)\n",
    "\n",
    "print('User Settings:\\n  ADC Samples: {}\\n  Packets per Type: {}\\n  Dataset Blocks: {}\\n  Dataset Block Sample Size: {}\\n'\\\n",
    "      .format(N_ADC_SAMPLES, N_PACKETS_PER_PACKET_TYPE, N_DATASET_BLOCKS, N_PACKETS_PER_BLOCK))\n",
    "\n",
    "ds_ratio_sel = 4\n",
    "# for block in tnrange(N_DATASET_BLOCKS):\n",
    "block = 0\n",
    "\n",
    "print(\"> > Create Tensor:\")\n",
    "offset_0 = 0\n",
    "n_downsamples_0 = 1 # N_DOWNSAMPLES to use. Limited to 1 bc\n",
    "verbose_load = True\n",
    "show_plot = False\n",
    "x_tensor, y_tensor, data_len_tensor, data_rate_tensor =\\\n",
    "    create_tensors(offset_0, RF_SIGNAL_LIST, N_PACKET_TYPES,\\\n",
    "                   N_CENTER_FREQS, n_downsamples_0, N_SNRS, N_MCSS,\\\n",
    "                   N_PACKETS_PER_BLOCK, N_ADC_SAMPLES, ds_ratio_sel,\\\n",
    "                   block, CLASSIFICATION_TYPE, verbose_load, show_plot)\n",
    "print(\"> > Split Tensor:\")\n",
    "valid_frac = 0.1\n",
    "test_frac =  0.2\n",
    "show_plot = True\n",
    "x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\\\n",
    "    = split_tensor(x_tensor, y_tensor, data_len_tensor, data_rate_tensor,\\\n",
    "                   valid_frac, test_frac, show_plot)\n",
    "\n",
    "del x_tensor, y_tensor, data_len_tensor, data_rate_tensor # Memory clearing\n",
    "\n",
    "print(\"> > Tensor Min Max Scale:\")\n",
    "show_plot = True\n",
    "x_train_tensor_norm, x_valid_tensor_norm, x_test_tensor_norm\\\n",
    "    = min_max_scale_x_data(x_train_tensor, x_valid_tensor, x_test_tensor,\\\n",
    "                           show_plot)\n",
    "\n",
    "del x_train_tensor, x_valid_tensor, x_test_tensor # Memory clearing\n",
    "\n",
    "print(\"> > Reshape to Unshuffled Matrix:\")\n",
    "show_plot = True\n",
    "x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled,\\\n",
    "    data_rate_train_unshuffled, x_valid_unshuffled, y_valid_unshuffled,\\\n",
    "    data_len_valid_unshuffled, data_rate_valid_unshuffled, x_test_unshuffled,\\\n",
    "    y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled\\\n",
    "    = reshape_to_unshuffled_matrix(x_train_tensor_norm, y_train_tensor,\\\n",
    "        data_len_train_tensor, data_rate_train_tensor, x_valid_tensor_norm,\\\n",
    "        y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "        x_test_tensor_norm, y_test_tensor, data_len_test_tensor,\\\n",
    "        data_rate_test_tensor, show_plot)\n",
    "\n",
    "# Memory clearing\n",
    "del x_test_tensor_norm, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\n",
    "del x_valid_tensor_norm, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor\n",
    "del x_train_tensor_norm, y_train_tensor, data_len_train_tensor, data_rate_train_tensor\n",
    "\n",
    "print(\"> > Write Unshuffled Matrix to .p:\")\n",
    "is_verbose = True\n",
    "data_type_0 = 'valid'\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "    y_valid_unshuffled, x_valid_unshuffled, is_verbose)\n",
    "del x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "    data_rate_valid_unshuffled\n",
    "\n",
    "data_type_0 = 'test'\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_test_unshuffled, data_len_test_unshuffled,\\\n",
    "    y_test_unshuffled, x_test_unshuffled, is_verbose)\n",
    "del x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled,\\\n",
    "    data_rate_test_unshuffled\n",
    "\n",
    "data_type_0 = 'train'\n",
    "x_unshuffled_dummy = None\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_train_unshuffled, data_len_train_unshuffled,\\\n",
    "    y_train_unshuffled, x_unshuffled_dummy, is_verbose)\n",
    "del y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled\n",
    "\n",
    "data_rate_unshuffled_dummy = None\n",
    "data_len_unshuffled_dummy = None\n",
    "y_unshuffled_dummy = None\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_unshuffled_dummy, data_len_unshuffled_dummy,\\\n",
    "    y_unshuffled_dummy, x_train_unshuffled, is_verbose)\n",
    "del x_train_unshuffled\n",
    "\n",
    "print('\\n---------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *.p to data_mat[ ][ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:25.047573Z",
     "start_time": "2020-01-24T07:50:25.035715Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def load_presegmented_dataset(data_type, classification_type, downsample_rate, block, is_verbose = True, show_plot = False):\n",
    "    if is_verbose:\n",
    "        print('Classification: {}| # of Classifications: {}\\n'.\\\n",
    "              format(classification_type, N_CLASSIFICATIONS))\n",
    "\n",
    "    pickle_data_filename = '/' + classification_type + '_x_' + data_type\\\n",
    "        + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    x_unshuffled = data_utils.unpickle_file(DATA_PATH + pickle_data_filename, is_verbose)\n",
    "    pickle_data_filename = '/' + classification_type + '_y_' + data_type\\\n",
    "        + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    y_unshuffled = np.squeeze(data_utils.unpickle_file(DATA_PATH\\\n",
    "        + pickle_data_filename, is_verbose)).astype(int)\n",
    "    pickle_data_filename = '/' + classification_type + '_data_len_'\\\n",
    "        + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    data_len_unshuffled = np.squeeze(data_utils.unpickle_file(DATA_PATH\\\n",
    "        + pickle_data_filename, is_verbose)).astype(int)\n",
    "    pickle_data_filename = '/' + classification_type + '_data_rate_'\\\n",
    "        + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    data_rate_unshuffled = np.squeeze(data_utils.unpickle_file(DATA_PATH\\\n",
    "        + pickle_data_filename, is_verbose))\n",
    "        \n",
    "    if is_verbose == True:\n",
    "        print(\"x_\"+data_type+\"_unshuffled: {}\".format(np.shape(x_unshuffled)))\n",
    "        print(\"y_\"+data_type+\"_unshuffled: {}\".format(np.shape(y_unshuffled)))\n",
    "        print(\"data_len_\"+data_type+\"_unshuffled: {}\".format(np.shape(data_len_unshuffled)))\n",
    "        print(\"data_rate_\"+data_type+\"_unshuffled: {}\".format(np.shape(data_rate_unshuffled)))\n",
    "        \n",
    "    if show_plot is True:\n",
    "        batch_idx = np.random.randint(0, np.size(x_unshuffled,0))\n",
    "        print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        data = x_unshuffled[batch_idx,:]\n",
    "        class_target = y_unshuffled[batch_idx]\n",
    "        data_len = data_len_unshuffled[batch_idx]\n",
    "        data_rate = data_rate_unshuffled[batch_idx]\n",
    "        dataset_name = data_type     \n",
    "        plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                  'NA', dataset_name, classification_type, 'adc',\\\n",
    "                  y_max = 1.0, x_max=data_len)\n",
    "        \n",
    "    return x_unshuffled, y_unshuffled, data_len_unshuffled, data_rate_unshuffled\n",
    "    \n",
    "# is_verbose = True\n",
    "# show_plot = True\n",
    "# downsample_rate = 1\n",
    "# block = 0\n",
    "\n",
    "# dataset_type = 'train'\n",
    "# x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)\n",
    "\n",
    "# dataset_type = 'valid'\n",
    "# x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled, data_rate_valid_unshuffled,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)\n",
    "\n",
    "# dataset_type = 'test'\n",
    "# x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T05:19:36.452191Z",
     "start_time": "2019-06-28T05:19:36.449342Z"
    }
   },
   "source": [
    "# Plotting Random Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:06:06.563062Z",
     "start_time": "2019-11-19T02:06:06.256642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Due to the large number of packets in x_train by comparison to x_valid, \n",
    "#  this function will only plot Noise for x_train.\n",
    "#  e.g. if x_train_tensor shape = (4, 9, 1, 1, 1, 8, 480, 8032),  \n",
    "#  x_valid_tensor shape = 10% = (4, 9, 1, 1, 1, 8, 53, 8032), and\n",
    "#  x_test_tensor shape = 20% = (4, 9, 1, 1, 1, 8, 133, 8032), then the maximum\n",
    "#  batch_idx for x_valid = 15264, but the last Noise packet for x_train = 34560,\n",
    "#  however, the last Noise packet for x_test = 9576\n",
    "\n",
    "y_max_0 = x_valid.max()\n",
    "\n",
    "batch_idx = np.random.randint(0, np.size(x_valid,0))\n",
    "print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "plt.figure(figsize=(20, 2))\n",
    "for ii in range(3):\n",
    "    plt.subplot(1, 3,ii+ 1)\n",
    "    if ii == 0:\n",
    "        data = x_train[batch_idx,:]\n",
    "        class_target = y_train[batch_idx]\n",
    "        data_len = data_len_train[batch_idx]\n",
    "        data_rate = data_rate_train[batch_idx]\n",
    "        dataset_name = 'Train'     \n",
    "    elif ii == 1:\n",
    "        data = x_valid[batch_idx, :]\n",
    "        class_target = y_valid[batch_idx]\n",
    "        data_len = data_len_valid[batch_idx]\n",
    "        data_rate = data_rate_valid[batch_idx]\n",
    "        dataset_name = 'Valid'\n",
    "    else:\n",
    "        data = x_test[batch_idx,:]\n",
    "        class_target = y_test[batch_idx]\n",
    "        data_len = data_len_test[batch_idx]\n",
    "        data_rate = data_rate_test[batch_idx]\n",
    "        dataset_name = 'Test'\n",
    "\n",
    "    plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "              'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "              y_max = y_max_0, x_max=data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T20:23:06.183002Z",
     "start_time": "2019-11-18T20:23:06.123533Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Note: Due to the large number of packets in x_train by comparison to x_valid, \n",
    "# #  this function will only plot Noise for x_train.\n",
    "# #  e.g. if x_train_tensor shape = (4, 9, 1, 1, 1, 8, 480, 8032),  \n",
    "# #  x_valid_tensor shape = 10% = (4, 9, 1, 1, 1, 8, 53, 8032), and\n",
    "# #  x_test_tensor shape = 20% = (4, 9, 1, 1, 1, 8, 133, 8032), then the maximum\n",
    "# #  batch_idx for x_valid = 15264, but the last Noise packet for x_train = 34560,\n",
    "# #  however, the last Noise packet for x_test = 9576\n",
    "\n",
    "# y_max_0 = 1.0\n",
    "\n",
    "# batch_idx = np.random.randint(0, np.size(x_valid_unshuffled,0))\n",
    "# print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "# plt.figure(figsize=(20, 2))\n",
    "# for ii in range(3):\n",
    "#     plt.subplot(1, 3,ii+ 1)\n",
    "#     if ii == 0:\n",
    "#         data = x_train_unshuffled[batch_idx,:]\n",
    "#         class_target = y_train_unshuffled[batch_idx]\n",
    "#         data_len = data_len_train_unshuffled[batch_idx]\n",
    "#         data_rate = data_rate_train_unshuffled[batch_idx]\n",
    "#         dataset_name = 'Train'     \n",
    "#     elif ii == 1:\n",
    "#         data = x_valid_unshuffled[batch_idx, :]\n",
    "#         class_target = y_valid_unshuffled[batch_idx]\n",
    "#         data_len = data_len_valid_unshuffled[batch_idx]\n",
    "#         data_rate = data_rate_valid_unshuffled[batch_idx]\n",
    "#         dataset_name = 'Valid'\n",
    "#     else:\n",
    "#         data = x_test_unshuffled[batch_idx,:]\n",
    "#         class_target = y_test_unshuffled[batch_idx]\n",
    "#         data_len = data_len_test_unshuffled[batch_idx]\n",
    "#         data_rate = data_rate_test_unshuffled[batch_idx]\n",
    "#         dataset_name = 'Test'\n",
    "\n",
    "#     plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "#               'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "#               y_max = y_max_0, x_max=data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:25.209562Z",
     "start_time": "2020-01-24T07:50:25.048951Z"
    },
    "code_folding": [
     1,
     19,
     151,
     177,
     231,
     283,
     289,
     308,
     340,
     346,
     365,
     407,
     437,
     477,
     516,
     580
    ],
    "init_cell": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--------------------------\\nIncluding Instantiation and training here for ease of running\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%pixie_debugger\n",
    "class log_history(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('sparse_categorical_accuracy'))\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_acc.append(logs.get('val_sparse_categorical_accuracy'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        if(np.size(self.loss)%50 == 0):\n",
    "            print('\\n--- Training Epoch: {}| Loss: {:.3f}| Acc: {:.3f} ---\\n'.\\\n",
    "                  format(np.size(self.loss), logs.get('loss'),\\\n",
    "                    logs.get('sparse_categorical_accuracy')))\n",
    "        \n",
    "class nn_classification_model_v19:\n",
    "    def __init__(self, batch_size, sequence_length, classification_type,\\\n",
    "                 n_classifications, dropout_probability, learning_rate,\\\n",
    "                 is_training, nn_type, max_epochs,\n",
    "                 cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "                 cnn_n_filters_2 = None, cnn_kernel_size_2 = None, cnn_pool_size_2 = None, \\\n",
    "                 rnn_type = None, n_rnn_stack = None, rnn_len = None):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.input_seq_len = sequence_length\n",
    "        self.input_shape = (sequence_length, 1)\n",
    "        self.classification_type = classification_type\n",
    "        self.n_classifications = n_classifications\n",
    "        self.dropout_probability = dropout_probability\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.nn_type = nn_type\n",
    "        \n",
    "        self.cnn_n_filters = cnn_n_filters\n",
    "        self.cnn_kernel_size = cnn_kernel_size\n",
    "        self.cnn_pool_size = cnn_pool_size\n",
    "        self.stride_size = 2\n",
    "        \n",
    "        self.cnn_n_filters_2 = cnn_n_filters_2\n",
    "        self.cnn_kernel_size_2 = cnn_kernel_size_2\n",
    "        self.cnn_pool_size_2 = cnn_pool_size_2\n",
    "        self.stride_size_2 = 2\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.rnn_len = rnn_len\n",
    "        self.n_rnn_stack = n_rnn_stack\n",
    "        \n",
    "        if nn_type == 'cnn_1layer' or nn_type == 'cnn_2layer':\n",
    "            self.pooled_sequence_length = self.n_classifications\n",
    "        elif nn_type == 'rnn':\n",
    "            self.rnn_sequence_length = self.input_seq_len\n",
    "        elif nn_type == 'cnn-rnn':\n",
    "            self.rnn_sequence_length = int(np.around(self.input_seq_len/self.cnn_pool_size))\n",
    "        \n",
    "        self.global_step = 0\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU (prevents kernel crashes)\n",
    "        self.session = tf.Session(config=config) \n",
    "        K.set_session(self.session) # set this TensorFlow session as the default session for Keras\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.model = None\n",
    "    \n",
    "    # Train on single block of pre-loaded data\n",
    "    def train_v00(self, dataset_blocks, optimization_target, no_improvement_limit,\\\n",
    "                x_train, y_train, data_len_train, data_rate_train,\\\n",
    "                x_valid, y_valid, data_len_valid, data_rate_valid, keras_verbose):\n",
    "        tf.keras.backend.set_learning_phase(1) #K.set_learning_phase(1) # 0 testing, 1 training mode        \n",
    "        data_utils.delete_files_in_folder(LOGS_DIR + '/train')\n",
    "        data_utils.delete_files_in_folder(LOGS_DIR + '/valid')\n",
    "        history_callback = log_history()\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINTS_DIR\\\n",
    "            + '/model-{epoch:02d}.hdf5', verbose=keras_verbose,\\\n",
    "            monitor='val_loss', save_best_only=True)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOGS_DIR)   \n",
    "        \n",
    "        if optimization_target == 'loss':\n",
    "            early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience\\\n",
    "                = no_improvement_limit, restore_best_weights=True)\n",
    "        elif optimization_target == 'accuracy':\n",
    "            early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor\\\n",
    "                = 'val_sparse_categorical_accuracy', patience\\\n",
    "                = no_improvement_limit, restore_best_weights=True)\n",
    "        \n",
    "        is_verbose = True\n",
    "        if(self.is_training is True):\n",
    "            if(is_verbose is True):\n",
    "                print(\">> Training Settings: \")\n",
    "                print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "                print(\">> NN Type: {}\".format(self.nn_type) + \"| Seq Len: {}\".format(self.input_seq_len) + \"| LR: {:.6f}\".format(self.learning_rate)\\\n",
    "                  + \"| DO: {:.4f}\".format(self.dropout_probability) + \"| b_size: {}\".format(self.batch_size))\n",
    "                if self.nn_type == 'cnn_1layer' or self.nn_type == 'cnn_2layer' or self.nn_type == 'cnn-rnn':\n",
    "                    print('>> cnn_n_filters: {}| cnn_kernel_size: {}| cnn_pool_size: {}'\\\n",
    "                          .format(self.cnn_n_filters, self.cnn_kernel_size, self.cnn_pool_size))\n",
    "\n",
    "                if self.nn_type == 'cnn_2layer':\n",
    "                    print('>> cnn_n_filters_2: {}| cnn_kernel_size_2: {}| cnn_pool_size_2: {}'\\\n",
    "                          .format(self.cnn_n_filters_2, self.cnn_kernel_size_2, self.cnn_pool_size_2))\n",
    "                elif self.nn_type == 'rnn' or self.nn_type == 'cnn-rnn':\n",
    "                    print('>> rnn_type: {}| rnn_layers: {}| rnn_len: {}'\\\n",
    "                          .format(self.rnn_type, self.n_rnn_stack, self.rnn_len))\n",
    "\n",
    "                print(\"    Tensorboard Logs Directory:\")\n",
    "                print(\"        \" + str(LOGS_DIR) + '/train')\n",
    "                print('\\n---------------------------------------------------------------\\n')\n",
    "            \n",
    "            start = timer()\n",
    "            \n",
    "            show_plot = False\n",
    "            is_testing = False  \n",
    "            self.train_len = np.size(y_train,0)\n",
    "            self.valid_len = np.size(y_valid,0)\n",
    "\n",
    "            x_train, y_train, data_len_train, data_rate_train = data_utils.\\\n",
    "                unison_shuffled_copies(x_train, y_train, data_len_train,\\\n",
    "                                       data_rate_train)\n",
    "            x_valid, y_valid, data_len_valid, data_rate_valid = data_utils.\\\n",
    "                unison_shuffled_copies(x_valid, y_valid, data_len_valid,\\\n",
    "                                       data_rate_valid)\n",
    "            \n",
    "            x_train_expanded = np.expand_dims(x_train, axis=2)\n",
    "            y_train_1hot = tf.keras.utils.to_categorical(y_train, num_classes=self.n_classifications)\n",
    "            x_valid_expanded = np.expand_dims(x_valid, axis=2)            \n",
    "            y_valid_1hot = tf.keras.utils.to_categorical(y_valid, num_classes=self.n_classifications)\n",
    "            with self.graph.as_default():\n",
    "                with self.session.as_default():\n",
    "                    training_history = self.model.fit(\n",
    "                        x_train_expanded, y_train, batch_size=self.batch_size,\\\n",
    "                        shuffle = False, verbose=keras_verbose,\\\n",
    "                        epochs=self.max_epochs,validation_data\\\n",
    "                        =(x_valid_expanded, y_valid), # ??? validation_callback to shuffle at end of epoch?\n",
    "                        callbacks=[history_callback, checkpoint_callback,\\\n",
    "                                   tensorboard_callback, early_stop_callback]                        \n",
    "                    )       \n",
    "            \n",
    "                    self.is_training = False        \n",
    "                    file_bytes = self.train_wrapup()\n",
    "            if(is_verbose == True):\n",
    "                print(\"Final Weights Save File:\\n  {}\".format(MODEL_FINAL_WEIGHTS_SAVE_PATH))\n",
    "                print(\"Final Model Save File:\\n  {}\".format(MODEL_FINAL_SAVE_PATH))\n",
    "                print('\\nFinal Training Epoch: {}| Loss: {:.3f}| Acc: {:.3f}\\n\\n'.format(\\\n",
    "                    np.size(training_history.history['loss'],0),\\\n",
    "                    training_history.history['loss'][-1],\\\n",
    "                    training_history.history['sparse_categorical_accuracy'][-1]))\n",
    "        return training_history, file_bytes\n",
    "    \n",
    "    # Saves the final checkpoint and measures graph properties (nodes, parameters, deployable file size)\n",
    "    def train_wrapup(self):        \n",
    "        self.timestamp = str(math.trunc(timer()))\n",
    "        \n",
    "        self.model.save_weights(MODEL_FINAL_WEIGHTS_SAVE_PATH)        \n",
    "        self.model.save(MODEL_FINAL_SAVE_PATH, include_optimizer=False)\n",
    "        \n",
    "#         model_json = self.model.to_json() # only the architecture of the model\n",
    "#         loaded_model_from_json = keras.models.model_from_json(model_final_architecture_file) #loads model architecture only        \n",
    "#         loaded_model_with_weights = loaded_model_from_json.load_weights(MODEL_FINAL_WEIGHTS_SAVE_PATH) # Alternatively, can create model using model = Sequential (...)\n",
    "#         loaded_model = keras.models.load_model(MODEL_FINAL_SAVE_PATH) # load model architecture, weights, training configuration, and state of optimizer when model was saved\n",
    "#         loaded_model.summary() # Should be the same\n",
    "        \n",
    "#         saver = tf.train.Saver()\n",
    "#         K.set_learning_phase(0)\n",
    "#         saved_file = saver.save(sess, CHECKPOINTS_DIR\\\n",
    "#                              + '/nn_valid_' + self.timestamp,\\\n",
    "#                              tf.train.global_step(sess, self.global_step)) # Saving the final check_point\n",
    "#         print(\"    Final Saved checkpoint: \" + saved_file)\n",
    "        file_bytes = os.path.getsize(MODEL_FINAL_SAVE_PATH)\n",
    "        file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "        print('\\nStripped Model Size: {:.3f}{}'.format(file_size, units))\n",
    "        \n",
    "#         file_bytes_0 = self.freeze_graph() # Save the frozen graph\n",
    "        \n",
    "        return file_bytes\n",
    "    \n",
    "    def freeze_graph(self, keep_var_names=None, clear_devices=True):\n",
    "        \"\"\"\n",
    "        Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "        Creates a new computation graph where variable nodes are replaced by\n",
    "        constants taking their current value in the session. The new graph will be\n",
    "        pruned so subgraphs that are not necessary to compute the requested\n",
    "        outputs are removed.\n",
    "        @param session The TensorFlow session to be frozen.\n",
    "        @param keep_var_names A list of variable names that should not be frozen,\n",
    "                              or None to freeze all the variables in the graph.\n",
    "        @param output_names Names of the relevant graph outputs.\n",
    "        @param clear_devices Remove the device directives from the graph for better portability.\n",
    "        @return The frozen graph definition.\n",
    "        \"\"\"\n",
    "                \n",
    "        sess = K.get_session()   \n",
    "        \n",
    "        output_names=[out.op.name for out in self.model.outputs]\n",
    "        input_names=[inputs_0.op.name for inputs_0 in self.model.inputs]\n",
    "        \n",
    "        graph = sess.graph\n",
    "        with graph.as_default():\n",
    "            freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "            output_names = output_names or []\n",
    "            output_names += [v.op.name for v in tf.global_variables()]\n",
    "            # Graph -> GraphDef ProtoBuf\n",
    "            input_graph_def = graph.as_graph_def()\n",
    "            if clear_devices:\n",
    "                for node in input_graph_def.node:\n",
    "                    node.device = \"\"\n",
    "            \n",
    "            print(\"\\nInput Node: {}\".format([inputs_0.op.name for inputs_0 in self.model.inputs]))\n",
    "            print(\"Output Node: {}\".format([outputs_0.op.name for outputs_0 in self.model.outputs]))\n",
    "            print('\\n---------------------------------------------------------------\\n')\n",
    "            \n",
    "            from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "            print('\\n')\n",
    "            frozen_graph = convert_variables_to_constants(sess, input_graph_def, \\\n",
    "                                                          output_names, freeze_var_names)\n",
    "        \n",
    "        print('\\n')\n",
    "        frozen_graph_dir = CHECKPOINTS_DIR \n",
    "        frozen_graph_filename = 'saved_model.pb'\n",
    "        tf.train.write_graph(frozen_graph, frozen_graph_dir, frozen_graph_filename, as_text=False )\n",
    "    \n",
    "        # Get frozen graph file size\n",
    "        frozen_graph_filepath = frozen_graph_dir + '/' + frozen_graph_filename\n",
    "        file_bytes = int(os.path.getsize(frozen_graph_filepath))\n",
    "        file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "#         print('\\nFrozen Graph File Size: {:.3f}{}'.format(file_size, units)) \n",
    "        \n",
    "        return file_bytes\n",
    "        \n",
    "    def build_cnn_1layer_model_v00(self):        \n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                self.model = tf.keras.Sequential([\n",
    "                    l.Conv1D(filters=self.cnn_n_filters,\\\n",
    "                        kernel_size=self.cnn_kernel_size, activation='relu',\\\n",
    "                        input_shape=self.input_shape,\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size,\\\n",
    "                        strides=self.stride_size, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Flatten(),\n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "            \n",
    "    def build_cnn_2layer_model_v00(self):\n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                self.model = tf.keras.Sequential([\n",
    "                    l.Conv1D(filters=self.cnn_n_filters,\\\n",
    "                        kernel_size=self.cnn_kernel_size, activation='relu',\\\n",
    "                        input_shape=self.input_shape,\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size,\\\n",
    "                        strides=self.stride_size, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Conv1D(filters=self.cnn_n_filters_2,\\\n",
    "                        kernel_size=self.cnn_kernel_size_2, activation='relu',\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size_2,\\\n",
    "                        strides=self.stride_size_2, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Flatten(),                    \n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "    \n",
    "    def build_rnn_model_v00(self):\n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                rnn_input_timesteps = self.rnn_sequence_length\n",
    "                n_input_features = 1 # Only one time sequence fed in at a time (as opposed to something like 3 dimen of velocity)\n",
    "                if self.n_rnn_stack == 1:\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.SimpleRNN(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRU(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTM(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        return rnn_layer\n",
    "                    rnn_stack = get_a_cell(self.rnn_len)\n",
    "                else: # Won't use CuDNN Kernel\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.RNNCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRUCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTMCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        return rnn_layer\n",
    "                    rnn_stacked_cells = [get_a_cell(self.rnn_len) for _ in range(self.n_rnn_stack)]           \n",
    "                    rnn_stack = l.RNN(rnn_stacked_cells,\n",
    "                                    input_shape=(rnn_input_timesteps, n_input_features))\n",
    "\n",
    "                self.model = tf.keras.models.Sequential([\n",
    "                    rnn_stack,\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "            \n",
    "    def build_cnn_to_rnn_model_v00(self):\n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                rnn_input_timesteps = self.rnn_sequence_length\n",
    "                n_input_features = 1 # Only one time sequence fed in at a time (as opposed to something like 3 dimen of velocity)\n",
    "                if self.n_rnn_stack == 1:\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.SimpleRNN(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRU(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTM(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        return rnn_layer\n",
    "                    rnn_stack = get_a_cell(self.rnn_len)\n",
    "                else: # Won't use CuDNN Kernel\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.RNNCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRUCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTMCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        return rnn_layer\n",
    "                    rnn_stacked_cells = [get_a_cell(self.rnn_len) for _ in range(self.n_rnn_stack)]           \n",
    "                    rnn_stack = l.RNN(rnn_stacked_cells,\n",
    "                                    input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                \n",
    "                self.model = tf.keras.Sequential([\n",
    "                    l.Conv1D(self.cnn_n_filters, kernel_size\\\n",
    "                        =self.cnn_kernel_size, activation='relu',\\\n",
    "                        input_shape=self.input_shape,\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size,\\\n",
    "                        strides=self.stride_size, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Flatten(),\n",
    "                    l.Dense(rnn_input_timesteps, activation='softmax'),                    \n",
    "                    l.Reshape((rnn_input_timesteps,n_input_features)),\n",
    "                    rnn_stack,\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "            \n",
    "    def sample_v00(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        batch_size_test = self.batch_size #self.test_len #\n",
    "        \n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        self.test_len = np.size(y_test,0)\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            feed = {self.tf_inputs: x_batch[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                    self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                    self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "            acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "            print(\"Test Accuracy= {}%\".format(np.asscalar(np.asarray(acc_test))*100.0))\n",
    "            if acc_test < 1.0:\n",
    "                print(\"Erroroneous Samples:\")\n",
    "                incorrect_pred_idx = np.squeeze(np.where(np.squeeze(correct_pred) == False))\n",
    "#                 print(incorrect_pred_idx)\n",
    "                print(\"Prediction: {}| y_batch classification: {}| data_len: {}| data_rate: {}\"\\\n",
    "                      .format(logits_argmax_1[incorrect_pred_idx].T, y_batch[incorrect_pred_idx],\\\n",
    "                              data_len_batch[incorrect_pred_idx], data_rate_batch[incorrect_pred_idx]))          \n",
    "    def sample_v11(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "        \n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "#         for iter_idx in tnrange(n_dataset_block):                    \n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot)\n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "        self.test_len = np.size(y_test,0)\n",
    "        batch_size_test = self.test_len #\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            snr_list = []\n",
    "            accuracy_list = []            \n",
    "            for snr_level in np.arange(15, -15, -1):\n",
    "                x_batch_noise = x_batch\n",
    "                for ii in range(batch_size_test):\n",
    "                    x_batch_noise[ii] = channels.awgn(x_batch[ii], snr_level, 1.0)\n",
    "\n",
    "                feed = {self.tf_inputs: x_batch_noise[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                        self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                        self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "                acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "                snr_list.append(snr_level)\n",
    "                accuracy_list.append(np.asscalar(np.asarray(acc_test)))\n",
    "        plt.figure()\n",
    "        plt.plot(snr_list, accuracy_list, linewidth=4)                             \n",
    "        plt.title('Accuracy vs SNR')\n",
    "        plt.xlabel('SNR (dB)')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.show()\n",
    "        \n",
    "        return accuracy_list, snr_list        \n",
    "    def sample_v12(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        batch_size_test = self.batch_size #self.test_len #\n",
    "        \n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "#         for iter_idx in tnrange(n_dataset_block):                    \n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "        self.test_len = np.size(y_test,0)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            decimation_list = []\n",
    "            accuracy_list = []         \n",
    "            for dec_factor in np.arange(1, 100):\n",
    "                x_batch_decimate = x_batch\n",
    "                for ii in range(batch_size_test):\n",
    "                    x_batch_decimate[ii] = data_utils.decimate(x_batch[ii], dec_factor)\n",
    "\n",
    "                feed = {self.tf_inputs: x_batch_decimate[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                        self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                        self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "                acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "                decimation_list.append(dec_factor)\n",
    "                accuracy_list.append(np.asscalar(np.asarray(acc_test)))\n",
    "        plt.figure()\n",
    "        plt.plot(snr_list, accuracy_list, linewidth=4)                             \n",
    "        plt.title('Accuracy vs Decimation')\n",
    "        plt.xlabel('Decimation')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.show()\n",
    "    def sample_v13(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        is_verbose = True\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        self.test_len = np.size(y_test,0)\n",
    "        batch_size_test = self.test_len #self.batch_size\n",
    "        \n",
    "        unique, counts = np.unique(y_test, return_counts=True)\n",
    "        sample_counts = dict(zip(unique, counts))\n",
    "        \n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            feed = {self.tf_inputs: x_batch[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                    self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                    self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "            acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "            print(\"Test Accuracy= {}% for Batch Size {}\".format(np.asscalar(np.asarray(acc_test))*100.0, batch_size_test))\n",
    "            if acc_test < 1.0:\n",
    "                print(\"Erroroneous Samples:\")\n",
    "                incorrect_pred_idx = np.squeeze(np.where(np.squeeze(correct_pred) == False))\n",
    "#                 print(incorrect_pred_idx)\n",
    "#                 print(\"Prediction: {}| y_batch classification: {}| data_len: {}| data_rate: {}\"\\\n",
    "#                       .format(logits_argmax_1[incorrect_pred_idx].T, y_batch[incorrect_pred_idx],\\\n",
    "#                               data_len_batch[incorrect_pred_idx], data_rate_batch[incorrect_pred_idx]))\n",
    "                prediction_errors = np.squeeze(logits_argmax_1[incorrect_pred_idx]).astype(int)\n",
    "                prediction_error_labels = y_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_len = y_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_rate = data_rate_batch[incorrect_pred_idx]  \n",
    "#                 print('{},{}, {}, {}'.format(np.shape(prediction_errors),np.shape(prediction_error_labels),np.shape(prediction_error_data_len),np.shape(prediction_error_data_rate)))\n",
    "                error_map = np.zeros((n_classifications, n_classifications))\n",
    "                for ii in range(len(incorrect_pred_idx)):\n",
    "                    error_map[prediction_error_labels[ii], prediction_errors[ii]] += 1\n",
    "                \n",
    "                for ii in range(n_classifications):\n",
    "                    error_map[:, ii] = error_map[:, ii]/sample_counts[ii]\n",
    "#                 print(error_map)\n",
    "                \n",
    "                signal_list_0 = []\n",
    "                for ii in range(n_classifications):\n",
    "                    signal_list_0.append(data_utils.protocol_and_packet_name(RF_SIGNAL_LIST, ii))\n",
    "#                 print(signal_list_0)\n",
    "                ax = plt.figure(figsize=(10, 10)).gca()\n",
    "                plt.rcParams.update({'font.size': 22})\n",
    "#                 from matplotlib.ticker import MaxNLocator\n",
    "#                 ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#                 ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                plt.imshow(error_map, cmap='viridis', vmin=0.0, vmax=0.5, interpolation='nearest', aspect=\"auto\", origin = 'lower')\n",
    "#                 locs, labels = plt.xticks()\n",
    "                plt.xticks(np.arange(n_classifications), signal_list_0, rotation=90)\n",
    "                plt.yticks(np.arange(n_classifications), signal_list_0)\n",
    "#                 plt.title('Percentage of Incorrect for Each Classification')\n",
    "                plt.xlabel('Misclassification')\n",
    "                plt.ylabel('Expected Classification')\n",
    "                plt.rcParams.update({'font.size': 22})\n",
    "                plt.colorbar()    \n",
    "    def sample_v14(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        self.test_len = np.size(y_test,0) # self.batch_size # \n",
    "        batch_size_test = self.test_len #self.batch_size\n",
    "        \n",
    "        unique, counts = np.unique(y_test, return_counts=True)\n",
    "        sample_counts = dict(zip(unique, counts))\n",
    "        \n",
    "        data_len_unique, data_len_counts = np.unique(data_len_test, return_counts=True)\n",
    "        data_len_dict = dict(zip(data_len_unique, data_len_counts))\n",
    "        print(data_len_dict)\n",
    "        \n",
    "        data_rate_unique, data_rate_counts = np.unique(data_rate_test, return_counts=True)\n",
    "        data_rate_dict = dict(zip(data_rate_unique, data_rate_counts))\n",
    "        print(data_rate_dict)\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            feed = {self.tf_inputs: x_batch[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                    self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                    self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "            acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "            print(\"Test Accuracy= {}% for Batch Size {}\".format(np.asscalar(np.asarray(acc_test))*100.0, batch_size_test))\n",
    "            if acc_test < 1.0:\n",
    "                print(\"Erroroneous Samples:\")\n",
    "                incorrect_pred_idx = np.squeeze(np.where(np.squeeze(correct_pred) == False))\n",
    "#                 print(incorrect_pred_idx)\n",
    "#                 print(\"Prediction: {}| y_batch classification: {}| data_len: {}| data_rate: {}\"\\\n",
    "#                       .format(logits_argmax_1[incorrect_pred_idx].T, y_batch[incorrect_pred_idx],\\\n",
    "#                               data_len_batch[incorrect_pred_idx], data_rate_batch[incorrect_pred_idx]))\n",
    "                prediction_errors = np.squeeze(logits_argmax_1[incorrect_pred_idx]).astype(int)\n",
    "                prediction_error_labels = y_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_len = data_len_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_rate = data_rate_batch[incorrect_pred_idx]  \n",
    "#                 print('{},{}, {}, {}'.format(np.shape(prediction_errors),np.shape(prediction_error_labels),np.shape(prediction_error_data_len),np.shape(prediction_error_data_rate)))\n",
    "\n",
    "                error_data_len = np.zeros(len(data_len_unique))\n",
    "                for ii in range(len(incorrect_pred_idx)):\n",
    "                    error_data_len[list(data_len_unique).index(prediction_error_data_len[ii])] += 1\n",
    "                \n",
    "                for ii in range(len(data_len_unique)):\n",
    "                    error_data_len[ii] = error_data_len[ii]/data_len_counts[ii]\n",
    "                \n",
    "#                 print('data_len_unique:\\n {}'.format(data_len_unique))\n",
    "#                 print('error_data_len:\\n {}'.format(np.round(error_data_len*100)))\n",
    "                \n",
    "#                 plt.figure(figsize=(10, 10)).gca()\n",
    "# #                 plt.rcParams.update({'font.size': 22})\n",
    "#                 plt.bar(data_len_unique, error_data_len, width=0.4, color='blue')\n",
    "#                 plt.xticks(np.log10(data_len_unique), data_len_unique, rotation=90)\n",
    "#                 plt.xlabel('Data Length (Samples)')\n",
    "#                 plt.ylabel('Percent Misclassified')\n",
    "# #                 plt.rcParams.update({'font.size': 22})\n",
    "                    \n",
    "                error_data_rate = np.zeros(len(data_rate_unique))\n",
    "                for ii in range(len(incorrect_pred_idx)):\n",
    "                    error_data_rate[list(data_rate_unique).index(prediction_error_data_rate[ii])] += 1\n",
    "                \n",
    "                for ii in range(1, len(data_rate_unique)):\n",
    "                    error_data_rate[ii] = error_data_rate[ii]/data_rate_counts[ii]\n",
    "\n",
    "                print(data_rate_unique)\n",
    "                print(error_data_rate)\n",
    "                plt.figure(figsize=(10, 5)).gca()\n",
    "                plt.rcParams.update({'font.size': 22})\n",
    "#                 from matplotlib.ticker import MaxNLocator\n",
    "#                 ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#                 ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                plt.bar(data_rate_unique[1:-1], error_data_rate[1:-1], width=2.0)\n",
    "                locs, labels = plt.xticks()\n",
    "#                 plt.xticks(data_rate_unique[9:-1], data_rate_unique[9:-1], rotation=90)\n",
    "#                 plt.xticks(np.arange(n_classifications), signal_list_0, rotation=90)\n",
    "#                 plt.yticks(np.arange(n_classifications), signal_list_0)\n",
    "#                 plt.title('Percentage of Incorrect for Each Classification')                \n",
    "#                 plt.xticks(data_rate_unique[1:-1], data_rate_unique[1:-1], rotation=90)\n",
    "                plt.xlabel('Data Rate (Mbps)')\n",
    "                plt.ylabel('Percent Misclassified')\n",
    "#                 plt.rcParams.update({'font.size': 22})\n",
    "                plt.show()\n",
    "\n",
    "    \n",
    "'''\n",
    "--------------------------\n",
    "Including Instantiation and training here for ease of running\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T01:48:24.752581Z",
     "start_time": "2019-06-22T01:48:24.750292Z"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Data Block Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T07:50:26.953157Z",
     "start_time": "2020-01-24T07:50:25.210820Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: protocolAndPacket| # of Classifications: 28\n",
      "\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_x_train_downsample_rate_1_block_0.p\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_y_train_downsample_rate_1_block_0.p\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_data_len_train_downsample_rate_1_block_0.p\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_data_rate_train_downsample_rate_1_block_0.p\n",
      "x_train_unshuffled: (129600, 7952)\n",
      "y_train_unshuffled: (129600,)\n",
      "data_len_train_unshuffled: (129600,)\n",
      "data_rate_train_unshuffled: (129600,)\n",
      "Random Batch Index: 93842\n",
      "Classification: protocolAndPacket| # of Classifications: 28\n",
      "\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_x_valid_downsample_rate_1_block_0.p\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_y_valid_downsample_rate_1_block_0.p\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_data_len_valid_downsample_rate_1_block_0.p\n",
      "File loaded:/mnt/2ndSSD/802-11_datasets/wlan_enveloped/data/protocolAndPacket_data_rate_valid_downsample_rate_1_block_0.p\n",
      "x_valid_unshuffled: (14400, 7952)\n",
      "y_valid_unshuffled: (14400,)\n",
      "data_len_valid_unshuffled: (14400,)\n",
      "data_rate_valid_unshuffled: (14400,)\n",
      "Random Batch Index: 693\n",
      "x_train shape = (129600, 7952)| y_train shape = (129600,)\n",
      "x_valid shape = (14400, 7952)| y_valid shape = (14400,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Training and Validation data from Data Block 'file_idx'\n",
    "is_verbose = True\n",
    "show_plot = True\n",
    "downsample_rate = 1\n",
    "block = 0\n",
    "\n",
    "dataset_type = 'train'\n",
    "x_train, y_train, data_len_train, data_rate_train,\\\n",
    "= load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "                            block, is_verbose, show_plot)\n",
    "\n",
    "dataset_type = 'valid'\n",
    "x_valid, y_valid, data_len_valid, data_rate_valid,\\\n",
    "= load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "                            block, is_verbose, show_plot)\n",
    "\n",
    "# dataset_type = 'test'\n",
    "# x_test, y_test, data_len_test, data_rate_test,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)\n",
    "# print('x_test shape = {}| y_test shape = {}'.format(x_test.shape, y_test.shape))\n",
    "\n",
    "x_train, y_train, data_len_train, data_rate_train =\\\n",
    "    data_utils.unison_shuffled_copies(x_train, y_train,\\\n",
    "                                      data_len_train, data_rate_train)\n",
    "print('x_train shape = {}| y_train shape = {}'.format(x_train.shape, y_train.shape))\n",
    "\n",
    "x_valid, y_valid, data_len_valid, data_rate_valid =\\\n",
    "    data_utils.unison_shuffled_copies(x_valid, y_valid,\\\n",
    "                                      data_len_valid, data_rate_valid)\n",
    "print('x_valid shape = {}| y_valid shape = {}'.format(x_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Single Set of Hyperparameters - Single Data Block Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T03:02:27.896151Z",
     "start_time": "2020-01-23T02:33:39.964838Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2019-11-22 - File Size: 22.364KB - seq len: 74| lr: 0.000911| dropout: 0.0158| batch_size: 68| logits_scaling: none| scaling_slope: 1.0\n",
    "#    cnn_n_filters: 1| cnn_kernel_size: 10| cnn_pool_size: 2    \n",
    "# 2019-11-22 - File Size: 31.990KB - seq len: 118| lr: 0.003025| dropout: 0.0066| batch_size: 66| logits_scaling: none| scaling_slope: 1.0\n",
    "#     cnn_n_filters: 1| cnn_kernel_size: 10| cnn_pool_size: 1   \n",
    "# 2019-11-22 - File Size: 64.108KB - seq len: 136| lr: 0.003213| dropout: 0.0203| batch_size: 64| logits_scaling: none| scaling_slope: 1.0\n",
    "#     cnn_n_filters: 2| cnn_kernel_size: 10| cnn_pool_size: 1  \n",
    "#  2019-11-22 -File Size: 56.476KB - seq len: 230| lr: 0.001217| dropout: 0.0704| batch_size: 62| logits_scaling: none| scaling_slope: 1.0\n",
    "# 2019-11-22 - File Size: 105.030KB - cnn_n_filters: 1| cnn_kernel_size: 9| cnn_pool_size: 2\n",
    "#     seq len: 450| lr: 0.018581| dropout: 0.0960| batch_size: 68| logits_scaling: none| scaling_slope: 1.0\n",
    "#     cnn_n_filters: 1| cnn_kernel_size: 7| cnn_pool_size: 1  \n",
    "# 2019-12-26 - Stripped Model Size: 243.383KB - ACC: .891 - seq len: 256| lr: 0.000795| dropout: 0.1163| batch_size: 64\n",
    "#     cnn_n_filters: 9| cnn_kernel_size: 9| cnn_pool_size: 10                              \n",
    "#     rnn_type: LSTM| rnn_layers: 1| rnn_len: 78      \n",
    "# 2019-12-26 - Stripped Model Size: 107.289KB - ACC: .881 - seq len: 256| lr: 0.002828| dropout: 0.4331| batch_size: 64\n",
    "#     cnn_n_filters: 2| cnn_kernel_size: 9| cnn_pool_size: 6                                 \n",
    "#     rnn_type: LSTM| rnn_layers: 1| rnn_len: 48\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LENGTH = 256 #int(n_adc_samples/4) # n_adc_samples # 2554 # int(n_adc_samples) # = np.size(x_train_unshuffled, 1)\n",
    "DROPOUT = 0.2250 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "LEARNING_RATE = 0.000375 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "NO_IMPROVEMENT_LIMIT = 10\n",
    "MAX_EPOCHS = 1000\n",
    "PRUNING_START_BATCH = 2000\n",
    "\n",
    "is_training = True\n",
    "nn_type = 'cnn_2layer' #'cnn_1layer' #cnn_2layer #rnn #cnn-rnn\n",
    "\n",
    "cnn_n_filters = 1\n",
    "cnn_kernel_size = 20\n",
    "cnn_pool_size = 5\n",
    "\n",
    "cnn_n_filters_2 = 1\n",
    "cnn_kernel_size_2 = 19\n",
    "cnn_pool_size_2 = 1\n",
    "\n",
    "rnn_type = 'LSTM' # rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "n_rnn_stack = 1 # n_layers_list = [1,3,5,7]\n",
    "rnn_len = 32 # rnn_len_list = [128, 256, 512, 1024]\n",
    "\n",
    "optimization_parameter = 'loss'\n",
    "\n",
    "nn_v19 = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH,\\\n",
    "            CLASSIFICATION_TYPE, N_CLASSIFICATIONS, DROPOUT, LEARNING_RATE,\\\n",
    "            is_training, nn_type, MAX_EPOCHS,\\\n",
    "            cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "            cnn_n_filters_2, cnn_kernel_size_2, cnn_pool_size_2,\\\n",
    "            rnn_type, n_rnn_stack, rnn_len)\n",
    "\n",
    "if nn_type == 'cnn_1layer':\n",
    "    nn_v19.build_cnn_1layer_model_v00()\n",
    "elif nn_type == 'cnn_2layer':\n",
    "    nn_v19.build_cnn_2layer_model_v00()\n",
    "elif nn_type == 'rnn':\n",
    "    nn_v19.build_rnn_model_v00()\n",
    "elif nn_type == 'cnn-rnn':\n",
    "    nn_v19.build_cnn_to_rnn_model_v00()\n",
    "else:\n",
    "   raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "\n",
    "keras_verbose = False\n",
    "batch_size_limit = int(np.size(y_train, 0))\n",
    "print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "training_history, file_bytes  =\\\n",
    "    nn_v19.train_v00(N_DATASET_BLOCKS, optimization_parameter,\\\n",
    "        NO_IMPROVEMENT_LIMIT, x_train[0:batch_size_limit, PADDING:PADDING+SEQUENCE_LENGTH],\\\n",
    "        y_train[0:batch_size_limit], data_len_train[0:batch_size_limit],\\\n",
    "        data_rate_train[0:batch_size_limit], x_valid[:, PADDING:PADDING+SEQUENCE_LENGTH],\\\n",
    "        y_valid, data_len_valid, data_rate_valid, keras_verbose)\n",
    "\n",
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "del nn_v19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Single Set of Hyperparameters - Continuous Data Block Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:46.180108Z",
     "start_time": "2019-11-05T19:09:46.172280Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2019-06-28-0116: 'loss': 0.0927734375, 'epochs': 5, 'metrics': {'accuracy': 0.90722656}}\n",
    "# # TPE best: {'BATCH_SIZE': 56.0, 'DROPOUT': 0.3841959763303179, 'LEARNING_RATE': 0.0021651506653211235, 'cnn_kernel_size': 5.0, 'cnn_n_filters': 16.0, 'cnn_pool_size': 2.0, 'seq_len': 2084.0}\n",
    "# #     CNN| Seq Len: 852| LR: 0.003010| DO: 0.2582| b_size: 48| logits_scaling: none| scaling_slope: 1.0| cnn_n_filters: 5| cnn_kernel_size: 9| cnn_pool_size: 1| cnn_n_filters_2: 5| cnn_kernel_size_2: 3| cnn_pool_size_2: 2\n",
    "# BATCH_SIZE = 64\n",
    "# SEQUENCE_LENGTH = 852 #int(n_adc_samples/4) # n_adc_samples # 2554 # int(n_adc_samples) # = np.size(x_train_unshuffled, 1)\n",
    "# NO_IMPROVEMENT_LIMIT = 5 # N_EPOCHS without improvement\n",
    "# DROPOUT = 0.2582 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "# LEARNING_RATE = 0.003010 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# logits_scaling = 'none' # logits_scaling_list['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# scaling_slope = 1.0 # scaling_slope_list[1, 2, 3, 5, 7, 11]\n",
    "# is_training = True\n",
    "# nn_type = 'cnn-rnn' #cnn_2layer #rnn #cnn-rnn\n",
    "\n",
    "# cnn_n_filters = 9\n",
    "# cnn_kernel_size = 5\n",
    "# cnn_pool_size = 2\n",
    "\n",
    "# cnn_n_filters_2 = 5\n",
    "# cnn_kernel_size_2 = 7\n",
    "# cnn_pool_size_2 = 2\n",
    "\n",
    "# rnn_type = 'LSTM' # rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "# n_rnn_stack = 1 # n_layers_list = [1,3,5,7]\n",
    "# rnn_len = 32 # rnn_len_list = [128, 256, 512, 1024]\n",
    "\n",
    "# optimization_parameter = 'loss'\n",
    "# nn_v19 = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH,\\\n",
    "#             CLASSIFICATION_TYPE, N_CLASSIFICATIONS, DROPOUT, LEARNING_RATE,\\\n",
    "#             is_training, nn_type, MAX_EPOCHS,\n",
    "#             cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "#             cnn_n_filters_2, cnn_kernel_size_2, cnn_pool_size_2,\\\n",
    "#             rnn_type, n_rnn_stack, rnn_len)\n",
    "\n",
    "# if nn_type == 'cnn_1layer':\n",
    "#     nn_v19.build_cnn_1layer_model_v00()\n",
    "# elif nn_type == 'cnn_2layer':\n",
    "#     nn_v19.build_cnn_2layer_model_v00()\n",
    "# elif nn_type == 'rnn':\n",
    "#     nn_v19.build_rnn_model_v00()\n",
    "# elif nn_type == 'cnn-rnn':\n",
    "#     nn_v19.build_cnn_to_rnn_model_v00()\n",
    "# else:\n",
    "#    raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "\n",
    "# batch_size_limit = int(np.size(y_train, 0))\n",
    "# print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "# loss, accuracy, epochs, file_bytes  = nn_v19.train_v01(N_DATASET_BLOCKS, optimization_parameter, NO_IMPROVEMENT_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt - Single Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T11:46:15.367038Z",
     "start_time": "2020-01-24T07:52:17.508027Z"
    },
    "code_folding": [
     44,
     66,
     95
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    **********************************************    \n",
      "\n",
      "Hyperopt Settings:                                   \n",
      "    seq len: 354| lr: 0.000113| dropout: 0.0268| batch_size: 80\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 95| cnn_pool_size: 5\n",
      "    cnn_n_filters_2: 1| cnn_kernel_size_2: 40| cnn_pool_size_2: 8\n",
      "Model: \"sequential_1\"                                \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 260, 2)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 130, 2)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 130, 2)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 91, 1)             81        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 46, 1)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 46, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                1316      \n",
      "=================================================================\n",
      "Total params: 1,589                                  \n",
      "Trainable params: 1,589                              \n",
      "Non-trainable params: 0                              \n",
      "_________________________________________________________________\n",
      "Training Block Batch Count: 129600                   \n",
      ">> Training Settings:                                \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28\n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 354| LR: 0.000113| DO: 0.0268| b_size: 80\n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 95| cnn_pool_size: 5\n",
      ">> cnn_n_filters_2: 1| cnn_kernel_size_2: 40| cnn_pool_size_2: 8\n",
      "    Tensorboard Logs Directory:                      \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                     \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 50| Loss: 1.147| Acc: 0.630 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 100| Loss: 0.801| Acc: 0.746 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 150| Loss: 0.654| Acc: 0.800 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 200| Loss: 0.569| Acc: 0.826 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 250| Loss: 0.511| Acc: 0.838 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 300| Loss: 0.470| Acc: 0.845 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 350| Loss: 0.437| Acc: 0.853 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 400| Loss: 0.413| Acc: 0.858 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 450| Loss: 0.393| Acc: 0.862 ---\n",
      "\n",
      "                                                     \n",
      "--- Training Epoch: 500| Loss: 0.376| Acc: 0.866 ---\n",
      "\n",
      "                                                     \n",
      "Stripped Model Size: 26.078KB\n",
      "Final Weights Save File:                             \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                               \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                     \n",
      "Final Training Epoch: 500| Loss: 0.376| Acc: 0.866\n",
      "\n",
      "\n",
      "!!! Still Training! Weighted Loss = 0.09807876073633878\n",
      "    **********************************************                                  \n",
      "\n",
      "Hyperopt Settings:                                                                  \n",
      "    seq len: 306| lr: 0.000116| dropout: 0.2856| batch_size: 80\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 99| cnn_pool_size: 8                         \n",
      "    cnn_n_filters_2: 3| cnn_kernel_size_2: 81| cnn_pool_size_2: 7                   \n",
      "Model: \"sequential\"                                                                 \n",
      "_________________________________________________________________                   \n",
      "Layer (type)                 Output Shape              Param #                      \n",
      "=================================================================                   \n",
      "conv1d (Conv1D)              (None, 208, 2)            200                          \n",
      "_________________________________________________________________                   \n",
      "max_pooling1d (MaxPooling1D) (None, 104, 2)            0                            \n",
      "_________________________________________________________________                   \n",
      "dropout (Dropout)            (None, 104, 2)            0                            \n",
      "_________________________________________________________________                   \n",
      "conv1d_1 (Conv1D)            (None, 24, 3)             489                          \n",
      "_________________________________________________________________                   \n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 3)             0                            \n",
      "_________________________________________________________________                   \n",
      "dropout_1 (Dropout)          (None, 12, 3)             0                            \n",
      "_________________________________________________________________                   \n",
      "flatten (Flatten)            (None, 36)                0                            \n",
      "_________________________________________________________________                   \n",
      "dense (Dense)                (None, 28)                1036                         \n",
      "=================================================================                   \n",
      "Total params: 1,725                                                                 \n",
      "Trainable params: 1,725                                                             \n",
      "Non-trainable params: 0                                                             \n",
      "_________________________________________________________________                   \n",
      "Training Block Batch Count: 129600                                                  \n",
      ">> Training Settings:                                                               \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                      \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 306| LR: 0.000116| DO: 0.2856| b_size: 80          \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 99| cnn_pool_size: 8                          \n",
      ">> cnn_n_filters_2: 3| cnn_kernel_size_2: 81| cnn_pool_size_2: 7                    \n",
      "    Tensorboard Logs Directory:                                                     \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                    \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 50| Loss: 1.597| Acc: 0.459 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 100| Loss: 1.278| Acc: 0.602 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 150| Loss: 1.063| Acc: 0.665 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 200| Loss: 0.910| Acc: 0.725 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 250| Loss: 0.783| Acc: 0.776 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 300| Loss: 0.697| Acc: 0.807 ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    \n",
      "--- Training Epoch: 350| Loss: 0.640| Acc: 0.822 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 400| Loss: 0.600| Acc: 0.828 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 450| Loss: 0.571| Acc: 0.833 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 500| Loss: 0.547| Acc: 0.836 ---\n",
      "\n",
      "                                                                                    \n",
      "Stripped Model Size: 26.977KB\n",
      "Final Weights Save File:                                                            \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                    \n",
      "Final Training Epoch: 500| Loss: 0.547| Acc: 0.836\n",
      "\n",
      "\n",
      "!!! Still Training! Weighted Loss = 0.1476421206145142                              \n",
      "    **********************************************                                  \n",
      "\n",
      "Hyperopt Settings:                                                                  \n",
      "    seq len: 196| lr: 0.003693| dropout: 0.2730| batch_size: 78\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 10| cnn_pool_size: 4                         \n",
      "    cnn_n_filters_2: 3| cnn_kernel_size_2: 50| cnn_pool_size_2: 4                   \n",
      "Model: \"sequential\"                                                                 \n",
      "_________________________________________________________________                   \n",
      "Layer (type)                 Output Shape              Param #                      \n",
      "=================================================================                   \n",
      "conv1d (Conv1D)              (None, 187, 2)            22                           \n",
      "_________________________________________________________________                   \n",
      "max_pooling1d (MaxPooling1D) (None, 94, 2)             0                            \n",
      "_________________________________________________________________                   \n",
      "dropout (Dropout)            (None, 94, 2)             0                            \n",
      "_________________________________________________________________                   \n",
      "conv1d_1 (Conv1D)            (None, 45, 3)             303                          \n",
      "_________________________________________________________________                   \n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 3)             0                            \n",
      "_________________________________________________________________                   \n",
      "dropout_1 (Dropout)          (None, 23, 3)             0                            \n",
      "_________________________________________________________________                   \n",
      "flatten (Flatten)            (None, 69)                0                            \n",
      "_________________________________________________________________                   \n",
      "dense (Dense)                (None, 28)                1960                         \n",
      "=================================================================                   \n",
      "Total params: 2,285                                                                 \n",
      "Trainable params: 2,285                                                             \n",
      "Non-trainable params: 0                                                             \n",
      "_________________________________________________________________                   \n",
      "Training Block Batch Count: 129600                                                  \n",
      ">> Training Settings:                                                               \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                      \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 196| LR: 0.003693| DO: 0.2730| b_size: 78          \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 10| cnn_pool_size: 4                          \n",
      ">> cnn_n_filters_2: 3| cnn_kernel_size_2: 50| cnn_pool_size_2: 4                    \n",
      "    Tensorboard Logs Directory:                                                     \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                    \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 50| Loss: 0.599| Acc: 0.813 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 100| Loss: 0.493| Acc: 0.843 ---\n",
      "\n",
      "                                                                                    \n",
      "--- Training Epoch: 150| Loss: 0.449| Acc: 0.851 ---\n",
      "\n",
      "                                                                                    \n",
      "Stripped Model Size: 28.586KB\n",
      "Final Weights Save File:                                                            \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                    \n",
      "Final Training Epoch: 191| Loss: 0.429| Acc: 0.854\n",
      "\n",
      "\n",
      "    **********************************************                                  \n",
      "\n",
      "Hyperopt Settings:                                                                 \n",
      "    seq len: 716| lr: 0.000019| dropout: 0.1726| batch_size: 36\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 69| cnn_pool_size: 6                        \n",
      "    cnn_n_filters_2: 3| cnn_kernel_size_2: 47| cnn_pool_size_2: 2                  \n",
      "Model: \"sequential\"                                                                \n",
      "_________________________________________________________________                  \n",
      "Layer (type)                 Output Shape              Param #                     \n",
      "=================================================================                  \n",
      "conv1d (Conv1D)              (None, 648, 2)            140                         \n",
      "_________________________________________________________________                  \n",
      "max_pooling1d (MaxPooling1D) (None, 324, 2)            0                           \n",
      "_________________________________________________________________                  \n",
      "dropout (Dropout)            (None, 324, 2)            0                           \n",
      "_________________________________________________________________                  \n",
      "conv1d_1 (Conv1D)            (None, 278, 3)            285                         \n",
      "_________________________________________________________________                  \n",
      "max_pooling1d_1 (MaxPooling1 (None, 139, 3)            0                           \n",
      "_________________________________________________________________                  \n",
      "dropout_1 (Dropout)          (None, 139, 3)            0                           \n",
      "_________________________________________________________________                  \n",
      "flatten (Flatten)            (None, 417)               0                           \n",
      "_________________________________________________________________                  \n",
      "dense (Dense)                (None, 28)                11704                       \n",
      "=================================================================                  \n",
      "Total params: 12,129                                                               \n",
      "Trainable params: 12,129                                                           \n",
      "Non-trainable params: 0                                                            \n",
      "_________________________________________________________________                  \n",
      "Training Block Batch Count: 129600                                                 \n",
      ">> Training Settings:                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Classification: protocolAndPacket| # of Classifications: 28                     \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 716| LR: 0.000019| DO: 0.1726| b_size: 36         \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 69| cnn_pool_size: 6                         \n",
      ">> cnn_n_filters_2: 3| cnn_kernel_size_2: 47| cnn_pool_size_2: 2                   \n",
      "    Tensorboard Logs Directory:                                                    \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                   \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                   \n",
      "--- Training Epoch: 50| Loss: 1.352| Acc: 0.558 ---\n",
      "\n",
      "                                                                                   \n",
      "--- Training Epoch: 100| Loss: 0.892| Acc: 0.702 ---\n",
      "\n",
      "                                                                                   \n",
      "--- Training Epoch: 150| Loss: 0.637| Acc: 0.778 ---\n",
      "\n",
      "                                                                                   \n",
      "--- Training Epoch: 200| Loss: 0.514| Acc: 0.817 ---\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 250| Loss: 0.442| Acc: 0.838 ---\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 300| Loss: 0.394| Acc: 0.849 ---\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 350| Loss: 0.362| Acc: 0.858 ---\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 400| Loss: 0.338| Acc: 0.865 ---\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 450| Loss: 0.320| Acc: 0.871 ---\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 500| Loss: 0.306| Acc: 0.875 ---\n",
      "\n",
      "                                                                                     \n",
      "Stripped Model Size: 66.648KB\n",
      "Final Weights Save File:                                                             \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                               \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                     \n",
      "Final Training Epoch: 500| Loss: 0.306| Acc: 0.875\n",
      "\n",
      "\n",
      "!!! Still Training! Weighted Loss = 0.20392444094491027                              \n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 298| lr: 0.000322| dropout: 0.3405| batch_size: 58\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 28| cnn_pool_size: 7                           \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 70| cnn_pool_size_2: 7                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 271, 2)            58                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 136, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 136, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "conv1d_1 (Conv1D)            (None, 67, 2)             282                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 34, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 34, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 68)                0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                1932                           \n",
      "=================================================================                     \n",
      "Total params: 2,272                                                                   \n",
      "Trainable params: 2,272                                                               \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 298| LR: 0.000322| DO: 0.3405| b_size: 58            \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 28| cnn_pool_size: 7                            \n",
      ">> cnn_n_filters_2: 2| cnn_kernel_size_2: 70| cnn_pool_size_2: 7                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 50| Loss: 0.865| Acc: 0.747 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 100| Loss: 0.585| Acc: 0.827 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 150| Loss: 0.492| Acc: 0.846 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 200| Loss: 0.440| Acc: 0.853 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 250| Loss: 0.411| Acc: 0.858 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 300| Loss: 0.391| Acc: 0.861 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 350| Loss: 0.378| Acc: 0.863 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 400| Loss: 0.369| Acc: 0.865 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 450| Loss: 0.361| Acc: 0.867 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 500| Loss: 0.355| Acc: 0.868 ---\n",
      "\n",
      "                                                                                      \n",
      "Stripped Model Size: 28.477KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 500| Loss: 0.355| Acc: 0.868\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Still Training! Weighted Loss = 0.10098743395786733                               \n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 168| lr: 0.017876| dropout: 0.4645| batch_size: 100\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 30| cnn_pool_size: 4                           \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 25| cnn_pool_size_2: 7                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 139, 2)            62                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 70, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 70, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "conv1d_1 (Conv1D)            (None, 46, 2)             102                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 23, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 46)                0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                1316                           \n",
      "=================================================================                     \n",
      "Total params: 1,480                                                                   \n",
      "Trainable params: 1,480                                                               \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 168| LR: 0.017876| DO: 0.4645| b_size: 100           \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 30| cnn_pool_size: 4                            \n",
      ">> cnn_n_filters_2: 2| cnn_kernel_size_2: 25| cnn_pool_size_2: 7                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "Stripped Model Size: 26.070KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 6| Loss: 3.036| Acc: 0.250\n",
      "\n",
      "\n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 654| lr: 0.000051| dropout: 0.4348| batch_size: 82\n",
      "    cnn_n_filters: 3| cnn_kernel_size: 26| cnn_pool_size: 9                           \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 99| cnn_pool_size_2: 5                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 629, 3)            81                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 315, 3)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 315, 3)            0                              \n",
      "_________________________________________________________________                     \n",
      "conv1d_1 (Conv1D)            (None, 217, 2)            596                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 109, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 109, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 218)               0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                6132                           \n",
      "=================================================================                     \n",
      "Total params: 6,809                                                                   \n",
      "Trainable params: 6,809                                                               \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 654| LR: 0.000051| DO: 0.4348| b_size: 82            \n",
      ">> cnn_n_filters: 3| cnn_kernel_size: 26| cnn_pool_size: 9                            \n",
      ">> cnn_n_filters_2: 2| cnn_kernel_size_2: 99| cnn_pool_size_2: 5                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 50| Loss: 0.939| Acc: 0.691 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 100| Loss: 0.537| Acc: 0.807 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 150| Loss: 0.436| Acc: 0.833 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 200| Loss: 0.387| Acc: 0.840 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 250| Loss: 0.364| Acc: 0.846 ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      \n",
      "--- Training Epoch: 300| Loss: 0.349| Acc: 0.851 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 350| Loss: 0.338| Acc: 0.856 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 400| Loss: 0.328| Acc: 0.861 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 450| Loss: 0.320| Acc: 0.865 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 500| Loss: 0.313| Acc: 0.868 ---\n",
      "\n",
      "                                                                                      \n",
      "Stripped Model Size: 47.203KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 500| Loss: 0.313| Acc: 0.868\n",
      "\n",
      "\n",
      "!!! Still Training! Weighted Loss = 0.1475164181742357                                \n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 168| lr: 0.000071| dropout: 0.3281| batch_size: 34\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 15| cnn_pool_size: 5                           \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 35| cnn_pool_size_2: 2                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 154, 2)            32                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 77, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 77, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "conv1d_1 (Conv1D)            (None, 43, 2)             142                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 22, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 22, 2)             0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 44)                0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                1260                           \n",
      "=================================================================                     \n",
      "Total params: 1,434                                                                   \n",
      "Trainable params: 1,434                                                               \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 168| LR: 0.000071| DO: 0.3281| b_size: 34            \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 15| cnn_pool_size: 5                            \n",
      ">> cnn_n_filters_2: 2| cnn_kernel_size_2: 35| cnn_pool_size_2: 2                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 50| Loss: 1.774| Acc: 0.421 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 100| Loss: 1.595| Acc: 0.487 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 150| Loss: 1.513| Acc: 0.527 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 200| Loss: 1.453| Acc: 0.557 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 250| Loss: 1.402| Acc: 0.567 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 300| Loss: 1.356| Acc: 0.581 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 350| Loss: 1.319| Acc: 0.592 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 400| Loss: 1.290| Acc: 0.593 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 450| Loss: 1.265| Acc: 0.600 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 500| Loss: 1.243| Acc: 0.609 ---\n",
      "\n",
      "                                                                                      \n",
      "Stripped Model Size: 25.852KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 500| Loss: 1.243| Acc: 0.609\n",
      "\n",
      "\n",
      "!!! Still Training! Weighted Loss = 0.321327067435466                                 \n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 546| lr: 0.000059| dropout: 0.0288| batch_size: 114\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 26| cnn_pool_size: 9                           \n",
      "    cnn_n_filters_2: 3| cnn_kernel_size_2: 62| cnn_pool_size_2: 6                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 521, 2)            54                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 261, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 261, 2)            0                              \n",
      "_________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d_1 (Conv1D)            (None, 200, 3)            375                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 3)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 100, 3)            0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 300)               0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                8428                           \n",
      "=================================================================                     \n",
      "Total params: 8,857                                                                   \n",
      "Trainable params: 8,857                                                               \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 546| LR: 0.000059| DO: 0.0288| b_size: 114           \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 26| cnn_pool_size: 9                            \n",
      ">> cnn_n_filters_2: 3| cnn_kernel_size_2: 62| cnn_pool_size_2: 6                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 50| Loss: 1.191| Acc: 0.592 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 100| Loss: 0.558| Acc: 0.815 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 150| Loss: 0.381| Acc: 0.853 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 200| Loss: 0.320| Acc: 0.874 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 250| Loss: 0.288| Acc: 0.884 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 300| Loss: 0.267| Acc: 0.890 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 350| Loss: 0.252| Acc: 0.895 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 400| Loss: 0.240| Acc: 0.899 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 450| Loss: 0.231| Acc: 0.902 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 500| Loss: 0.223| Acc: 0.905 ---\n",
      "\n",
      "                                                                                      \n",
      "Stripped Model Size: 53.852KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 500| Loss: 0.223| Acc: 0.905\n",
      "\n",
      "\n",
      "!!! Still Training! Weighted Loss = 0.12001044099994836                               \n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 288| lr: 0.003581| dropout: 0.3398| batch_size: 86\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 26| cnn_pool_size: 10                          \n",
      "    cnn_n_filters_2: 1| cnn_kernel_size_2: 48| cnn_pool_size_2: 8                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 263, 2)            54                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 132, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 132, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "conv1d_1 (Conv1D)            (None, 85, 1)             97                             \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 43, 1)             0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 43, 1)             0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 43)                0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                1232                           \n",
      "=================================================================                     \n",
      "Total params: 1,383                                                                   \n",
      "Trainable params: 1,383                                                               \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 288| LR: 0.003581| DO: 0.3398| b_size: 86            \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 26| cnn_pool_size: 10                           \n",
      ">> cnn_n_filters_2: 1| cnn_kernel_size_2: 48| cnn_pool_size_2: 8                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "Stripped Model Size: 25.742KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 12| Loss: 3.035| Acc: 0.250\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    **********************************************                                     \n",
      "\n",
      "Hyperopt Settings:                                                                     \n",
      "    seq len: 408| lr: 0.000212| dropout: 0.0895| batch_size: 76\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 4| cnn_pool_size: 9                             \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 25| cnn_pool_size_2: 2                      \n",
      "Model: \"sequential\"                                                                    \n",
      "_________________________________________________________________                      \n",
      "Layer (type)                 Output Shape              Param #                         \n",
      "=================================================================                      \n",
      "conv1d (Conv1D)              (None, 405, 2)            10                              \n",
      "_________________________________________________________________                      \n",
      "max_pooling1d (MaxPooling1D) (None, 203, 2)            0                               \n",
      "_________________________________________________________________                      \n",
      "dropout (Dropout)            (None, 203, 2)            0                               \n",
      "_________________________________________________________________                      \n",
      "conv1d_1 (Conv1D)            (None, 179, 2)            102                             \n",
      "_________________________________________________________________                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 90, 2)             0                               \n",
      "_________________________________________________________________                      \n",
      "dropout_1 (Dropout)          (None, 90, 2)             0                               \n",
      "_________________________________________________________________                      \n",
      "flatten (Flatten)            (None, 180)               0                               \n",
      "_________________________________________________________________                      \n",
      "dense (Dense)                (None, 28)                5068                            \n",
      "=================================================================                      \n",
      "Total params: 5,180                                                                    \n",
      "Trainable params: 5,180                                                                \n",
      "Non-trainable params: 0                                                                \n",
      "_________________________________________________________________                      \n",
      "Training Block Batch Count: 129600                                                     \n",
      ">> Training Settings:                                                                  \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                         \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 408| LR: 0.000212| DO: 0.0895| b_size: 76             \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 4| cnn_pool_size: 9                              \n",
      ">> cnn_n_filters_2: 2| cnn_kernel_size_2: 25| cnn_pool_size_2: 2                       \n",
      "    Tensorboard Logs Directory:                                                        \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                       \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                       \n",
      "Stripped Model Size: 40.727KB\n",
      "Final Weights Save File:                                                               \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                 \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                       \n",
      "Final Training Epoch: 28| Loss: 3.034| Acc: 0.250\n",
      "\n",
      "\n",
      "    **********************************************                                     \n",
      "\n",
      "Hyperopt Settings:                                                                    \n",
      "    seq len: 662| lr: 0.000544| dropout: 0.4421| batch_size: 52\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 66| cnn_pool_size: 8                           \n",
      "    cnn_n_filters_2: 3| cnn_kernel_size_2: 67| cnn_pool_size_2: 6                     \n",
      "Model: \"sequential\"                                                                   \n",
      "_________________________________________________________________                     \n",
      "Layer (type)                 Output Shape              Param #                        \n",
      "=================================================================                     \n",
      "conv1d (Conv1D)              (None, 597, 2)            134                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d (MaxPooling1D) (None, 299, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout (Dropout)            (None, 299, 2)            0                              \n",
      "_________________________________________________________________                     \n",
      "conv1d_1 (Conv1D)            (None, 233, 3)            405                            \n",
      "_________________________________________________________________                     \n",
      "max_pooling1d_1 (MaxPooling1 (None, 117, 3)            0                              \n",
      "_________________________________________________________________                     \n",
      "dropout_1 (Dropout)          (None, 117, 3)            0                              \n",
      "_________________________________________________________________                     \n",
      "flatten (Flatten)            (None, 351)               0                              \n",
      "_________________________________________________________________                     \n",
      "dense (Dense)                (None, 28)                9856                           \n",
      "=================================================================                     \n",
      "Total params: 10,395                                                                  \n",
      "Trainable params: 10,395                                                              \n",
      "Non-trainable params: 0                                                               \n",
      "_________________________________________________________________                     \n",
      "Training Block Batch Count: 129600                                                    \n",
      ">> Training Settings:                                                                 \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                        \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 662| LR: 0.000544| DO: 0.4421| b_size: 52            \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 66| cnn_pool_size: 8                            \n",
      ">> cnn_n_filters_2: 3| cnn_kernel_size_2: 67| cnn_pool_size_2: 6                      \n",
      "    Tensorboard Logs Directory:                                                       \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                      \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 50| Loss: 0.203| Acc: 0.914 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 100| Loss: 0.167| Acc: 0.928 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 150| Loss: 0.150| Acc: 0.936 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 200| Loss: 0.141| Acc: 0.939 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 250| Loss: 0.134| Acc: 0.942 ---\n",
      "\n",
      "                                                                                      \n",
      "--- Training Epoch: 300| Loss: 0.129| Acc: 0.944 ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      \n",
      "Stripped Model Size: 61.430KB\n",
      "Final Weights Save File:                                                              \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                                \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                      \n",
      "Final Training Epoch: 335| Loss: 0.126| Acc: 0.946\n",
      "\n",
      "\n",
      "    **********************************************                                    \n",
      "\n",
      "Hyperopt Settings:                                                                   \n",
      "    seq len: 590| lr: 0.017497| dropout: 0.2600| batch_size: 48\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 14| cnn_pool_size: 7                          \n",
      "    cnn_n_filters_2: 3| cnn_kernel_size_2: 52| cnn_pool_size_2: 5                    \n",
      "Model: \"sequential\"                                                                  \n",
      "_________________________________________________________________                    \n",
      "Layer (type)                 Output Shape              Param #                       \n",
      "=================================================================                    \n",
      "conv1d (Conv1D)              (None, 577, 2)            30                            \n",
      "_________________________________________________________________                    \n",
      "max_pooling1d (MaxPooling1D) (None, 289, 2)            0                             \n",
      "_________________________________________________________________                    \n",
      "dropout (Dropout)            (None, 289, 2)            0                             \n",
      "_________________________________________________________________                    \n",
      "conv1d_1 (Conv1D)            (None, 238, 3)            315                           \n",
      "_________________________________________________________________                    \n",
      "max_pooling1d_1 (MaxPooling1 (None, 119, 3)            0                             \n",
      "_________________________________________________________________                    \n",
      "dropout_1 (Dropout)          (None, 119, 3)            0                             \n",
      "_________________________________________________________________                    \n",
      "flatten (Flatten)            (None, 357)               0                             \n",
      "_________________________________________________________________                    \n",
      "dense (Dense)                (None, 28)                10024                         \n",
      "=================================================================                    \n",
      "Total params: 10,369                                                                 \n",
      "Trainable params: 10,369                                                             \n",
      "Non-trainable params: 0                                                              \n",
      "_________________________________________________________________                    \n",
      "Training Block Batch Count: 129600                                                   \n",
      ">> Training Settings:                                                                \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                       \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 590| LR: 0.017497| DO: 0.2600| b_size: 48           \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 14| cnn_pool_size: 7                           \n",
      ">> cnn_n_filters_2: 3| cnn_kernel_size_2: 52| cnn_pool_size_2: 5                     \n",
      "    Tensorboard Logs Directory:                                                      \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                     \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 50| Loss: 0.254| Acc: 0.897 ---\n",
      "\n",
      "                                                                                     \n",
      "Stripped Model Size: 60.086KB\n",
      "Final Weights Save File:                                                             \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                               \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                     \n",
      "Final Training Epoch: 73| Loss: 0.244| Acc: 0.899\n",
      "\n",
      "\n",
      "    **********************************************                                   \n",
      "\n",
      "Hyperopt Settings:                                                                   \n",
      "    seq len: 564| lr: 0.001726| dropout: 0.1851| batch_size: 82\n",
      "    cnn_n_filters: 2| cnn_kernel_size: 91| cnn_pool_size: 2                          \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 93| cnn_pool_size_2: 4                    \n",
      "Model: \"sequential\"                                                                  \n",
      "_________________________________________________________________                    \n",
      "Layer (type)                 Output Shape              Param #                       \n",
      "=================================================================                    \n",
      "conv1d (Conv1D)              (None, 474, 2)            184                           \n",
      "_________________________________________________________________                    \n",
      "max_pooling1d (MaxPooling1D) (None, 237, 2)            0                             \n",
      "_________________________________________________________________                    \n",
      "dropout (Dropout)            (None, 237, 2)            0                             \n",
      "_________________________________________________________________                    \n",
      "conv1d_1 (Conv1D)            (None, 145, 2)            374                           \n",
      "_________________________________________________________________                    \n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 2)             0                             \n",
      "_________________________________________________________________                    \n",
      "dropout_1 (Dropout)          (None, 73, 2)             0                             \n",
      "_________________________________________________________________                    \n",
      "flatten (Flatten)            (None, 146)               0                             \n",
      "_________________________________________________________________                    \n",
      "dense (Dense)                (None, 28)                4116                          \n",
      "=================================================================                    \n",
      "Total params: 4,674                                                                  \n",
      "Trainable params: 4,674                                                              \n",
      "Non-trainable params: 0                                                              \n",
      "_________________________________________________________________                    \n",
      "Training Block Batch Count: 129600                                                   \n",
      ">> Training Settings:                                                                \n",
      ">> Classification: protocolAndPacket| # of Classifications: 28                       \n",
      "\n",
      ">> NN Type: cnn_2layer| Seq Len: 564| LR: 0.001726| DO: 0.1851| b_size: 82           \n",
      ">> cnn_n_filters: 2| cnn_kernel_size: 91| cnn_pool_size: 2                           \n",
      ">> cnn_n_filters_2: 2| cnn_kernel_size_2: 93| cnn_pool_size_2: 4                     \n",
      "    Tensorboard Logs Directory:                                                      \n",
      "        /mnt/2ndSSD/802-11_datasets/wlan_enveloped/logs_nn_classification_v19/run-20200124075012/train\n",
      "                                                                                     \n",
      "---------------------------------------------------------------\n",
      "\n",
      "                                                                                     \n",
      "--- Training Epoch: 50| Loss: 0.149| Acc: 0.935 ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                     \n",
      "--- Training Epoch: 100| Loss: 0.126| Acc: 0.946 ---\n",
      "\n",
      "                                                                                     \n",
      "Stripped Model Size: 39.008KB\n",
      "Final Weights Save File:                                                             \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_weights.h5\n",
      "Final Model Save File:                                                               \n",
      "  /mnt/2ndSSD/802-11_datasets/wlan_enveloped/nn_classification_v19_final_model.h5\n",
      "                                                                                     \n",
      "Final Training Epoch: 135| Loss: 0.117| Acc: 0.950\n",
      "\n",
      "\n",
      "    **********************************************                                     \n",
      "\n",
      "Hyperopt Settings:                                                                     \n",
      "    seq len: 166| lr: 0.000041| dropout: 0.4109| batch_size: 50\n",
      "    cnn_n_filters: 3| cnn_kernel_size: 21| cnn_pool_size: 2                            \n",
      "    cnn_n_filters_2: 2| cnn_kernel_size_2: 87| cnn_pool_size_2: 8                      \n",
      " 14%|â–ˆâ–        | 14/100 [3:53:57<23:57:12, 1002.70s/it, best loss: 0.045535583245302755]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 87 from 73 for 'conv1d_1/conv1d' (op: 'Conv2D') with input shapes: [?,1,73,3], [1,87,3,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1864\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 87 from 73 for 'conv1d_1/conv1d' (op: 'Conv2D') with input shapes: [?,1,73,3], [1,87,3,2].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-578f8da01c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_MODEL_HP_VARIATIONS_TO_TEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TPE best: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    420\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    421\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 856\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-578f8da01c1e>\u001b[0m in \u001b[0;36moptimizer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mnn_v19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_cnn_1layer_model_v00\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cnn_2layer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mnn_v19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_cnn_2layer_model_v00\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rnn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mnn_v19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_rnn_model_v00\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-75d408941ba0>\u001b[0m in \u001b[0;36mbuild_cnn_2layer_model_v00\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 ])\n\u001b[1;32m    276\u001b[0m                 self.adam_optimizer = tf.keras.optimizers.Adam(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'causal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv1d\u001b[0;34m(self, input, filter, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;31m# pylint: enable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name, input, dilations)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1625\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspatial_start_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m   1072\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3616\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 2027\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 87 from 73 for 'conv1d_1/conv1d' (op: 'Conv2D') with input shapes: [?,1,73,3], [1,87,3,2]."
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "n_adc_samples = 852 #N_ADC_SAMPLES\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LENGTH = 256 # n_adc_samples # = np.size(x_train_unshuffled, 1)\n",
    "EPOCHS = 5\n",
    "DROPOUT = 0.225 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "LEARNING_RATE = 0.000375 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "CNN_N_FILTERS  = 1\n",
    "CNN_N_FILTERS_2 = 1\n",
    "\n",
    "rnn_type = 'LSTM' # \n",
    "rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "\n",
    "is_training = True\n",
    "\n",
    "NN_TYPE = 'cnn_2layer'#'cnn-rnn' #'cnn_1layer' #cnn_2layer #rnn\n",
    "\n",
    "NO_IMPROVEMENT_LIMIT = 5\n",
    "MAX_EPOCHS = 500\n",
    "OPTIMIZATION_TARGET = 'loss'\n",
    "N_MODEL_HP_VARIATIONS_TO_TEST = 100\n",
    "\n",
    "HYPEROPT_EARLY_EXIT = True\n",
    "IS_PARAM_MAXED = False\n",
    "PREVIOUS_BEST_ACCURACY = 0\n",
    "\n",
    "# Used for plotting features vs ACC/Deployed File Size\n",
    "accuracy_list = []\n",
    "file_size_list = []\n",
    "seq_len_list = []\n",
    "dropout_list = []\n",
    "learning_rate_list = []\n",
    "cnn_n_filters_list = []                                        \n",
    "cnn_kernel_size_list = []\n",
    "cnn_pool_size_list = []\n",
    "cnn_n_filters_2_list = []\n",
    "cnn_kernel_size_2_list = []\n",
    "cnn_pool_size_2_list = []\n",
    "rnn_type_list = []\n",
    "n_rnn_stack_list = []\n",
    "rnn_len_list = []\n",
    "\n",
    "class HyperParams(object):\n",
    "    def __init__(self, seq_len=None, learning_rate=None,\\\n",
    "                 dropout=None, batch_size=None, logits_scaling=None, scaling_slope=None,\\\n",
    "                 nn_type=None, cnn_n_filters=None, cnn_kernel_size=None, cnn_pool_size=None,\n",
    "                 cnn_n_filters_2=None, cnn_kernel_size_2=None, cnn_pool_size_2=None,\n",
    "                 rnn_type=None, n_rnn_stack=None, rnn_len=None):\n",
    "        self.seq_len = SEQUENCE_LENGTH if seq_len is None else int(seq_len)\n",
    "        self.learning_rate = LEARNING_RATE if learning_rate is None else learning_rate\n",
    "        self.dropout = DROPOUT if dropout is None else dropout\n",
    "        self.batch_size = BATCH_SIZE if batch_size is None else int(batch_size)\n",
    "        self.logits_scaling = 'none' if logits_scaling is None else logits_scaling\n",
    "        self.scaling_slope = 1.0 if scaling_slope is None else scaling_slope\n",
    "        self.nn_type = NN_TYPE if nn_type is None else nn_type\n",
    "        self.cnn_n_filters = CNN_N_FILTERS if cnn_n_filters is None else int(cnn_n_filters)\n",
    "        self.cnn_kernel_size = 3 if cnn_kernel_size is None else int(cnn_kernel_size)\n",
    "        self.cnn_pool_size = 2 if cnn_pool_size is None else int(cnn_pool_size)\n",
    "        self.cnn_n_filters_2 = CNN_N_FILTERS_2 if cnn_n_filters_2 is None else int(cnn_n_filters_2)\n",
    "        self.cnn_kernel_size_2 = 3 if cnn_kernel_size_2 is None else int(cnn_kernel_size_2)\n",
    "        self.cnn_pool_size_2 = 2 if cnn_pool_size_2 is None else int(cnn_pool_size_2)\n",
    "        self.rnn_type = 'LSTM' if rnn_type is None else rnn_type\n",
    "        self.n_rnn_stack = 1 if n_rnn_stack is None else int(n_rnn_stack)\n",
    "        self.rnn_len = 32 if rnn_len is None else int(rnn_len)\n",
    "\n",
    "    def to_string(self):\n",
    "        print('    **********************************************    \\n')\n",
    "        print('Hyperopt Settings:\\n    seq len: {}| lr: {:.6f}| dropout: {:.4f}| batch_size: {}'\\\n",
    "              .format(self.seq_len, self.learning_rate, self.dropout, self.batch_size))\n",
    "        \n",
    "        if self.nn_type == 'cnn_1layer' or self.nn_type == 'cnn_2layer' or self.nn_type == 'cnn-rnn':\n",
    "            print('    cnn_n_filters: {}| cnn_kernel_size: {}| cnn_pool_size: {}'\\\n",
    "                  .format(self.cnn_n_filters, self.cnn_kernel_size, self.cnn_pool_size))\n",
    "            \n",
    "        if self.nn_type == 'cnn_2layer':\n",
    "            print('    cnn_n_filters_2: {}| cnn_kernel_size_2: {}| cnn_pool_size_2: {}'\\\n",
    "                  .format(self.cnn_n_filters_2, self.cnn_kernel_size_2, self.cnn_pool_size_2))\n",
    "        elif self.nn_type == 'rnn' or self.nn_type == 'cnn-rnn':\n",
    "            print('    rnn_type: {}| rnn_layers: {}| rnn_len: {}'\\\n",
    "                  .format(self.rnn_type, self.n_rnn_stack, self.rnn_len))\n",
    "\n",
    "def optimizer(args):\n",
    "    global x_train, y_train, data_len_train, data_rate_train\n",
    "    global x_valid, y_valid, data_len_valid, data_rate_valid\n",
    "    \n",
    "    global file_size_list, seq_len_list, dropout_list, lr_list\n",
    "    global cnn_n_filters_list, cnn_kernel_size_list, cnn_pool_size_list\n",
    "    global cnn_n_filters_2_list, cnn_kernel_size_2_list, cnn_pool_size_2_list\n",
    "    global accuracy_list\n",
    "    \n",
    "    hyper = HyperParams(**args)\n",
    "    hyper.to_string()\n",
    "    \n",
    "    nn_v19 =\\\n",
    "        nn_classification_model_v19(\n",
    "            hyper.batch_size, hyper.seq_len, CLASSIFICATION_TYPE,\\\n",
    "            N_CLASSIFICATIONS, hyper.dropout, hyper.learning_rate,\\\n",
    "            is_training, hyper.nn_type, MAX_EPOCHS,\\\n",
    "            hyper.cnn_n_filters, hyper.cnn_kernel_size, hyper.cnn_pool_size,\\\n",
    "            hyper.cnn_n_filters_2, hyper.cnn_kernel_size_2,\\\n",
    "            hyper.cnn_pool_size_2, hyper.rnn_type, hyper.n_rnn_stack,\\\n",
    "            hyper.rnn_len)\n",
    "\n",
    "    if hyper.nn_type == 'cnn_1layer':\n",
    "        nn_v19.build_cnn_1layer_model_v00()\n",
    "    elif hyper.nn_type == 'cnn_2layer':\n",
    "        nn_v19.build_cnn_2layer_model_v00()\n",
    "    elif hyper.nn_type == 'rnn':\n",
    "        nn_v19.build_rnn_model_v00()\n",
    "    elif hyper.nn_type == 'cnn-rnn':\n",
    "        nn_v19.build_cnn_to_rnn_model_v00()\n",
    "    else:\n",
    "       raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "        \n",
    "    optimization_parameter = 'loss'\n",
    "\n",
    "    # Used for plotting features vs ACC/Deployed File Size\n",
    "    seq_len_list.append(hyper.seq_len)\n",
    "    dropout_list.append(hyper.dropout)\n",
    "    learning_rate_list.append(hyper.learning_rate)\n",
    "    \n",
    "    # Appending data to appropriate lists\n",
    "    if hyper.nn_type == 'cnn_1layer' or hyper.nn_type == 'cnn_2layer' or hyper.nn_type == 'cnn-rnn':\n",
    "        cnn_n_filters_list.append(hyper.cnn_n_filters)                                       \n",
    "        cnn_kernel_size_list.append(hyper.cnn_kernel_size)\n",
    "        cnn_pool_size_list.append(hyper.cnn_pool_size)\n",
    "    if hyper.nn_type == 'cnn_2layer':\n",
    "        cnn_n_filters_2_list.append(hyper.cnn_n_filters_2)\n",
    "        cnn_kernel_size_2_list.append(hyper.cnn_kernel_size_2)\n",
    "        cnn_pool_size_2_list.append(hyper.cnn_pool_size_2)\n",
    "    elif hyper.nn_type == 'rnn'  or hyper.nn_type == 'cnn-rnn':\n",
    "        rnn_type_list.append(hyper.rnn_type)\n",
    "        n_rnn_stack_list.append(hyper.n_rnn_stack)\n",
    "        rnn_len_list.append(hyper.rnn_len)\n",
    "\n",
    "    keras_verbose = False\n",
    "    batch_size_limit = int(np.size(y_train, 0))\n",
    "    print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "    training_history, file_bytes  =\\\n",
    "        nn_v19.train_v00(N_DATASET_BLOCKS, optimization_parameter,\\\n",
    "            NO_IMPROVEMENT_LIMIT, x_train[0:batch_size_limit, PADDING:PADDING+hyper.seq_len],\\\n",
    "            y_train[0:batch_size_limit], data_len_train[0:batch_size_limit],\\\n",
    "            data_rate_train[0:batch_size_limit], x_valid[:, PADDING:PADDING+hyper.seq_len],\\\n",
    "            y_valid, data_len_valid, data_rate_valid, keras_verbose)\n",
    "    \n",
    "    final_loss = training_history.history['loss'][-1]\n",
    "    weighted_loss = final_loss*(file_bytes/(100*1024)) # Want model smaller than 100KB\n",
    "    epochs = np.size(training_history.history['loss'],0)\n",
    "    final_acc = training_history.history['sparse_categorical_accuracy'][-1]\n",
    "    \n",
    "    file_size_list.append(file_bytes)\n",
    "    accuracy_list.append(final_acc)\n",
    "    \n",
    "    if epochs >= MAX_EPOCHS-1:\n",
    "        print('!!! Still Training! Weighted Loss = {}'.format(weighted_loss))\n",
    "    \n",
    "#     if final_acc > 0.7:\n",
    "    opt_status = STATUS_OK\n",
    "#     else:\n",
    "#         opt_status = STATUS_FAIL\n",
    "#         print('\\n!!! Accuracy Fail: {:.6f} !!!\\n'.format(final_acc))\n",
    "            \n",
    "    if OPTIMIZATION_TARGET == 'loss':\n",
    "        return_struct = {\n",
    "                        'status': opt_status,\n",
    "                        'loss': weighted_loss,\n",
    "                        'epochs': epochs,\n",
    "                        'metrics': {\n",
    "                            'accuracy': final_acc\n",
    "                            }\n",
    "                        }\n",
    "    elif OPTIMIZATION_TARGET == 'accuracy':\n",
    "        return_struct = {\n",
    "                        'status': opt_status,\n",
    "                        'loss': 1-final_acc,\n",
    "                        'epochs': epochs,\n",
    "                        'metrics': {\n",
    "                            'accuracy': final_acc\n",
    "                            }\n",
    "                        }\n",
    "    # Cleaning memory\n",
    "    del nn_v19\n",
    "    sleep(5)\n",
    "    K.clear_session()   \n",
    "    sleep(10)\n",
    "    \n",
    "    return return_struct\n",
    "'''\n",
    "------------------------------------------------------------\n",
    "Hyperparameters to sweep\n",
    "'''\n",
    "if NN_TYPE == 'cnn_1layer':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 80, 2),\n",
    "        'seq_len': hp.quniform('seq_len', 32, N_ADC_SAMPLES/4, 2),\n",
    "        'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 10, 1),\n",
    "        'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 20, 1),\n",
    "        'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1)\n",
    "        }\n",
    "elif NN_TYPE == 'cnn_2layer':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "        'batch_size': hp.quniform('batch_size', 32, 128, 2),\n",
    "        'seq_len': hp.quniform('seq_len', 128, 1024, 2), # N_ADC_SAMPLES/4, 2),\n",
    "        'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 3, 1),\n",
    "        'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 100, 1),\n",
    "        'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1),\n",
    "        'cnn_n_filters_2': hp.quniform('cnn_n_filters_2', 1, 3, 1),\n",
    "        'cnn_kernel_size_2': hp.quniform('cnn_kernel_size_2', 1, 100, 1),\n",
    "        'cnn_pool_size_2': hp.quniform('cnn_pool_size_2', 1, 10, 1)\n",
    "        }\n",
    "elif NN_TYPE == 'rnn':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "        'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "#         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "        'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "        'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "        }\n",
    "elif NN_TYPE == 'cnn-rnn':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 128, 2500, 2),\n",
    "        'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 20, 1),\n",
    "        'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "        'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1),\n",
    "        #         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "#         'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "        'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "        }\n",
    "\n",
    "'''\n",
    "------------------------------------------------------------\n",
    "Hyperparameter Sweep\n",
    "'''\n",
    "\n",
    "t = Trials()\n",
    "best = fmin(optimizer, space, algo=tpe.suggest, max_evals=N_MODEL_HP_VARIATIONS_TO_TEST, trials=t)\n",
    "print('TPE best: {}'.format(best))\n",
    "\n",
    "for trial in t.trials:\n",
    "    print('{} --> {}'.format(trial['result'], trial['misc']['vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt - Single Data Block - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.153580Z",
     "start_time": "2019-11-05T19:09:48.124461Z"
    },
    "code_folding": [
     44,
     196,
     206
    ]
   },
   "outputs": [],
   "source": [
    "# n_adc_samples = n_adc_samples\n",
    "# BATCH_SIZE = 128\n",
    "# SEQUENCE_LENGTH = n_adc_samples # = np.size(x_train_unshuffled, 1)\n",
    "# EPOCHS = 10\n",
    "# DROPOUT = 0.0 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "# LEARNING_RATE = 0.01 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "# CNN_N_FILTERS  = 5\n",
    "# CNN_N_FILTERS_2 = 5\n",
    "\n",
    "# rnn_type = 'LSTM' # \n",
    "# rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "\n",
    "# logits_scaling = 'none' # logits_scaling_list = ['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# logits_scaling_list = ['none', 'decreasing linear', 'decreasing exp']\n",
    "# scaling_slope = 1.0\n",
    "# is_training = False\n",
    "\n",
    "# NN_TYPE = 'cnn-rnn' #'cnn_1layer' #cnn_2layer #rnn #cnn-rnn\n",
    "\n",
    "# NO_IMPROVEMENT_LIMIT = 3\n",
    "# OPTIMIZATION_TARGET = 'loss'\n",
    "# N_MODEL_HP_VARIATIONS_TO_TEST = 50\n",
    "\n",
    "# HYPEROPT_EARLY_EXIT = True\n",
    "# IS_PARAM_MAXED = False\n",
    "# PREVIOUS_BEST_ACCURACY = 0\n",
    "\n",
    "# # Used for plotting features vs ACC/Deployed File Size\n",
    "# accuracy_list = []\n",
    "# file_size_list = []\n",
    "# seq_len_list = []\n",
    "# dropout_list = []\n",
    "# learning_rate_list = []\n",
    "# cnn_n_filters_list = []                                        \n",
    "# cnn_kernel_size_list = []\n",
    "# cnn_pool_size_list = []\n",
    "# cnn_n_filters_2_list = []\n",
    "# cnn_kernel_size_2_list = []\n",
    "# cnn_pool_size_2_list = []\n",
    "# rnn_type_list = []\n",
    "# n_rnn_stack_list = []\n",
    "# rnn_len_list = []\n",
    "\n",
    "# class HyperParams(object):\n",
    "#     def __init__(self, seq_len=None, learning_rate=None,\\\n",
    "#                  dropout=None, batch_size=None, logits_scaling=None, scaling_slope=None,\\\n",
    "#                  nn_type=None, cnn_n_filters=None, cnn_kernel_size=None, cnn_pool_size=None,\n",
    "#                  cnn_n_filters_2=None, cnn_kernel_size_2=None, cnn_pool_size_2=None,\n",
    "#                  rnn_type=None, n_rnn_stack=None, rnn_len=None):\n",
    "#         self.seq_len = SEQUENCE_LENGTH if seq_len is None else int(seq_len)\n",
    "#         self.learning_rate = 0.001 if learning_rate is None else learning_rate\n",
    "#         self.dropout = 0.5 if dropout is None else dropout\n",
    "#         self.batch_size = 64 if batch_size is None else int(batch_size)\n",
    "#         self.logits_scaling = 'none' if logits_scaling is None else logits_scaling\n",
    "#         self.scaling_slope = 1.0 if scaling_slope is None else scaling_slope\n",
    "#         self.nn_type = NN_TYPE if nn_type is None else nn_type\n",
    "#         self.cnn_n_filters = CNN_N_FILTERS if cnn_n_filters is None else int(cnn_n_filters)\n",
    "#         self.cnn_kernel_size = 3 if cnn_kernel_size is None else int(cnn_kernel_size)\n",
    "#         self.cnn_pool_size = 2 if cnn_pool_size is None else int(cnn_pool_size)\n",
    "#         self.cnn_n_filters_2 = CNN_N_FILTERS_2 if cnn_n_filters_2 is None else int(cnn_n_filters_2)\n",
    "#         self.cnn_kernel_size_2 = 3 if cnn_kernel_size_2 is None else int(cnn_kernel_size_2)\n",
    "#         self.cnn_pool_size_2 = 2 if cnn_pool_size_2 is None else int(cnn_pool_size_2)\n",
    "#         self.rnn_type = 'LSTM' if rnn_type is None else rnn_type\n",
    "#         self.n_rnn_stack = 1 if n_rnn_stack is None else int(n_rnn_stack)\n",
    "#         self.rnn_len = 32 if rnn_len is None else int(rnn_len)\n",
    "\n",
    "#     def to_string(self):\n",
    "#         print('Hyperopt Settings:\\n    seq len: {}| lr: {:.6f}| dropout: {:.4f}| batch_size: {}| logits_scaling: {}| scaling_slope: {}'\\\n",
    "#               .format(self.seq_len, self.learning_rate, self.dropout, self.batch_size, self.logits_scaling, self.scaling_slope))\n",
    "        \n",
    "#         if self.nn_type == 'cnn_1layer' or self.nn_type == 'cnn_2layer' or self.nn_type == 'cnn-rnn':\n",
    "#             print('    cnn_n_filters: {}| cnn_kernel_size: {}| cnn_pool_size: {}'\\\n",
    "#                   .format(self.cnn_n_filters, self.cnn_kernel_size, self.cnn_pool_size))\n",
    "            \n",
    "#         if self.nn_type == 'cnn_2layer':\n",
    "#             print('    cnn_n_filters_2: {}| cnn_kernel_size_2: {}| cnn_pool_size_2: {}'\\\n",
    "#                   .format(self.cnn_n_filters_2, self.cnn_kernel_size_2, self.cnn_pool_size_2))\n",
    "#         elif self.nn_type == 'rnn' or self.nn_type == 'cnn-rnn':\n",
    "#             print('    rnn_type: {}| rnn_layers: {}| rnn_len: {}'\\\n",
    "#                   .format(self.rnn_type, self.n_rnn_stack, self.rnn_len))\n",
    "\n",
    "# def optimizer(args):    \n",
    "#     global IS_PARAM_MAXED, HYPEROPT_EARLY_EXIT, OPTIMIZATION_TARGET, PREVIOUS_BEST_ACCURACY\n",
    "#     global x_train, y_train, data_len_train, data_rate_train\n",
    "#     global x_valid, y_valid, data_len_valid, data_rate_valid\n",
    "\n",
    "#     global file_size_list, seq_len_list, dropout_list, learning_rate_list\n",
    "#     global cnn_n_filters_list, cnn_kernel_size_list, cnn_pool_size_list\n",
    "#     global cnn_n_filters_2_list, cnn_kernel_size_2_list, cnn_pool_size_2_list\n",
    "#     global accuracy_list\n",
    "\n",
    "#     hyper = HyperParams(**args)\n",
    "#     if IS_PARAM_MAXED == False:\n",
    "#         hyper.to_string()\n",
    "\n",
    "#     if IS_PARAM_MAXED == True:\n",
    "#         loss, accuracy, epochs = 128.0, 0.0, 0\n",
    "#     else:\n",
    "#         nn_v19 = nn_classification_model_v19(hyper.batch_size, hyper.seq_len, CLASSIFICATION_TYPE, N_CLASSIFICATIONS,\\\n",
    "#                                         hyper.dropout, hyper.learning_rate, hyper.logits_scaling,\\\n",
    "#                                         hyper.scaling_slope, is_training, hyper.nn_type,\\\n",
    "#                                         hyper.cnn_n_filters, hyper.cnn_kernel_size, hyper.cnn_pool_size,\\\n",
    "#                                         hyper.cnn_n_filters_2, hyper.cnn_kernel_size_2, hyper.cnn_pool_size_2,\\\n",
    "#                                         hyper.rnn_type, hyper.n_rnn_stack, hyper.rnn_len)\n",
    "#         if hyper.nn_type == 'cnn_1layer':\n",
    "#             nn_v19.build_cnn_1layer_model_v00()\n",
    "#         elif hyper.nn_type == 'cnn_2layer':\n",
    "#             nn_v19.build_cnn_2layer_model_v00()\n",
    "#         elif hyper.nn_type == 'rnn':\n",
    "#             nn_v19.build_rnn_model_v00()\n",
    "#         elif hyper.nn_type == 'cnn-rnn':\n",
    "#             nn_v19.build_cnn_to_rnn_model_v00()\n",
    "#         else:\n",
    "#            raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "\n",
    "#         optimization_parameter = 'loss'\n",
    "\n",
    "#         # Used for plotting features vs ACC/Deployed File Size\n",
    "#         seq_len_list.append(hyper.seq_len)\n",
    "#         dropout_list.append(hyper.dropout)\n",
    "#         learning_rate_list.append(hyper.learning_rate)\n",
    "\n",
    "#         if hyper.nn_type == 'cnn_1layer' or hyper.nn_type == 'cnn_2layer' or hyper.nn_type == 'cnn-rnn':\n",
    "#             cnn_n_filters_list.append(hyper.cnn_n_filters)                                       \n",
    "#             cnn_kernel_size_list.append(hyper.cnn_kernel_size)\n",
    "#             cnn_pool_size_list.append(hyper.cnn_pool_size)\n",
    "\n",
    "#         if hyper.nn_type == 'cnn_2layer':\n",
    "#             cnn_n_filters_2_list.append(hyper.cnn_n_filters_2)\n",
    "#             cnn_kernel_size_2_list.append(hyper.cnn_kernel_size_2)\n",
    "#             cnn_pool_size_2_list.append(hyper.cnn_pool_size_2)\n",
    "#         elif hyper.nn_type == 'rnn'  or hyper.nn_type == 'cnn-rnn':\n",
    "#             rnn_type_list.append(hyper.rnn_type)\n",
    "#             n_rnn_stack_list.append(hyper.n_rnn_stack)\n",
    "#             rnn_len_list.append(hyper.rnn_len)\n",
    "        \n",
    "#         batch_size_limit = int(np.size(y_train, 0))\n",
    "#         print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "#         loss, accuracy, epochs, file_bytes  = nn_v19.train_v00(N_DATASET_BLOCKS, OPTIMIZATION_TARGET, NO_IMPROVEMENT_LIMIT,\\\n",
    "#                                                     x_train[0:batch_size_limit], y_train[0:batch_size_limit], data_len_train[0:batch_size_limit], data_rate_train[0:batch_size_limit],\\\n",
    "#                                                     x_valid, y_valid, data_len_valid, data_rate_valid)\n",
    "#         file_size_list.append(file_bytes)\n",
    "#         accuracy_list.append(accuracy)\n",
    "\n",
    "#     if HYPEROPT_EARLY_EXIT == True and IS_PARAM_MAXED == False:\n",
    "#         if PREVIOUS_BEST_ACCURACY < accuracy:\n",
    "#             PREVIOUS_BEST_ACCURACY = accuracy\n",
    "#     if accuracy >= 1.0: # if loss <= 0.001:\n",
    "#         IS_PARAM_MAXED = True\n",
    "\n",
    "#     if OPTIMIZATION_TARGET == 'loss':\n",
    "#         return_struct = {\n",
    "#                         'status': STATUS_OK,\n",
    "#                         'loss': loss,\n",
    "#                         'epochs': epochs,\n",
    "#                         'metrics': {\n",
    "#                             'accuracy': accuracy\n",
    "#                             }\n",
    "#                         }\n",
    "#     elif OPTIMIZATION_TARGET == 'accuracy':\n",
    "#         return_struct = {\n",
    "#                         'status': STATUS_OK,\n",
    "#                         'loss': 1-accuracy,\n",
    "#                         'epochs': epochs,\n",
    "#                         'metrics': {\n",
    "#                             'accuracy': accuracy\n",
    "#                             }\n",
    "#                         }\n",
    "#     return return_struct\n",
    "# '''\n",
    "# ------------------------------------------------------------\n",
    "# Hyperparameters to sweep\n",
    "# '''\n",
    "# if NN_TYPE == 'cnn_1layer':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "#         'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 5, 1),\n",
    "#         'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "#         'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1)\n",
    "#         }\n",
    "# elif NN_TYPE == 'cnn_2layer':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "#         'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 5, 1),\n",
    "#         'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "#         'cnn_pool_size': hp.quniform('cnn_pool_size_2', 1, 10, 1),\n",
    "#         'cnn_n_filters_2': hp.quniform('cnn_n_filters_2', 1, 20, 1),\n",
    "#         'cnn_kernel_size_2': hp.quniform('cnn_kernel_size_2', 1, 10, 1),\n",
    "#         'cnn_pool_size_2': hp.quniform('cnn_pool_size_2', 1, 10, 1)\n",
    "#         }\n",
    "# elif NN_TYPE == 'rnn':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "# #         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "#         'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "#         'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "#         }\n",
    "# elif NN_TYPE == 'cnn-rnn':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 128, 2500, 2),\n",
    "#         'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 20, 1),\n",
    "#         'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "#         'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1),\n",
    "#         #         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "#         'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "#         'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "#         }\n",
    "\n",
    "# '''\n",
    "# ------------------------------------------------------------\n",
    "# Hyperparameter Sweep\n",
    "# '''\n",
    "\n",
    "# for CNN_N_FILTERS in list(reversed(range(20))):\n",
    "#     divisor = 16\n",
    "#     SEQUENCE_LENGTH = n_adc_samples_max_0\n",
    "#     while divisor >= 1:\n",
    "#         PREVIOUS_BEST_ACCURACY = 1.0\n",
    "#         HYPEROPT_EARLY_EXIT = True\n",
    "#         while PREVIOUS_BEST_ACCURACY >= 0.9:\n",
    "#             PREVIOUS_BEST_ACCURACY = 0.0\n",
    "#             SEQUENCE_LENGTH = int(SEQUENCE_LENGTH/divisor - 1)\n",
    "#             IS_PARAM_MAXED = False\n",
    "#             t = Trials()\n",
    "#             best = fmin(optimizer, space, algo=tpe.suggest, max_evals=N_MODEL_HP_VARIATIONS_TO_TEST, trials=t)\n",
    "#             print('TPE best: {}'.format(best))\n",
    "#             print('Seq Len: {}| Best Accuracy: {}| Divisor: {}'.format(SEQUENCE_LENGTH, PREVIOUS_BEST_ACCURACY, divisor))\n",
    "#             print('------------------------------------------------')\n",
    "#             if PREVIOUS_BEST_ACCURACY < 0.9:\n",
    "#                 SEQUENCE_LENGTH = SEQUENCE_LENGTH*divisor\n",
    "#                 divisor = divisor/2\n",
    "\n",
    "# for trial in t.trials:\n",
    "#     print('{} --> {}'.format(trial['result'], trial['misc']['vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T20:12:10.115986Z",
     "start_time": "2019-06-28T20:12:10.113247Z"
    }
   },
   "source": [
    "## Plotting Results vs Hyperopt Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T00:57:12.714629Z",
     "start_time": "2019-11-28T00:57:10.710291Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(seq_len_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(seq_len_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(seq_len_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('seq_len')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(dropout_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(dropout_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(dropout_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('dropout_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(np.log10(learning_rate_list), accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(np.log10(learning_rate_list), np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(np.log10(learning_rate_list), np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('np.log10(learning_rate_list)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_n_filters_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_n_filters_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_n_filters_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_n_filters_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_kernel_size_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_kernel_size_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_kernel_size_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_kernel_size_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_pool_size_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_pool_size_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_pool_size_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_pool_size_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_n_filters_2_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_n_filters_2_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_n_filters_2_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_n_filters_2_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_kernel_size_2_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_kernel_size_2_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_kernel_size_2_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_kernel_size_2_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_pool_size_2_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_pool_size_2_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_pool_size_2_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_pool_size_2_list')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.155543Z",
     "start_time": "2019-11-05T19:09:46.857Z"
    }
   },
   "outputs": [],
   "source": [
    "# # cnn_valid_5054935-11529 >> CNN| Seq Len: 1225| LR: 0.002223| DO: 0.0711| b_size: 126| logits_scaling: none| scaling_slope: 1.0| cnn_n_filters: 19| cnn_kernel_size: 7| cnn_pool_size: 4\n",
    "\n",
    "# logits_scaling = 'none' # logits_scaling_list['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# scaling_slope = 1.0 # scaling_slope_list[1, 2, 3, 5, 7, 11]\n",
    "# is_training = True\n",
    "\n",
    "# cnn_n_filters = 9\n",
    "# cnn_kernel_size = 5\n",
    "# cnn_pool_size = 2\n",
    "\n",
    "# cnn_n_filters_2 = 5\n",
    "# cnn_kernel_size_2 = 7\n",
    "# cnn_pool_size_2 = 2\n",
    "\n",
    "# checkpoint = \"/\" + \"cnn_valid_5083579-40525\" # 'none' cnn_valid_5083763-63219\n",
    "# # checkpoint = \"/\" + \"\" # 'decreasing exp'\n",
    "# # checkpoint = \"/\" + \"cnn_valid_3690586-454\" # 'decreasing linear'\n",
    "# meta_to_load = CHECKPOINTS_DIR + checkpoint + \".meta\"\n",
    "# checkpoint_to_load = CHECKPOINTS_DIR + checkpoint\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# optimization_parameter = 'accuracy'\n",
    "# cnn_v19_test = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, CLASSIFICATION_TYPE, N_CLASSIFICATIONS, DROPOUT,\\\n",
    "#                                       LEARNING_RATE, logits_scaling, scaling_slope,\\\n",
    "#                                       is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "#                                       cnn_n_filters_2, cnn_kernel_size_2, cnn_pool_size_2)\n",
    "\n",
    "# saver = tf.train.import_meta_graph(meta_to_load)\n",
    "# with tf.Session() as sess:\n",
    "# #     saver.restore(sess,tf.train.latest_checkpoint(CHECKPOINTS_DIR))\n",
    "#     saver.restore(sess, checkpoint_to_load)        \n",
    "#     print('Restored from: {}'.format(checkpoint_to_load))\n",
    "#     print('Input Sequence Length: {}'.format(SEQUENCE_LENGTH))\n",
    "#     threshold=0.8\n",
    "#     accuracy_list, snr_list = cnn_v19_test.sample_v11(sess, N_DATASET_BLOCKS, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing: Determining Minimum SEQUENCE_LENGTH for 100% Accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.156226Z",
     "start_time": "2019-11-05T19:09:47.095Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # TPE best: {'BATCH_SIZE': 30.0, 'DROPOUT': 0.733620479775405, 'LEARNING_RATE': 0.004139365777138103, 'n_layers': 3.0, 'cnn_len': 78.0}\n",
    "# 6e-5 = cnn_type: LSTM| n_layers: 1| cnn_len: 32| lr: 0.001522| dropout: 0.0305| batch_size: 64| logits_scaling: decreasing exp| scaling_slope: 5.0| cnn_n_filters: 6| cnn_kernel_size: 7| cnn_pool_size: 5\n",
    "# 3e-5 = cnn_type: LSTM| n_layers: 2| cnn_len: 168| lr: 0.002650| dropout: 0.0763| batch_size: 44| logits_scaling: decreasing exp| scaling_slope: 1.0| cnn_n_filters: 5| cnn_kernel_size: 6| cnn_pool_size: 3\n",
    "# cnn_type: LSTM| n_layers: 2| cnn_len: 32| lr: 0.000754| dropout: 0.0809| batch_size: 70| logits_scaling: none| scaling_slope: 3.0| cnn_n_filters: 4| cnn_kernel_size: 8| cnn_pool_size: 3\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# SEQUENCE_LENGTH = n_adc_samples # = np.size(x_train_unshuffled, 1)\n",
    "# NO_IMPROVEMENT_LIMIT = 3 # N_EPOCHS without improvement\n",
    "# DROPOUT = 0.2 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "# LEARNING_RATE = 0.0015 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# cnn_type = 'LSTM' # cnn_type_list = ['LSTM', 'GRU', 'cnn']\n",
    "# n_layers = 1 # n_layers_list = [1,3,5,7]\n",
    "# cnn_len = 32 # cnn_len_list = [128, 256, 512, 1024]\n",
    "# logits_scaling = 'none' # logits_scaling_list['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# scaling_slope = 5.0 # scaling_slope_list[1, 2, 3, 5, 7, 11]\n",
    "# is_training = True\n",
    "\n",
    "# cnn_n_filters = 6\n",
    "# cnn_kernel_size = 7\n",
    "# cnn_pool_size = 5\n",
    "\n",
    "# optimization_parameter = 'loss'\n",
    "\n",
    "# cnn_v19 = cnn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, N_PACKET_TYPES, cnn_len, n_layers, DROPOUT, LEARNING_RATE, cnn_type, logits_scaling, scaling_slope, is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size)\n",
    "\n",
    "# sequence_length_list = []\n",
    "# accuracy_list = []\n",
    "# threshold = 0.9\n",
    "# while SEQUENCE_LENGTH > 7: # Breaks at SEQUENCE_LENGTH == 7 for some reason\n",
    "#     print('Input Sequence Length: {}'.format(SEQUENCE_LENGTH))\n",
    "#     cnn_v19 = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, N_PACKET_TYPES, cnn_len, n_layers, DROPOUT, LEARNING_RATE, cnn_type, logits_scaling, scaling_slope, is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size)\n",
    "#     loss, accuracy, epochs  = cnn_v19.train(x_train_unshuffled[:,0:SEQUENCE_LENGTH], y_train_unshuffled, x_valid_unshuffled[:,0:SEQUENCE_LENGTH], y_valid_unshuffled, optimization_parameter, NO_IMPROVEMENT_LIMIT)\n",
    "#     sequence_length_list.append(SEQUENCE_LENGTH)\n",
    "#     accuracy_list.append(accuracy)\n",
    "#     SEQUENCE_LENGTH = SEQUENCE_LENGTH - 1\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(sequence_length_list, accuracy_list, linewidth=4)                             \n",
    "# plt.title('Minimizing Input Sequence\\nNecessary for Training')\n",
    "# plt.xlabel('Trained Sequence Length')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.xlim([76, 86])\n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tested Network Node and Parameter Count</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.156947Z",
     "start_time": "2019-11-05T19:09:47.266Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint = \"/\" + \"cnn_valid_4155106-2768\" # 'decreasing exp'\n",
    "# meta_to_load = CHECKPOINTS_DIR + checkpoint + \".meta\"\n",
    "# checkpoint_to_load = CHECKPOINTS_DIR + checkpoint\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# cnn_v19_test = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, N_PACKET_TYPES, DROPOUT, LEARNING_RATE, logits_scaling, scaling_slope, is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size)\n",
    "# # graph = tf.get_default_graph()\n",
    "# # with graph.as_default():\n",
    "# saver = tf.train.import_meta_graph(meta_to_load)\n",
    "# with tf.Session() as sess:\n",
    "# #     saver.restore(sess,tf.train.latest_checkpoint(CHECKPOINTS_DIR))\n",
    "#     saver.restore(sess, checkpoint_to_load)        \n",
    "#     print('Restored from: {}'.format(checkpoint_to_load))\n",
    "# #     threshold=0.8\n",
    "# #     cnn_v19_test.sample_v02(sess, x_test_unshuffled, y_test_unshuffled, threshold, logits_scaling)\n",
    "# #     print('Finished')\n",
    "    \n",
    "# print('\\n# Nodes in Checkpoint: {}\\n'.format(np.size(tf.get_default_graph().as_graph_def().node)))\n",
    "\n",
    "# # Open TensorFlow ckpt\n",
    "# reader = tf.train.NewCheckpointReader(checkpoint_to_load)\n",
    "\n",
    "# param_map = reader.get_variable_to_shape_map()\n",
    "# total_count = 0\n",
    "# for k, v in param_map.items():\n",
    "#     if 'Momentum' not in k and 'global_step' not in k:\n",
    "#         temp = np.prod(v)\n",
    "#         total_count += temp\n",
    "#         print('%s: %s => %d' % (k, str(v), temp))\n",
    "\n",
    "# print('\\nTotal Param Count in Checkpoint: %d' % total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Freezing Graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.157596Z",
     "start_time": "2019-11-05T19:09:47.605Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_loaded_graph(model_save_file=MODEL_FINAL_SAVE_PATH, keep_var_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "\n",
    "    K.set_learning_phase(0)#tf.keras.backend.set_learning_phase(0) # 0 testing, 1 training mode        \n",
    "    sess = K.get_session()   \n",
    "    \n",
    "    from keras.models import load_model\n",
    "    loaded_model = load_model(model_save_file) #tf.keras.models.load_model(MODEL_FINAL_SAVE_PATH)\n",
    "    output_names=[out.op.name for out in loaded_model.outputs]\n",
    "    input_names=[inputs_0.op.name for inputs_0 in loaded_model.inputs]\n",
    "\n",
    "    graph = sess.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        print(\"Loaded Input Node: {}\".format([inputs_0.op.name for inputs_0 in loaded_model.inputs]))\n",
    "        print(\"Loaded Output Node: {}\".format([outputs_0.op.name for outputs_0 in loaded_model.outputs]))\n",
    "#         print(\"Loaded Freeze Vals: {}\".format(freeze_var_names))\n",
    "        print('\\n---------------------------------------------------------------\\n')\n",
    "    \n",
    "        from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "        frozen_graph = convert_variables_to_constants(sess, input_graph_def, \\\n",
    "                                                      output_names, freeze_var_names)\n",
    "    return frozen_graph\n",
    "\n",
    "frozen_graph = freeze_loaded_graph(MODEL_FINAL_SAVE_PATH) # Save the frozen graph\n",
    "\n",
    "frozen_graph_dir = CHECKPOINTS_DIR \n",
    "frozen_graph_filename = 'saved_model.pb'\n",
    "tf.train.write_graph(frozen_graph, frozen_graph_dir, frozen_graph_filename, as_text=False )\n",
    "\n",
    "# Get frozen graph file size\n",
    "frozen_graph_filepath = frozen_graph_dir + '/' + frozen_graph_filename\n",
    "file_bytes = int(os.path.getsize(frozen_graph_filepath))\n",
    "file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "K.clear_session()\n",
    "\n",
    "print('File Size: {:.3f}{}'.format(file_size, units))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Frozen Graph and Counting the Nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.158297Z",
     "start_time": "2019-11-05T19:09:47.774Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def load_frozen_graph(graph_dir):\n",
    "    with tf.gfile.FastGFile(graph_dir, \"rb\") as file:\n",
    "        print(\"Graph restored from:\\n{}\".format(graph_dir))\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(file.read())            \n",
    "        print(\"\\n# of Nodes in GraphDef: {}\".format(np.size(graph_def.node)))\n",
    "\n",
    "frozen_graph_dir = CHECKPOINTS_DIR \n",
    "frozen_graph_filename = 'saved_model.pb'\n",
    "frozen_graph_filepath = frozen_graph_dir + '/' + frozen_graph_filename\n",
    "load_frozen_graph(frozen_graph_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TFLite Conversion:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T03:14:29.160439Z",
     "start_time": "2019-12-11T03:14:26.743981Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FINAL_SAVE_PATH)\n",
    "\n",
    "# TFLite Version\n",
    "tflite_model = converter.convert()    \n",
    "with open(MODEL_TFLITE_PATH, 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "file_bytes = int(os.path.getsize(MODEL_TFLITE_PATH))\n",
    "file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "print('\\nTFLite File Size: {:.3f}{}'.format(file_size, units)) \n",
    "\n",
    "# Quantized TFLite Version\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "with open(MODEL_TFLITE_QUANT_PATH, 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n",
    "\n",
    "file_bytes = int(os.path.getsize(MODEL_TFLITE_QUANT_PATH))\n",
    "file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "print('\\nTFLite Quantized File Size: {:.3f}{}'.format(file_size, units))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "TF_K_SKL_P_GPU_191124",
   "language": "python",
   "name": "tf_k_skl_p_gpu_191124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 538,
   "position": {
    "height": "560px",
    "left": "855px",
    "right": "20px",
    "top": "197px",
    "width": "660px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
