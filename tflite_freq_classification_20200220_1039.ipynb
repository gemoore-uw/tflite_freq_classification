{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tflite_freq_classification_20200220_1039 = nn_classification_v19 + using sinusoid(freq_0 -> 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T01:46:25.081690Z",
     "start_time": "2019-06-22T01:46:25.078908Z"
    }
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:01.725104Z",
     "start_time": "2020-02-27T00:56:01.719034Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:02.937820Z",
     "start_time": "2020-02-27T00:56:01.726156Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gemoore/anaconda3/envs/tf_k_skl_p_gpu_191124/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.14.0\n",
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # !!! \"=-1\" Forces system to not use GPU (This can allow for simultaneous training via GPU and testing via CPU)\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    " \n",
    "import tensorflow as tf\n",
    "# Logging Verbosity\n",
    "#   Level | Level for Humans | Level Description                  \n",
    "#  -------|------------------|------------------------------------ \n",
    "#   0     | DEBUG            | [Default] Print all messages       \n",
    "#   1     | INFO             | Filter out INFO messages           \n",
    "#   2     | WARNING          | Filter out INFO & WARNING messages \n",
    "#   3     | ERROR            | Filter out all messages      \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.autograph.set_verbosity(3)\n",
    "tf.get_logger().setLevel('ERROR') # Filter out INFO messages\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK, STATUS_FAIL\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import channels\n",
    "\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from time import sleep\n",
    "from tqdm import tnrange, tqdm_notebook, notebook\n",
    "get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "# import pixiedust # Visual Debugger\n",
    "\n",
    "import importlib  \n",
    "data_utils = importlib.import_module(\"data_utils_frequency_2020-02-25-1239\")\n",
    "# import 'data_utils_frequency_2020-02-25-1239' as data_utils\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "    \n",
    "pickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:02.944090Z",
     "start_time": "2020-02-27T00:56:02.938891Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FOLDER_PATH = '/mnt/2ndSSD/802-11_datasets/sinusoid_freq'\n",
    "DATA_PATH = FOLDER_PATH + '/data'\n",
    "PROJECT_NAME = 'tf_lite_freq_classification'\n",
    "VERSION = '_20200220_1039'\n",
    "\n",
    "from datetime import datetime\n",
    "NOW = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "ROOT_LOG_DIR = FOLDER_PATH + \"/logs_\" + PROJECT_NAME + VERSION\n",
    "if not os.path.exists(ROOT_LOG_DIR):\n",
    "    os.mkdir(ROOT_LOG_DIR)\n",
    "\n",
    "# Tensorboard folders\n",
    "LOGS_DIR = \"{}/run-{}\".format(ROOT_LOG_DIR,NOW)\n",
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.mkdir(LOGS_DIR)\n",
    "\n",
    "if not os.path.exists(LOGS_DIR + '/train'):\n",
    "    os.mkdir(LOGS_DIR + '/train')\n",
    "    \n",
    "if not os.path.exists(LOGS_DIR + '/valid'):\n",
    "    os.mkdir(LOGS_DIR + '/valid')\n",
    "\n",
    "# Checkpoints folder\n",
    "CHECKPOINTS_DIR = FOLDER_PATH + \"/checkpoints_\" + PROJECT_NAME + VERSION\n",
    "if not os.path.exists(CHECKPOINTS_DIR):\n",
    "    os.mkdir(CHECKPOINTS_DIR)\n",
    "    \n",
    "MODEL_FINAL_WEIGHTS_SAVE_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION + '_final_weights.h5'\n",
    "MODEL_FINAL_SAVE_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION + '_final_model.h5'\n",
    "MODEL_TFLITE_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION +'.tflite'\n",
    "MODEL_TFLITE_QUANT_PATH = FOLDER_PATH + '/' + PROJECT_NAME + VERSION +'quantized.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Dataset Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:02.959381Z",
     "start_time": "2020-02-27T00:56:02.945108Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fnmatch import fnmatch\n",
    "import scipy.io as sio\n",
    "\n",
    "def calculate_dataset_limits_01():\n",
    "    for filename in notebook.tqdm(sorted(listdir(DATA_PATH)),desc='Find Max # Samples Loop', leave=False):\n",
    "        filepath = join(DATA_PATH, filename)\n",
    "        if fnmatch(filepath, '*.mat'):\n",
    "            dataDict = sio.loadmat(filepath, squeeze_me = True, struct_as_record=False)\n",
    "            if fnmatch(filepath, '*_class_*'):\n",
    "                classifications = dataDict['classifications']\n",
    "            if fnmatch(filepath, '*_sweep_*'):\n",
    "                f_sweep_data = dataDict['dataset']\n",
    "                n_packets = f_sweep_data.shape[1]\n",
    "                n_adc_samples_per_packet = f_sweep_data.shape[2]\n",
    "                padding_max = 0\n",
    "\n",
    "    return n_packets, n_adc_samples_per_packet, padding_max\n",
    "\n",
    "# n_matlab_packets_min, n_matlab_adc_samples_per_packet_max, padding_max = calculate_dataset_limits_01()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.516337Z",
     "start_time": "2020-02-27T00:56:02.960244Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Find Max # Samples Loop', max=4.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "User Settings:\n",
      "  Maximum # of ADC Samples in Dataset = 27000\n",
      "  Minimum # of Packets per Packet Type = 500\n",
      "  # of Packets per Block = 500\n",
      "  Classification: frequency| # of Classifications: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_matlab_protocols = 1\n",
    "n_matlab_data_packet_types = 28\n",
    "n_matlab_ctrl_packet_types = 0\n",
    "n_matlab_snrs = 1\n",
    "n_matlab_center_freqs = 1\n",
    "n_matlab_downsamples = 1\n",
    "n_matlab_mcss = 1\n",
    "n_matlab_hts = 1\n",
    "\n",
    "n_matlab_packets_min, n_matlab_adc_samples_per_packet_max, padding_max = calculate_dataset_limits_01()\n",
    "\n",
    "# n_matlab_adc_samples_per_packet_max = 8032 # !!! Hard-coded for speed\n",
    "# n_matlab_packets_min = 1998 #2000 # !!! Hard-coded for speed\n",
    "\n",
    "N_PROTOCOLS = n_matlab_protocols \n",
    "RF_SIGNAL_LIST =  ['f', 'n'] # ['x', 'g', 'n']  # !!! Hard-coded for speeding up Debugging\n",
    "N_RF_SIGNALS = len(RF_SIGNAL_LIST)\n",
    "\n",
    "N_SNRS = n_matlab_snrs\n",
    "PADDING = padding_max\n",
    "\n",
    "N_DATA_PACKET_TYPES = n_matlab_data_packet_types\n",
    "N_CTRL_PACKET_TYPES = n_matlab_ctrl_packet_types\n",
    "N_PACKET_TYPES = N_DATA_PACKET_TYPES + N_CTRL_PACKET_TYPES\n",
    "PACKET_TYPE_LIST = range(N_PACKET_TYPES)\n",
    "N_PACKETS_PER_PACKET_TYPE = n_matlab_packets_min # 2000\n",
    "\n",
    "N_DATASET_BLOCKS = 1\n",
    "N_PACKETS_PER_BLOCK = int(N_PACKETS_PER_PACKET_TYPE/N_DATASET_BLOCKS)\n",
    "\n",
    "N_ADC_SAMPLES = n_matlab_adc_samples_per_packet_max\n",
    "\n",
    "N_CENTER_FREQS = n_matlab_center_freqs\n",
    "N_SNRS = n_matlab_snrs\n",
    "N_DOWNSAMPLES = n_matlab_downsamples\n",
    "N_MCSS = n_matlab_mcss\n",
    "N_HTS = n_matlab_hts\n",
    "\n",
    "print(\"User Settings:\")\n",
    "print(\"  Maximum # of ADC Samples in Dataset = {}\".format(N_ADC_SAMPLES))\n",
    "print(\"  Minimum # of Packets per Packet Type = {}\".format(N_PACKETS_PER_PACKET_TYPE))\n",
    "print(\"  # of Packets per Block = {}\".format(N_PACKETS_PER_BLOCK))\n",
    "\n",
    "# Classification Type:\n",
    "CLASSIFICATION_TYPE = 'frequency' # 'protocol' #'address' #'protocolAndPacket'\n",
    "\n",
    "if CLASSIFICATION_TYPE == 'frequency':\n",
    "    N_CLASSIFICATIONS = 28\n",
    "    \n",
    "print('  Classification: {}| # of Classifications: {}\\n'.format(CLASSIFICATION_TYPE, N_CLASSIFICATIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Plot Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.523089Z",
     "start_time": "2020-02-27T00:56:06.517221Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def plot_envelope(data, class_idx=2**8-1, data_rate=0, data_len=2048,\\\n",
    "                  signal_name='NA', descriptor = '',\\\n",
    "                  classification_type ='address', y_axis_label='v',\\\n",
    "                  y_max=2.0, x_max=7832):    \n",
    "\n",
    "    if signal_name == 'NA':                   \n",
    "        if classification_type == 'protocol':\n",
    "            signal_name = data_utils.convert_idx_to_rf_signal_name(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'packet':\n",
    "            signal_name = data_utils.convert_idx_to_packet_name(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'protocolAndPacket':\n",
    "            signal_name = data_utils.convert_idx_to_protocol_and_packet_name(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'address':\n",
    "            signal_name = data_utils.convert_idx_to_address(\\\n",
    "                RF_SIGNAL_LIST, class_idx)\n",
    "        elif classification_type == 'frequency':\n",
    "            signal_name = data_utils.convert_idx_to_frequency(class_idx)\n",
    "        else:\n",
    "           raise Exception('[{}] is not a valid classification_type'\\\n",
    "                           .format(classification_type))\n",
    "    \n",
    "    if y_axis_label == 'v':\n",
    "        y_label = 'Amplitude (V)'\n",
    "    elif y_axis_label == 'adc':\n",
    "        y_label = 'Amplitude (ADC Counts)'\n",
    "    else:\n",
    "       raise Exception('[{}] is not a valid Y Label'.format(y_axis_label))\n",
    "    \n",
    "    for data_idx in range(data.shape[0]):\n",
    "        plt.plot(data[data_idx,:])\n",
    "        plt.title('{} - {}\\nClass: {}| MCS: {}| Length: {} adc_samples'.\\\n",
    "                  format(descriptor, signal_name, class_idx, data_rate, int(data_len)))\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel(y_label)\n",
    "        plt.ylim((-0.01*y_max, y_max*1.01))\n",
    "        plt.xlim((-0.01*x_max, x_max*1.01))\n",
    "\n",
    "# for filename in tqdm_notebook(sorted(listdir(DATA_PATH)),desc='Find Max # Samples Loop', leave=False):\n",
    "#     filepath = join(DATA_PATH, filename)\n",
    "#     if fnmatch(filepath, '*.mat'):\n",
    "#         dataDict = sio.loadmat(filepath, squeeze_me = True, struct_as_record=False)\n",
    "#         if fnmatch(filepath, '*_class_*'):\n",
    "#             classifications = dataDict['classifications']\n",
    "#         if fnmatch(filepath, '*_sweep_*'):\n",
    "#             f_sweep_data = dataDict['dataset']\n",
    "#             n_packets = f_sweep_data.shape[1]\n",
    "#             n_adc_samples_per_packet = f_sweep_data.shape[2]\n",
    "#             padding_max = 0    \n",
    "   \n",
    "# plot_envelope(f_sweep_data[range(27, -1, -9),0,:], class_idx=0, descriptor = 'frequency',\\\n",
    "#               classification_type ='frequency', data_len = N_ADC_SAMPLES,\\\n",
    "#               y_axis_label = 'adc', y_max = 2**14, x_max = N_ADC_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *.dat to *.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T00:57:30.662200Z",
     "start_time": "2020-02-26T00:57:24.653580Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum # of ADC Samples in Dataset = 27000\n",
      "Minimum # of Packets per Packet Type = 500\n",
      "# of Packets per Block = 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc5f270349c48a39b4ad8d25fe74f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downsample Loop', max=1.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Tensor Shape = (1, 28, 1, 1, 1, 1, 500, 27000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Find Max # Samples Loop', max=4.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b7084d26a042f590acd3e4df4b6d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Packet Sample Loop', max=28.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File saved as:/mnt/2ndSSD/802-11_datasets/sinusoid_freq/data/f_sweep_0.p\n",
      "File saved as:/mnt/2ndSSD/802-11_datasets/sinusoid_freq/data/f_classifications_0.p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: This is compatible with data produced after 2019-10-25\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fnmatch import fnmatch\n",
    "import scipy.io as sio\n",
    "import heapq\n",
    "\n",
    "# N_PACKETS_PER_PACKET_TYPE, N_ADC_SAMPLES, PADDING_MAX = calculate_dataset_limits()\n",
    "N_PACKETS_PER_BLOCK = int(N_PACKETS_PER_PACKET_TYPE/N_DATASET_BLOCKS)\n",
    "print(\"Maximum # of ADC Samples in Dataset = {}\".format(N_ADC_SAMPLES))\n",
    "print(\"Minimum # of Packets per Packet Type = {}\".format(N_PACKETS_PER_PACKET_TYPE))\n",
    "print(\"# of Packets per Block = {}\".format(N_PACKETS_PER_BLOCK))\n",
    "\n",
    "sig_avg_max_list = []\n",
    "\n",
    "for block in notebook.trange(N_DATASET_BLOCKS):\n",
    "    for downsample_idx in notebook.trange(N_DOWNSAMPLES, desc='Downsample Loop', leave=False):\n",
    "        # Looping through each of the downsample ratios (1x, 2x, 4x)\n",
    "        n_downsamples_per_pickle = 1\n",
    "        data = (np.ones([N_PROTOCOLS, N_PACKET_TYPES, N_CENTER_FREQS,\\\n",
    "                         n_downsamples_per_pickle, N_SNRS, N_MCSS,\\\n",
    "                         N_PACKETS_PER_BLOCK, N_ADC_SAMPLES],\\\n",
    "                         dtype=np.uint16)*int(2**16-1)).astype(np.int16)\n",
    "        print(\"Data Tensor Shape = {}\".format(data.shape))\n",
    "        data_features = (np.ones([N_PROTOCOLS, N_PACKET_TYPES,\\\n",
    "                                  N_CENTER_FREQS, n_downsamples_per_pickle,\\\n",
    "                                  N_SNRS, N_MCSS, N_PACKETS_PER_BLOCK], dtype=np.uint16)*\\\n",
    "                                  int(2**16-1)).astype(np.uint16)\n",
    "             # [center_freq_, snr_, downsample_ratio_, mcs_, data_rate_, ht_, n_samples_per_packet_temp]\n",
    "        for filename in notebook.tqdm(sorted(listdir(DATA_PATH)),desc='Find Max # Samples Loop', leave=False):\n",
    "            filepath = join(DATA_PATH, filename)\n",
    "            if fnmatch(filepath, '*.mat'):\n",
    "                dataDict = sio.loadmat(filepath, squeeze_me = True, struct_as_record=False)\n",
    "                if fnmatch(filepath, '*_class_*'):\n",
    "                    classifications = dataDict['classifications']\n",
    "                    packet_type_list = classifications\n",
    "                if fnmatch(filepath, '*_sweep_*'):\n",
    "                    f_sweep_data = dataDict['dataset']\n",
    "                    n_adc_samples_per_packet = f_sweep_data.shape[2]\n",
    "                    padding_max = 0   \n",
    "                \n",
    "                    if f_sweep_data.shape[1] < N_PACKETS_PER_BLOCK:\n",
    "                        n_packets = f_sweep_data.shape[1]\n",
    "                    else:\n",
    "                        n_packets = N_PACKETS_PER_BLOCK\n",
    "#                 snr_list = dataStruct.snr\n",
    "#                 n_samples_list = dataStruct.numSamples\n",
    "#                 downsample_ratio_list = dataStruct.downsampleRate\n",
    "#                 packet_n_list = dataStruct.packetNo\n",
    "#                 std_802p11_list = np.tile(dataStruct.standard, N_PACKETS_PER_PACKET_TYPE)\n",
    "#                 center_freq_list = np.tile(dataStruct.center_freq, N_PACKETS_PER_PACKET_TYPE)\n",
    "#                 mcs_list = np.tile((dataStruct.mcs).split('S')[1], N_PACKETS_PER_PACKET_TYPE)\n",
    "#                 mcs_list2 = np.tile(filename.split('_')[9][-1], N_PACKETS_PER_PACKET_TYPE) # mcs_list = dataStruct.mcs\n",
    "#                 if mcs_list[0] != mcs_list2[0]:\n",
    "#                     set_trace()\n",
    "                \n",
    "#                 ht_list = np.tile(dataStruct.ht, N_PACKETS_PER_PACKET_TYPE)\n",
    "#                 data_rate_list = np.tile(dataStruct.dataRate, N_PACKETS_PER_PACKET_TYPE)\n",
    "#                 padding_list = np.tile(dataStruct.padding, N_PACKETS_PER_PACKET_TYPE)\n",
    "#                 if dataStruct.padding != 60:\n",
    "#                     print(filename)\n",
    "#                     print('Padding: {}'.format(dataStruct.padding))\n",
    "#                     set_trace()\n",
    "#                 address_list = np.tile(2**8-1, N_PACKETS_PER_PACKET_TYPE) #!!! Replace eventually\n",
    "#                 if data_utils.convert_downsample_to_idx(\\\n",
    "#                                      downsample_ratio_list[0]) != downsample_idx:\n",
    "#                     continue # !!! Note: this is inefficient but the data set isn't so large that it\n",
    "#                              #       matters at the moment\n",
    "\n",
    "                # Scaling and quantizing the signal\n",
    "#                 n_largest = 440\n",
    "# #                 for sig_idx in range(np.size(sig_list,0)):\n",
    "# #                     # Determining the Avg max Vpeak\n",
    "# #                     avg_max = np.mean(heapq.nlargest(n_largest, sig_list[sig_idx,:]))\n",
    "# #                     sig_avg_max_list.append(avg_max)\n",
    "#                 scaling_target_v = 1.0\n",
    "#                 adc_resolution = 14\n",
    "#                 adc_v_ref = 2.0\n",
    "#                 env_sig = data_utils.scale_and_quantize_sig(sig_list, n_largest,\\\n",
    "#                     scaling_target_v, adc_resolution, adc_v_ref)\n",
    "                \n",
    "        for packet_type_idx_0 in notebook.trange(N_PACKET_TYPES, desc='Packet Sample Loop'):\n",
    "            packet_idx_offset = int(block*n_packets)\n",
    "            packet_idx = packet_idx_offset + n_packets\n",
    "#             print(packet_type_list[packet_idx, 0])\n",
    "            packet_type_idx = data_utils.convert_frequency_to_idx(packet_type_list[packet_type_idx_0, 0])\n",
    "            \n",
    "            packet_length = n_adc_samples_per_packet\n",
    "            std_idx = 0\n",
    "            center_freq_idx = 0\n",
    "            ds_ratio_idx = 0\n",
    "            snr_idx = 0\n",
    "            mcs_idx = 0\n",
    "\n",
    "            data[std_idx, packet_type_idx, center_freq_idx, ds_ratio_idx,\\\n",
    "                 snr_idx, mcs_idx,0:n_packets, 0:packet_length] =\\\n",
    "                    f_sweep_data[packet_type_idx, packet_idx_offset:packet_idx_offset+n_packets, 0:packet_length]\n",
    "\n",
    "            std_0 = 127\n",
    "            std_idx_including_noise = 127\n",
    "            packet_type_0 = np.uint16(data_utils.convert_frequency_to_idx(\\\n",
    "              packet_type_list[packet_type_idx_0, 0])*np.ones(n_packets))\n",
    "#             set_trace()\n",
    "            std_and_packet_type_0 = 127\n",
    "            center_freq_0 = 127\n",
    "            snr_0 = 127\n",
    "            ds_ratio_0 = 127\n",
    "            data_rate_0 = 127     \n",
    "            mcs_0 = 127 \n",
    "            ht_0 = 127\n",
    "            address_0 = 127\n",
    "            data_features[std_idx, packet_type_idx, center_freq_idx,\\\n",
    "                          ds_ratio_idx, snr_idx, mcs_idx,:] = packet_type_0\n",
    "            \n",
    "\n",
    "        # Pickling each downsample_ratio data tensor\n",
    "\n",
    "        if pickle is True:\n",
    "            pickle_data_filename = '/f_sweep_'+ str(block)+'.p'\n",
    "            data_utils.pickle_tensor(data, DATA_PATH + pickle_data_filename)   \n",
    "            pickle_data_filename = '/f_classifications_' + str(block) + '.p'\n",
    "            data_utils.pickle_tensor(data_features, DATA_PATH + pickle_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Preprocess\n",
    "Data = [Protocol Type]x[PacketType]x[Center Freq]x[Downsample Ratio]x[SNR]x[MCS]x[# of Packets Per Type]x[Eveloped Signal]\n",
    "\n",
    "Feature List = [Protocol Type]x[PacketType]x[Protocol Type, PacketType, Protocol + Packet Type, Center Freq, SNR, Downsample Ratio, MCS, Data Rate, HT, Address, # of ADC Samples]\n",
    "\n",
    "| Protocol Type |  Label  |\n",
    "|------------|---------|\n",
    "|   x   |    0    |\n",
    "|   b   |    1    |\n",
    "|   g   |    2    |\n",
    "|   n   |    3    |\n",
    "\n",
    "| PacketType |  Label  |\n",
    "|------------|---------|\n",
    "|   Beacon   |    0    |\n",
    "|   Data     |    1    |\n",
    "|  QoS Data  |    2    |\n",
    "|    ACK     |    3    |\n",
    "|    RTS     |    4    |\n",
    "|    CTS     |    5    |\n",
    "|    Null    |    6    |\n",
    "|  QoS Null  |    7    |\n",
    "|  Blk ACK   |    8    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *.p to Enveloped data_tensor[ ][ ][ ][ ][ ][ ][ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.542626Z",
     "start_time": "2020-02-27T00:56:06.524209Z"
    },
    "code_folding": [
     98
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fnmatch import fnmatch\n",
    "# del data, data_features\n",
    "\n",
    "# Adding Noise to the full tensor\n",
    "def create_tensors(offset, rf_signal_list, n_packet_types,\\\n",
    "                   n_center_freq, n_downsamples, n_snrs, n_mcss,\\\n",
    "                   n_packets_per_packet_type, n_adc_samples, ds_ratio,\\\n",
    "                   block, classification_type, verbose_load, print_each_plot):\n",
    "    n_rf_signal = np.size(rf_signal_list)\n",
    "    \n",
    "    x_tensor_size = (n_rf_signal, n_packet_types, n_center_freq,\\\n",
    "                     n_downsamples, n_snrs, n_mcss,\\\n",
    "                     n_packets_per_packet_type, n_adc_samples)\n",
    "    x_tensor = np.zeros(x_tensor_size, dtype=np.uint16)\n",
    "    noise_tensor_size = (1, 1, n_center_freq,\\\n",
    "             n_downsamples, n_snrs, n_mcss,\\\n",
    "             n_packets_per_packet_type, n_adc_samples)\n",
    "    mu, std_dev = 0.0, 0.05\n",
    "    noise_scaling = 2.0\n",
    "    for packet_types_idx in tnrange(n_packet_types):\n",
    "        noise_tensor = np.abs(np.random.normal(mu, std_dev, size =\\\n",
    "            noise_tensor_size)*noise_scaling) \n",
    "        # Creating a tensor with random.normal len(N_ADC_SAMPLES)\n",
    "        noise_tensor_quantized = data_utils.quantize_sig(noise_tensor,\\\n",
    "                                             adc_resolution=14, adc_v_ref=2.0)\n",
    "        del noise_tensor    \n",
    "        x_tensor[0,packet_types_idx,:,:,:,:,:,:] = noise_tensor_quantized\n",
    "        del noise_tensor_quantized\n",
    "    \n",
    "    features_tensor = np.zeros((N_PROTOCOLS, n_packet_types, n_center_freq,\\\n",
    "                     n_downsamples, n_snrs, n_mcss, 11))\n",
    "        #[std, packet_type, std_and_packet_type, center_freq, snr, \n",
    "        #  downsample_ratio, mcs, data_rate, ht, n_samples_per_packet_temp]\n",
    "    if classification_type == 'address' or classification_type =='data_rate':\n",
    "        y_tensor = np.zeros((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                            n_packets_per_packet_type), dtype=np.uint16)\n",
    "    else:\n",
    "        y_tensor = np.zeros((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                            n_packets_per_packet_type), dtype=np.uint8)\n",
    "        \n",
    "    data_len_tensor = (np.ones((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                        n_packets_per_packet_type), dtype=np.uint16)\\\n",
    "                        *int(2**16-1)).astype(np.uint16)\n",
    "    data_rate_tensor = (np.ones((n_rf_signal, n_packet_types, n_mcss,\\\n",
    "                        n_packets_per_packet_type), dtype=np.uint16)\\\n",
    "                        *int(2**8-1)).astype(np.uint8)\n",
    "    print('x_tensor = {}| features_tensor = {}\\n'.format(x_tensor.shape,\\\n",
    "                                                     features_tensor.shape))\n",
    "    print('y_tensor = {}| data_len_tensor = {}| data_rate_tensor = {}\\n'.\\\n",
    "          format(y_tensor.shape, data_len_tensor.shape, data_rate_tensor.shape))\n",
    "    for filename in sorted(listdir(DATA_PATH)):\n",
    "        filepath_data = join(DATA_PATH, filename)\n",
    "        if fnmatch(filepath_data, '*.p') and fnmatch(filepath_data, '*_data_*')\\\n",
    "            and fnmatch(filepath_data, '*_ratio_'+str(ds_ratio)+'*') and\\\n",
    "            fnmatch(filepath_data, '*_block_'+str(block)+'*'):\n",
    "            start_adc_sample = 0\n",
    "            end_adc_sample = n_adc_samples\n",
    "        \n",
    "            x_tensor[1:N_PROTOCOLS+1,:,:,:,:,:,start_adc_sample:end_adc_sample]=\\\n",
    "                data_utils.unpickle_file(filepath_data) # 0th rf_signal_idx=Noise\n",
    "            \n",
    "            filename_features = filename.split('data')[0]+'features'+\\\n",
    "                                                    filename.split('data')[1]\n",
    "            filepath_features = join(DATA_PATH, filename_features)\n",
    "            features_tensor = data_utils.unpickle_file(filepath_features)\n",
    "\n",
    "            data_len_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                np.repeat(features_tensor[:,:,0,0,0,:,10,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3)\n",
    "            data_rate_tensor[1:N_PROTOCOLS+1,:,:] =\\\n",
    "                np.repeat(features_tensor[:,:,0,0,0,:,6,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3)\n",
    "            if classification_type == 'protocol':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,0,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Protocol Classification\n",
    "            elif classification_type == 'packet':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,1,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Packet Type Classification\n",
    "            elif classification_type == 'protocolAndPacket':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,2,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Protolcol and Packet \n",
    "            elif classification_type == 'address':\n",
    "                y_tensor[1:N_PROTOCOLS+1,:,:,:] =\\\n",
    "                    np.repeat(features_tensor[:,:,0,0,0,:,9,np.newaxis],\\\n",
    "                          n_packets_per_packet_type, axis=3) # Address Classification\n",
    "            \n",
    "            # Matlab Dataset Labeling Error\n",
    "            if 2**8-1 in y_tensor or 2**16-1 in y_tensor:\n",
    "                # No portion of the y_tensor should be unlabeled (i.e. features_tensor[:,:,0,0,0,:,#] == 2**8-1)\n",
    "                print(\"Error in y_tensor labeling: {}\".format(classification_type))\n",
    "                set_trace()\n",
    "                \n",
    "            if print_each_plot is True:\n",
    "                for rf_signal_idx in tnrange(n_rf_signal):\n",
    "                    print(\"RF Signal Index: {}\".format(rf_signal_idx))\n",
    "                    packet_type_idx, c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                        packet_idx = 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "                    plt.figure(figsize=(25, 2))\n",
    "                    plt.subplot(1, 4,rf_signal_idx+ 1) \n",
    "\n",
    "                    data = x_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                                    packet_idx,:]\n",
    "                    class_target = y_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                    data_len = data_len_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                    data_rate = data_rate_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                    data_set_name = data_utils.convert_idx_to_rf_signal_name(\\\n",
    "                        rf_signal_list, rf_signal_idx)\n",
    "                    plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', data_set_name, classification_type, 'adc', y_max = 16384,\\\n",
    "                      x_max=data_len)\n",
    "\n",
    "    return x_tensor, y_tensor, data_len_tensor, data_rate_tensor\n",
    "\n",
    "# offset_0 = 0\n",
    "\n",
    "# n_downsamples_0 = 1 # N_DOWNSAMPLES\n",
    "# n_packets_per_packet_type_0 = N_PACKETS_PER_BLOCK\n",
    "# ds_ratio_sel = 4\n",
    "# block_sel = 0\n",
    "# verbose_load = True\n",
    "# show_plot = True\n",
    "# x_tensor, y_tensor, data_len_tensor, data_rate_tensor =\\\n",
    "#     create_tensors(offset_0, RF_SIGNAL_LIST, N_PACKET_TYPES,\\\n",
    "#                    N_CENTER_FREQS, n_downsamples_0, N_SNRS, N_MCSS,\\\n",
    "#                    n_packets_per_packet_type_0, N_ADC_SAMPLES, ds_ratio_sel,\\\n",
    "#                    block_sel, CLASSIFICATION_TYPE, verbose_load, show_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.553791Z",
     "start_time": "2020-02-27T00:56:06.544634Z"
    },
    "code_folding": [],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_tensor(x_tensor, y_tensor, data_len_tensor, data_rate_tensor, valid_frac, test_frac, show_plot = False):\n",
    "    x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "    x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\\\n",
    "        = data_utils.split_tensor_without_shuffle(x_tensor, y_tensor,\\\n",
    "                                              data_len_tensor, data_rate_tensor,\\\n",
    "                                              validation_fraction = test_frac) # Segmenting to train and test\n",
    "    # Memory clearing\n",
    "    del x_tensor, y_tensor, data_len_tensor, data_rate_tensor\n",
    "\n",
    "    x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "    x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor\\\n",
    "        = data_utils.split_tensor_without_shuffle(x_train_tensor, y_train_tensor,\\\n",
    "                                                data_len_train_tensor, data_rate_train_tensor,\\\n",
    "                                                validation_fraction = valid_frac) # Sub-segmenting training into train and valid\n",
    "    print('x_train_tensor shape = {}\\nx_valid_tensor shape = {}\\nx_test_tensor shape = {}'\\\n",
    "          .format(x_train_tensor.shape, x_valid_tensor.shape, x_test_tensor.shape))\n",
    "    print('y_train_tensor shape = {}| y_valid_tensor shape = {}| y_test_tensor shape = {}'\\\n",
    "          .format(y_train_tensor.shape, y_valid_tensor.shape, y_test_tensor.shape))\n",
    "    \n",
    "    if show_plot == True:\n",
    "        rf_signal_idx = np.random.randint(0, np.size(x_valid_tensor,0))\n",
    "        print(\"Random RF Signal Index: {}\".format(rf_signal_idx))\n",
    "        packet_type_idx, c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "            packet_idx = 0, 0, 0, 0, 0, 0\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        for ii in range(3):\n",
    "            plt.subplot(1, 3,ii+ 1)\n",
    "            if ii == 0:\n",
    "                data = x_train_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_train_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Train'     \n",
    "            elif ii == 1:\n",
    "                data = x_valid_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_valid_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Valid'     \n",
    "            else:\n",
    "                data = x_test_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_test_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Test'        \n",
    "\n",
    "            plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "                      y_max = 2**14-1, x_max=data_len)\n",
    "    return x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "            x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "            x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\n",
    "\n",
    "# valid_frac = 0.1\n",
    "# test_frac =  0.2\n",
    "# show_plot = True\n",
    "# x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "# x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "# x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\\\n",
    "#     = split_tensor(x_tensor, y_tensor, data_len_tensor, data_rate_tensor, valid_frac, test_frac, show_plot)\n",
    "\n",
    "# # Memory clearing\n",
    "# del x_tensor, y_tensor, data_len_tensor, data_rate_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data - Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.564014Z",
     "start_time": "2020-02-27T00:56:06.555414Z"
    },
    "code_folding": [],
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def min_max_scale_x_data(x_train_tensor, x_valid_tensor, x_test_tensor, show_plot = False):\n",
    "    # Min-Max Scaling x_train\n",
    "    scaling_range_min = 0.0\n",
    "    scaling_range_max = 1.0 #2**14-1\n",
    "    scaling_range = (scaling_range_min, scaling_range_max)\n",
    "    x_train_tensor = np.float32(data_utils.min_max_scale(x_train_tensor, scaling_range))\n",
    "    x_valid_tensor = np.float32(data_utils.min_max_scale(x_valid_tensor, scaling_range))\n",
    "    x_test_tensor = np.float32(data_utils.min_max_scale(x_test_tensor, scaling_range))\n",
    "    \n",
    "    if show_plot is True:\n",
    "        rf_signal_idx = np.random.randint(0, np.size(x_valid_tensor,0))\n",
    "        print(\"Random RF Signal Index: {}\".format(rf_signal_idx))\n",
    "        packet_type_idx, c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "            packet_idx = 0, 0, 0, 0, 0, 0\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        for ii in range(3):\n",
    "            plt.subplot(1, 3,ii+ 1)\n",
    "            if ii == 0:\n",
    "                data = x_train_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_train_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_train_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Train'     \n",
    "            elif ii == 1:\n",
    "                data = x_valid_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_valid_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_valid_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Valid'     \n",
    "            else:\n",
    "                data = x_test_tensor[rf_signal_idx, packet_type_idx,\\\n",
    "                    c_freq_idx, ds_idx, snr_idx, mcs_idx,\\\n",
    "                    packet_idx,:]\n",
    "                class_target = y_test_tensor[rf_signal_idx,packet_type_idx,mcs_idx, packet_idx]\n",
    "                data_len = data_len_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                data_rate = data_rate_test_tensor[rf_signal_idx, packet_type_idx, mcs_idx, packet_idx]\n",
    "                dataset_name = 'Test'        \n",
    "\n",
    "            plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "                      y_max = scaling_range_max, x_max=data_len)\n",
    "    return x_train_tensor, x_valid_tensor, x_test_tensor\n",
    "\n",
    "# show_plot = True\n",
    "# x_train_tensor_norm, x_valid_tensor_norm, x_test_tensor_norm\\\n",
    "#     = min_max_scale_x_data(x_train_tensor, x_valid_tensor, x_test_tensor, show_plot)\n",
    "\n",
    "# del x_train_tensor, x_valid_tensor, x_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape to Unshuffled Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.576900Z",
     "start_time": "2020-02-27T00:56:06.565010Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def reshape_to_unshuffled_matrix(x_train_tensor_norm, y_train_tensor,\\\n",
    "    data_len_train_tensor, data_rate_train_tensor,x_valid_tensor_norm,\\\n",
    "    y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "    x_test_tensor_norm, y_test_tensor, data_len_test_tensor,\\\n",
    "    data_rate_test_tensor, show_plot = False):\n",
    "    \n",
    "#     data = [N_RF_SIGNALS, N_PACKET_TYPES, N_CENTER_FREQS,\\\n",
    "#         n_downsamples_per_pickle, N_SNRS, N_MCSS,\\\n",
    "#         N_PACKETS_PER_BLOCK, N_ADC_SAMPLES]\n",
    "\n",
    "#     data_features = [N_RF_SIGNALS, N_PACKET_TYPES,\\\n",
    "#         N_CENTER_FREQS, n_downsamples_per_pickle,\\\n",
    "#         N_SNRS, N_MCSS, 11]\n",
    "    \n",
    "    n_rf_signal = np.size(x_valid_tensor_norm, 0)\n",
    "    n_packet_types = np.size(x_valid_tensor_norm, 1)\n",
    "    n_center_freqs = np.size(x_valid_tensor_norm, 2)\n",
    "    n_downsamples_per_pickle = np.size(x_valid_tensor_norm, 3)\n",
    "    n_snrs = np.size(x_valid_tensor_norm, 4)\n",
    "    n_mcss = np.size(x_valid_tensor_norm, 5)\n",
    "    n_adc_samples = np.size(x_valid_tensor_norm, 7)\n",
    "    \n",
    "    # Reshaping the tensors into a [Combined Samples]x[ADC Samples] Matrix\n",
    "    n_packet_samples_train = np.size(x_train_tensor_norm, 6)\n",
    "    batches_in_train = n_packet_samples_train*n_rf_signal*n_packet_types\\\n",
    "        *n_center_freqs*n_downsamples_per_pickle*n_snrs*n_mcss\n",
    "    x_train_unshuffled = x_train_tensor_norm.reshape(\\\n",
    "        (batches_in_train, n_adc_samples)).astype(np.float32)\n",
    "    y_train_unshuffled = np.squeeze(y_train_tensor.reshape(\\\n",
    "        (batches_in_train, 1))).astype(np.uint8)\n",
    "    data_len_train_unshuffled = np.squeeze(data_len_train_tensor.reshape(\\\n",
    "        (batches_in_train, 1))).astype(np.uint16)\n",
    "    data_rate_train_unshuffled = np.squeeze(data_rate_train_tensor.reshape(\\\n",
    "        (batches_in_train, 1))).astype(np.uint16)\n",
    "\n",
    "    n_packet_samples_valid = np.size(x_valid_tensor_norm, 6)\n",
    "    batches_in_valid = n_packet_samples_valid*n_rf_signal*n_packet_types\\\n",
    "        *n_center_freqs*n_downsamples_per_pickle*n_snrs*n_mcss\n",
    "    x_valid_unshuffled = x_valid_tensor_norm.reshape(\\\n",
    "        (batches_in_valid, n_adc_samples)).astype(np.float32)\n",
    "    y_valid_unshuffled = np.squeeze(y_valid_tensor.reshape(\\\n",
    "        (batches_in_valid, 1))).astype(np.uint8)\n",
    "    data_len_valid_unshuffled = np.squeeze(data_len_valid_tensor.reshape(\\\n",
    "        (batches_in_valid, 1))).astype(np.uint16)\n",
    "    data_rate_valid_unshuffled = np.squeeze(data_rate_valid_tensor.reshape(\\\n",
    "        (batches_in_valid, 1))).astype(np.uint16)\n",
    "\n",
    "    n_packet_samples_test = np.size(x_test_tensor_norm, 6)\n",
    "    batches_in_test = n_packet_samples_test*n_rf_signal*n_packet_types\\\n",
    "        *n_center_freqs*n_downsamples_per_pickle*n_snrs*n_mcss\n",
    "    x_test_unshuffled = x_test_tensor_norm.reshape(\\\n",
    "        (batches_in_test, n_adc_samples)).astype(np.float32)\n",
    "    y_test_unshuffled = np.squeeze(y_test_tensor.reshape(\\\n",
    "        (batches_in_test, 1))).astype(np.uint8)\n",
    "    data_len_test_unshuffled = np.squeeze(data_len_test_tensor.reshape(\\\n",
    "        (batches_in_test, 1))).astype(np.uint16)\n",
    "    data_rate_test_unshuffled = np.squeeze(data_rate_test_tensor.reshape(\\\n",
    "        (batches_in_test, 1))).astype(np.uint16)\n",
    "\n",
    "    if show_plot is True:\n",
    "        batch_idx = np.random.randint(0, np.size(x_valid_unshuffled,0))\n",
    "        print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        for ii in range(3):\n",
    "            plt.subplot(1, 3,ii+ 1)\n",
    "            if ii == 0:\n",
    "                data = x_train_unshuffled[batch_idx,:]\n",
    "                class_target = y_train_unshuffled[batch_idx]\n",
    "                data_len = data_len_train_unshuffled[batch_idx]\n",
    "                data_rate = data_rate_train_unshuffled[batch_idx]\n",
    "                dataset_name = 'Train'     \n",
    "            elif ii == 1:\n",
    "                data = x_valid_unshuffled[batch_idx, :]\n",
    "                class_target = y_valid_unshuffled[batch_idx]\n",
    "                data_len = data_len_valid_unshuffled[batch_idx]\n",
    "                data_rate = data_rate_valid_unshuffled[batch_idx]\n",
    "                dataset_name = 'Valid'\n",
    "            else:\n",
    "                data = x_test_unshuffled[batch_idx,:]\n",
    "                class_target = y_test_unshuffled[batch_idx]\n",
    "                data_len = data_len_test_unshuffled[batch_idx]\n",
    "                data_rate = data_rate_test_unshuffled[batch_idx]\n",
    "                dataset_name = 'Test'        \n",
    "\n",
    "            plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                      'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "                      y_max = 1.0, x_max=data_len)\n",
    "\n",
    "    return x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled,\\\n",
    "        x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled, data_rate_valid_unshuffled,\\\n",
    "        x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled\n",
    "\n",
    "# print(\"# of Protocols in Dataset (not including 'Noise' classification): {}\".format(N_CLASSIFICATIONS-1))\n",
    "\n",
    "# show_plot = True\n",
    "# x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled,\\\n",
    "#     data_rate_train_unshuffled, x_valid_unshuffled, y_valid_unshuffled,\\\n",
    "#     data_len_valid_unshuffled, data_rate_valid_unshuffled, x_test_unshuffled,\\\n",
    "#     y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled\\\n",
    "#     = reshape_to_unshuffled_matrix(x_train_tensor_norm, y_train_tensor,\\\n",
    "#         data_len_train_tensor, data_rate_train_tensor, x_valid_tensor_norm,\\\n",
    "#         y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "#         x_test_tensor_norm, y_test_tensor, data_len_test_tensor,\\\n",
    "#         data_rate_test_tensor, show_plot)\n",
    "\n",
    "# # Memory clearing\n",
    "# del x_test_tensor_norm, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\n",
    "# del x_valid_tensor_norm, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor\n",
    "# del x_train_tensor_norm, y_train_tensor, data_len_train_tensor, data_rate_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_mat[ ][ ] to *.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.582992Z",
     "start_time": "2020-02-27T00:56:06.577809Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def write_unshuffled_matrix_to_p(data_type, classification_type,\\\n",
    "    downsample_rate, block, data_rate_unshuffled = None, \\\n",
    "    data_len_unshuffled = None, y_unshuffled = None, x_unshuffled = None,\\\n",
    "    is_verbose = True):\n",
    "    \n",
    "    if data_rate_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_data_rate_'\\\n",
    "            + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(data_rate_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "        \n",
    "    if data_len_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_data_len_'\\\n",
    "            + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(data_len_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "    \n",
    "    if y_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_y_' + data_type\\\n",
    "            + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(y_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "\n",
    "    # Note: x_unshuffled run alone in order to free up as much memory as possible before writing\n",
    "    if x_unshuffled is not None:\n",
    "        pickle_data_filename = '/' + classification_type + '_x_' + data_type\\\n",
    "            + '_downsample_rate_' + str(downsample_rate)\\\n",
    "            + '_block_' + str(block) + '.p'\n",
    "        data_utils.pickle_tensor(x_unshuffled, DATA_PATH\\\n",
    "            + pickle_data_filename, is_verbose)\n",
    "\n",
    "# ds_rate_0 = 1\n",
    "# block_0 = 0\n",
    "# is_verbose = True\n",
    "# data_type_0 = 'valid'\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "#     y_valid_unshuffled, x_valid_unshuffled, is_verbose)\n",
    "# del x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "#     data_rate_valid_unshuffled\n",
    "\n",
    "# data_type_0 = 'test'\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_test_unshuffled, data_len_test_unshuffled,\\\n",
    "#     y_test_unshuffled, x_test_unshuffled, is_verbose)\n",
    "# del x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled,\\\n",
    "#     data_rate_test_unshuffled\n",
    "\n",
    "# data_type_0 = 'train'\n",
    "# x_unshuffled_dummy = None\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_train_unshuffled, data_len_train_unshuffled,\\\n",
    "#     y_train_unshuffled, x_unshuffled_dummy, is_verbose)\n",
    "# del y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled\n",
    "\n",
    "# data_rate_unshuffled_dummy = None\n",
    "# data_len_unshuffled_dummy = None\n",
    "# y_unshuffled_dummy = None\n",
    "# write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "#     block, data_rate_unshuffled_dummy, data_len_unshuffled_dummy,\\\n",
    "#     y_unshuffled_dummy, x_train_unshuffled, is_verbose)\n",
    "# del x_train_unshuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T09:56:49.040708Z",
     "start_time": "2019-06-27T09:56:49.037885Z"
    }
   },
   "source": [
    "## Load and Preprocess All Data Blocks To *.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:09:31.908433Z",
     "start_time": "2019-11-19T02:08:16.870257Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification: {}| # of Classifications: {}\\n'.format(CLASSIFICATION_TYPE, N_CLASSIFICATIONS))\n",
    "\n",
    "N_PACKETS_PER_PACKET_TYPE, N_ADC_SAMPLES = calculate_dataset_limits()\n",
    "N_PACKETS_PER_BLOCK = int(N_PACKETS_PER_PACKET_TYPE/N_DATASET_BLOCKS)\n",
    "\n",
    "print('User Settings:\\n  ADC Samples: {}\\n  Packets per Type: {}\\n  Dataset Blocks: {}\\n  Dataset Block Sample Size: {}\\n'\\\n",
    "      .format(N_ADC_SAMPLES, N_PACKETS_PER_PACKET_TYPE, N_DATASET_BLOCKS, N_PACKETS_PER_BLOCK))\n",
    "\n",
    "ds_ratio_sel = 4\n",
    "# for block in tnrange(N_DATASET_BLOCKS):\n",
    "block = 0\n",
    "\n",
    "print(\"> > Create Tensor:\")\n",
    "offset_0 = 0\n",
    "n_downsamples_0 = 1 # N_DOWNSAMPLES to use. Limited to 1 bc\n",
    "verbose_load = True\n",
    "show_plot = False\n",
    "x_tensor, y_tensor, data_len_tensor, data_rate_tensor =\\\n",
    "    create_tensors(offset_0, RF_SIGNAL_LIST, N_PACKET_TYPES,\\\n",
    "                   N_CENTER_FREQS, n_downsamples_0, N_SNRS, N_MCSS,\\\n",
    "                   N_PACKETS_PER_BLOCK, N_ADC_SAMPLES, ds_ratio_sel,\\\n",
    "                   block, CLASSIFICATION_TYPE, verbose_load, show_plot)\n",
    "print(\"> > Split Tensor:\")\n",
    "valid_frac = 0.1\n",
    "test_frac =  0.2\n",
    "show_plot = True\n",
    "x_train_tensor, y_train_tensor, data_len_train_tensor, data_rate_train_tensor,\\\n",
    "x_valid_tensor, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "x_test_tensor, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\\\n",
    "    = split_tensor(x_tensor, y_tensor, data_len_tensor, data_rate_tensor,\\\n",
    "                   valid_frac, test_frac, show_plot)\n",
    "\n",
    "del x_tensor, y_tensor, data_len_tensor, data_rate_tensor # Memory clearing\n",
    "\n",
    "print(\"> > Tensor Min Max Scale:\")\n",
    "show_plot = True\n",
    "x_train_tensor_norm, x_valid_tensor_norm, x_test_tensor_norm\\\n",
    "    = min_max_scale_x_data(x_train_tensor, x_valid_tensor, x_test_tensor,\\\n",
    "                           show_plot)\n",
    "\n",
    "del x_train_tensor, x_valid_tensor, x_test_tensor # Memory clearing\n",
    "\n",
    "print(\"> > Reshape to Unshuffled Matrix:\")\n",
    "show_plot = True\n",
    "x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled,\\\n",
    "    data_rate_train_unshuffled, x_valid_unshuffled, y_valid_unshuffled,\\\n",
    "    data_len_valid_unshuffled, data_rate_valid_unshuffled, x_test_unshuffled,\\\n",
    "    y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled\\\n",
    "    = reshape_to_unshuffled_matrix(x_train_tensor_norm, y_train_tensor,\\\n",
    "        data_len_train_tensor, data_rate_train_tensor, x_valid_tensor_norm,\\\n",
    "        y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor,\\\n",
    "        x_test_tensor_norm, y_test_tensor, data_len_test_tensor,\\\n",
    "        data_rate_test_tensor, show_plot)\n",
    "\n",
    "# Memory clearing\n",
    "del x_test_tensor_norm, y_test_tensor, data_len_test_tensor, data_rate_test_tensor\n",
    "del x_valid_tensor_norm, y_valid_tensor, data_len_valid_tensor, data_rate_valid_tensor\n",
    "del x_train_tensor_norm, y_train_tensor, data_len_train_tensor, data_rate_train_tensor\n",
    "\n",
    "print(\"> > Write Unshuffled Matrix to .p:\")\n",
    "is_verbose = True\n",
    "data_type_0 = 'valid'\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "    y_valid_unshuffled, x_valid_unshuffled, is_verbose)\n",
    "del x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled,\\\n",
    "    data_rate_valid_unshuffled\n",
    "\n",
    "data_type_0 = 'test'\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_test_unshuffled, data_len_test_unshuffled,\\\n",
    "    y_test_unshuffled, x_test_unshuffled, is_verbose)\n",
    "del x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled,\\\n",
    "    data_rate_test_unshuffled\n",
    "\n",
    "data_type_0 = 'train'\n",
    "x_unshuffled_dummy = None\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_train_unshuffled, data_len_train_unshuffled,\\\n",
    "    y_train_unshuffled, x_unshuffled_dummy, is_verbose)\n",
    "del y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled\n",
    "\n",
    "data_rate_unshuffled_dummy = None\n",
    "data_len_unshuffled_dummy = None\n",
    "y_unshuffled_dummy = None\n",
    "write_unshuffled_matrix_to_p(data_type_0, CLASSIFICATION_TYPE, ds_ratio_sel,\\\n",
    "    block, data_rate_unshuffled_dummy, data_len_unshuffled_dummy,\\\n",
    "    y_unshuffled_dummy, x_train_unshuffled, is_verbose)\n",
    "del x_train_unshuffled\n",
    "\n",
    "print('\\n---------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *.p to data_mat[ ][ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.591506Z",
     "start_time": "2020-02-27T00:56:06.583920Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def load_presegmented_dataset(data_type, classification_type, downsample_rate, block, is_verbose = True, show_plot = False):\n",
    "    if is_verbose:\n",
    "        print('Classification: {}| # of Classifications: {}\\n'.\\\n",
    "              format(classification_type, N_CLASSIFICATIONS))\n",
    "\n",
    "    pickle_data_filename = '/' + classification_type + '_x_' + data_type\\\n",
    "        + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    x_unshuffled = data_utils.unpickle_file(DATA_PATH + pickle_data_filename, is_verbose)\n",
    "    pickle_data_filename = '/' + classification_type + '_y_' + data_type\\\n",
    "        + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    y_unshuffled = np.squeeze(data_utils.unpickle_file(DATA_PATH\\\n",
    "        + pickle_data_filename, is_verbose)).astype(int)\n",
    "    pickle_data_filename = '/' + classification_type + '_data_len_'\\\n",
    "        + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    data_len_unshuffled = np.squeeze(data_utils.unpickle_file(DATA_PATH\\\n",
    "        + pickle_data_filename, is_verbose)).astype(int)\n",
    "    pickle_data_filename = '/' + classification_type + '_data_rate_'\\\n",
    "        + data_type + '_downsample_rate_' + str(downsample_rate)\\\n",
    "        + '_block_' + str(block) + '.p'\n",
    "    data_rate_unshuffled = np.squeeze(data_utils.unpickle_file(DATA_PATH\\\n",
    "        + pickle_data_filename, is_verbose))\n",
    "        \n",
    "    if is_verbose == True:\n",
    "        print(\"x_\"+data_type+\"_unshuffled: {}\".format(np.shape(x_unshuffled)))\n",
    "        print(\"y_\"+data_type+\"_unshuffled: {}\".format(np.shape(y_unshuffled)))\n",
    "        print(\"data_len_\"+data_type+\"_unshuffled: {}\".format(np.shape(data_len_unshuffled)))\n",
    "        print(\"data_rate_\"+data_type+\"_unshuffled: {}\".format(np.shape(data_rate_unshuffled)))\n",
    "        \n",
    "    if show_plot is True:\n",
    "        batch_idx = np.random.randint(0, np.size(x_unshuffled,0))\n",
    "        print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        data = x_unshuffled[batch_idx,:]\n",
    "        class_target = y_unshuffled[batch_idx]\n",
    "        data_len = data_len_unshuffled[batch_idx]\n",
    "        data_rate = data_rate_unshuffled[batch_idx]\n",
    "        dataset_name = data_type     \n",
    "        plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "                  'NA', dataset_name, classification_type, 'adc',\\\n",
    "                  y_max = 1.0, x_max=data_len)\n",
    "        \n",
    "    return x_unshuffled, y_unshuffled, data_len_unshuffled, data_rate_unshuffled\n",
    "    \n",
    "# is_verbose = True\n",
    "# show_plot = True\n",
    "# downsample_rate = 1\n",
    "# block = 0\n",
    "\n",
    "# dataset_type = 'train'\n",
    "# x_train_unshuffled, y_train_unshuffled, data_len_train_unshuffled, data_rate_train_unshuffled,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)\n",
    "\n",
    "# dataset_type = 'valid'\n",
    "# x_valid_unshuffled, y_valid_unshuffled, data_len_valid_unshuffled, data_rate_valid_unshuffled,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)\n",
    "\n",
    "# dataset_type = 'test'\n",
    "# x_test_unshuffled, y_test_unshuffled, data_len_test_unshuffled, data_rate_test_unshuffled,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T05:19:36.452191Z",
     "start_time": "2019-06-28T05:19:36.449342Z"
    }
   },
   "source": [
    "# Plotting Random Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:06:06.563062Z",
     "start_time": "2019-11-19T02:06:06.256642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Due to the large number of packets in x_train by comparison to x_valid, \n",
    "#  this function will only plot Noise for x_train.\n",
    "#  e.g. if x_train_tensor shape = (4, 9, 1, 1, 1, 8, 480, 8032),  \n",
    "#  x_valid_tensor shape = 10% = (4, 9, 1, 1, 1, 8, 53, 8032), and\n",
    "#  x_test_tensor shape = 20% = (4, 9, 1, 1, 1, 8, 133, 8032), then the maximum\n",
    "#  batch_idx for x_valid = 15264, but the last Noise packet for x_train = 34560,\n",
    "#  however, the last Noise packet for x_test = 9576\n",
    "\n",
    "y_max_0 = x_valid.max()\n",
    "\n",
    "batch_idx = np.random.randint(0, np.size(x_valid,0))\n",
    "print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "plt.figure(figsize=(20, 2))\n",
    "for ii in range(3):\n",
    "    plt.subplot(1, 3,ii+ 1)\n",
    "    if ii == 0:\n",
    "        data = x_train[batch_idx,:]\n",
    "        class_target = y_train[batch_idx]\n",
    "        data_len = data_len_train[batch_idx]\n",
    "        data_rate = data_rate_train[batch_idx]\n",
    "        dataset_name = 'Train'     \n",
    "    elif ii == 1:\n",
    "        data = x_valid[batch_idx, :]\n",
    "        class_target = y_valid[batch_idx]\n",
    "        data_len = data_len_valid[batch_idx]\n",
    "        data_rate = data_rate_valid[batch_idx]\n",
    "        dataset_name = 'Valid'\n",
    "    else:\n",
    "        data = x_test[batch_idx,:]\n",
    "        class_target = y_test[batch_idx]\n",
    "        data_len = data_len_test[batch_idx]\n",
    "        data_rate = data_rate_test[batch_idx]\n",
    "        dataset_name = 'Test'\n",
    "\n",
    "    plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "              'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "              y_max = y_max_0, x_max=data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T20:23:06.183002Z",
     "start_time": "2019-11-18T20:23:06.123533Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Note: Due to the large number of packets in x_train by comparison to x_valid, \n",
    "# #  this function will only plot Noise for x_train.\n",
    "# #  e.g. if x_train_tensor shape = (4, 9, 1, 1, 1, 8, 480, 8032),  \n",
    "# #  x_valid_tensor shape = 10% = (4, 9, 1, 1, 1, 8, 53, 8032), and\n",
    "# #  x_test_tensor shape = 20% = (4, 9, 1, 1, 1, 8, 133, 8032), then the maximum\n",
    "# #  batch_idx for x_valid = 15264, but the last Noise packet for x_train = 34560,\n",
    "# #  however, the last Noise packet for x_test = 9576\n",
    "\n",
    "# y_max_0 = 1.0\n",
    "\n",
    "# batch_idx = np.random.randint(0, np.size(x_valid_unshuffled,0))\n",
    "# print(\"Random Batch Index: {}\".format(batch_idx))\n",
    "# plt.figure(figsize=(20, 2))\n",
    "# for ii in range(3):\n",
    "#     plt.subplot(1, 3,ii+ 1)\n",
    "#     if ii == 0:\n",
    "#         data = x_train_unshuffled[batch_idx,:]\n",
    "#         class_target = y_train_unshuffled[batch_idx]\n",
    "#         data_len = data_len_train_unshuffled[batch_idx]\n",
    "#         data_rate = data_rate_train_unshuffled[batch_idx]\n",
    "#         dataset_name = 'Train'     \n",
    "#     elif ii == 1:\n",
    "#         data = x_valid_unshuffled[batch_idx, :]\n",
    "#         class_target = y_valid_unshuffled[batch_idx]\n",
    "#         data_len = data_len_valid_unshuffled[batch_idx]\n",
    "#         data_rate = data_rate_valid_unshuffled[batch_idx]\n",
    "#         dataset_name = 'Valid'\n",
    "#     else:\n",
    "#         data = x_test_unshuffled[batch_idx,:]\n",
    "#         class_target = y_test_unshuffled[batch_idx]\n",
    "#         data_len = data_len_test_unshuffled[batch_idx]\n",
    "#         data_rate = data_rate_test_unshuffled[batch_idx]\n",
    "#         dataset_name = 'Test'\n",
    "\n",
    "#     plot_envelope(data, class_target, data_rate, data_len,\\\n",
    "#               'NA', dataset_name, CLASSIFICATION_TYPE, 'adc',\\\n",
    "#               y_max = y_max_0, x_max=data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.730022Z",
     "start_time": "2020-02-27T00:56:06.592604Z"
    },
    "code_folding": [
     1,
     19,
     151,
     177,
     231,
     283,
     289,
     308,
     340,
     346,
     365,
     407,
     437,
     477,
     516,
     580
    ],
    "init_cell": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--------------------------\\nIncluding Instantiation and training here for ease of running\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%pixie_debugger\n",
    "class log_history(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('sparse_categorical_accuracy'))\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_acc.append(logs.get('val_sparse_categorical_accuracy'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        if(np.size(self.loss)%50 == 0):\n",
    "            print('\\n--- Training Epoch: {}| Loss: {:.3f}| Acc: {:.3f} ---\\n'.\\\n",
    "                  format(np.size(self.loss), logs.get('loss'),\\\n",
    "                    logs.get('sparse_categorical_accuracy')))\n",
    "        \n",
    "class nn_classification_model_v19:\n",
    "    def __init__(self, batch_size, sequence_length, classification_type,\\\n",
    "                 n_classifications, dropout_probability, learning_rate,\\\n",
    "                 is_training, nn_type, max_epochs,\n",
    "                 cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "                 cnn_n_filters_2 = None, cnn_kernel_size_2 = None, cnn_pool_size_2 = None, \\\n",
    "                 rnn_type = None, n_rnn_stack = None, rnn_len = None):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.input_seq_len = sequence_length\n",
    "        self.input_shape = (sequence_length, 1)\n",
    "        self.classification_type = classification_type\n",
    "        self.n_classifications = n_classifications\n",
    "        self.dropout_probability = dropout_probability\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.nn_type = nn_type\n",
    "        \n",
    "        self.cnn_n_filters = cnn_n_filters\n",
    "        self.cnn_kernel_size = cnn_kernel_size\n",
    "        self.cnn_pool_size = cnn_pool_size\n",
    "        self.stride_size = 2\n",
    "        \n",
    "        self.cnn_n_filters_2 = cnn_n_filters_2\n",
    "        self.cnn_kernel_size_2 = cnn_kernel_size_2\n",
    "        self.cnn_pool_size_2 = cnn_pool_size_2\n",
    "        self.stride_size_2 = 2\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.rnn_len = rnn_len\n",
    "        self.n_rnn_stack = n_rnn_stack\n",
    "        \n",
    "        if nn_type == 'cnn_1layer' or nn_type == 'cnn_2layer':\n",
    "            self.pooled_sequence_length = self.n_classifications\n",
    "        elif nn_type == 'rnn':\n",
    "            self.rnn_sequence_length = self.input_seq_len\n",
    "        elif nn_type == 'cnn-rnn':\n",
    "            self.rnn_sequence_length = int(np.around(self.input_seq_len/self.cnn_pool_size))\n",
    "        \n",
    "        self.global_step = 0\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU (prevents kernel crashes)\n",
    "        self.session = tf.Session(config=config) \n",
    "        K.set_session(self.session) # set this TensorFlow session as the default session for Keras\n",
    "        self.graph = tf.get_default_graph()\n",
    "        self.model = None\n",
    "    \n",
    "    # Train on single block of pre-loaded data\n",
    "    def train_v00(self, dataset_blocks, optimization_target, no_improvement_limit,\\\n",
    "                x_train, y_train, data_len_train, data_rate_train,\\\n",
    "                x_valid, y_valid, data_len_valid, data_rate_valid, keras_verbose):\n",
    "        tf.keras.backend.set_learning_phase(1) #K.set_learning_phase(1) # 0 testing, 1 training mode        \n",
    "        data_utils.delete_files_in_folder(LOGS_DIR + '/train')\n",
    "        data_utils.delete_files_in_folder(LOGS_DIR + '/valid')\n",
    "        history_callback = log_history()\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINTS_DIR\\\n",
    "            + '/model-{epoch:02d}.hdf5', verbose=keras_verbose,\\\n",
    "            monitor='val_loss', save_best_only=True)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOGS_DIR)   \n",
    "        \n",
    "        if optimization_target == 'loss':\n",
    "            early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience\\\n",
    "                = no_improvement_limit, restore_best_weights=True)\n",
    "        elif optimization_target == 'accuracy':\n",
    "            early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor\\\n",
    "                = 'val_sparse_categorical_accuracy', patience\\\n",
    "                = no_improvement_limit, restore_best_weights=True)\n",
    "        \n",
    "        is_verbose = True\n",
    "        if(self.is_training is True):\n",
    "            if(is_verbose is True):\n",
    "                print(\">> Training Settings: \")\n",
    "                print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "                print(\">> NN Type: {}\".format(self.nn_type) + \"| Seq Len: {}\".format(self.input_seq_len) + \"| LR: {:.6f}\".format(self.learning_rate)\\\n",
    "                  + \"| DO: {:.4f}\".format(self.dropout_probability) + \"| b_size: {}\".format(self.batch_size))\n",
    "                if self.nn_type == 'cnn_1layer' or self.nn_type == 'cnn_2layer' or self.nn_type == 'cnn-rnn':\n",
    "                    print('>> cnn_n_filters: {}| cnn_kernel_size: {}| cnn_pool_size: {}'\\\n",
    "                          .format(self.cnn_n_filters, self.cnn_kernel_size, self.cnn_pool_size))\n",
    "\n",
    "                if self.nn_type == 'cnn_2layer':\n",
    "                    print('>> cnn_n_filters_2: {}| cnn_kernel_size_2: {}| cnn_pool_size_2: {}'\\\n",
    "                          .format(self.cnn_n_filters_2, self.cnn_kernel_size_2, self.cnn_pool_size_2))\n",
    "                elif self.nn_type == 'rnn' or self.nn_type == 'cnn-rnn':\n",
    "                    print('>> rnn_type: {}| rnn_layers: {}| rnn_len: {}'\\\n",
    "                          .format(self.rnn_type, self.n_rnn_stack, self.rnn_len))\n",
    "\n",
    "                print(\"    Tensorboard Logs Directory:\")\n",
    "                print(\"        \" + str(LOGS_DIR) + '/train')\n",
    "                print('\\n---------------------------------------------------------------\\n')\n",
    "            \n",
    "            start = timer()\n",
    "            \n",
    "            show_plot = False\n",
    "            is_testing = False  \n",
    "            self.train_len = np.size(y_train,0)\n",
    "            self.valid_len = np.size(y_valid,0)\n",
    "\n",
    "            x_train, y_train, data_len_train, data_rate_train = data_utils.\\\n",
    "                unison_shuffled_copies(x_train, y_train, data_len_train,\\\n",
    "                                       data_rate_train)\n",
    "            x_valid, y_valid, data_len_valid, data_rate_valid = data_utils.\\\n",
    "                unison_shuffled_copies(x_valid, y_valid, data_len_valid,\\\n",
    "                                       data_rate_valid)\n",
    "            \n",
    "            x_train_expanded = np.expand_dims(x_train, axis=2)\n",
    "            y_train_1hot = tf.keras.utils.to_categorical(y_train, num_classes=self.n_classifications)\n",
    "            x_valid_expanded = np.expand_dims(x_valid, axis=2)            \n",
    "            y_valid_1hot = tf.keras.utils.to_categorical(y_valid, num_classes=self.n_classifications)\n",
    "            with self.graph.as_default():\n",
    "                with self.session.as_default():\n",
    "                    training_history = self.model.fit(\n",
    "                        x_train_expanded, y_train, batch_size=self.batch_size,\\\n",
    "                        shuffle = False, verbose=keras_verbose,\\\n",
    "                        epochs=self.max_epochs,validation_data\\\n",
    "                        =(x_valid_expanded, y_valid), # ??? validation_callback to shuffle at end of epoch?\n",
    "                        callbacks=[history_callback, checkpoint_callback,\\\n",
    "                                   tensorboard_callback, early_stop_callback]                        \n",
    "                    )       \n",
    "            \n",
    "                    self.is_training = False        \n",
    "                    file_bytes = self.train_wrapup()\n",
    "            if(is_verbose == True):\n",
    "                print(\"Final Weights Save File:\\n  {}\".format(MODEL_FINAL_WEIGHTS_SAVE_PATH))\n",
    "                print(\"Final Model Save File:\\n  {}\".format(MODEL_FINAL_SAVE_PATH))\n",
    "                print('\\nFinal Training Epoch: {}| Loss: {:.3f}| Acc: {:.3f}\\n\\n'.format(\\\n",
    "                    np.size(training_history.history['loss'],0),\\\n",
    "                    training_history.history['loss'][-1],\\\n",
    "                    training_history.history['sparse_categorical_accuracy'][-1]))\n",
    "        return training_history, file_bytes\n",
    "    \n",
    "    # Saves the final checkpoint and measures graph properties (nodes, parameters, deployable file size)\n",
    "    def train_wrapup(self):        \n",
    "        self.timestamp = str(math.trunc(timer()))\n",
    "        \n",
    "        self.model.save_weights(MODEL_FINAL_WEIGHTS_SAVE_PATH)        \n",
    "        self.model.save(MODEL_FINAL_SAVE_PATH, include_optimizer=False)\n",
    "        \n",
    "#         model_json = self.model.to_json() # only the architecture of the model\n",
    "#         loaded_model_from_json = keras.models.model_from_json(model_final_architecture_file) #loads model architecture only        \n",
    "#         loaded_model_with_weights = loaded_model_from_json.load_weights(MODEL_FINAL_WEIGHTS_SAVE_PATH) # Alternatively, can create model using model = Sequential (...)\n",
    "#         loaded_model = keras.models.load_model(MODEL_FINAL_SAVE_PATH) # load model architecture, weights, training configuration, and state of optimizer when model was saved\n",
    "#         loaded_model.summary() # Should be the same\n",
    "        \n",
    "#         saver = tf.train.Saver()\n",
    "#         K.set_learning_phase(0)\n",
    "#         saved_file = saver.save(sess, CHECKPOINTS_DIR\\\n",
    "#                              + '/nn_valid_' + self.timestamp,\\\n",
    "#                              tf.train.global_step(sess, self.global_step)) # Saving the final check_point\n",
    "#         print(\"    Final Saved checkpoint: \" + saved_file)\n",
    "        file_bytes = os.path.getsize(MODEL_FINAL_SAVE_PATH)\n",
    "        file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "        print('\\nStripped Model Size: {:.3f}{}'.format(file_size, units))\n",
    "        \n",
    "#         file_bytes_0 = self.freeze_graph() # Save the frozen graph\n",
    "        \n",
    "        return file_bytes\n",
    "    \n",
    "    def freeze_graph(self, keep_var_names=None, clear_devices=True):\n",
    "        \"\"\"\n",
    "        Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "        Creates a new computation graph where variable nodes are replaced by\n",
    "        constants taking their current value in the session. The new graph will be\n",
    "        pruned so subgraphs that are not necessary to compute the requested\n",
    "        outputs are removed.\n",
    "        @param session The TensorFlow session to be frozen.\n",
    "        @param keep_var_names A list of variable names that should not be frozen,\n",
    "                              or None to freeze all the variables in the graph.\n",
    "        @param output_names Names of the relevant graph outputs.\n",
    "        @param clear_devices Remove the device directives from the graph for better portability.\n",
    "        @return The frozen graph definition.\n",
    "        \"\"\"\n",
    "                \n",
    "        sess = K.get_session()   \n",
    "        \n",
    "        output_names=[out.op.name for out in self.model.outputs]\n",
    "        input_names=[inputs_0.op.name for inputs_0 in self.model.inputs]\n",
    "        \n",
    "        graph = sess.graph\n",
    "        with graph.as_default():\n",
    "            freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "            output_names = output_names or []\n",
    "            output_names += [v.op.name for v in tf.global_variables()]\n",
    "            # Graph -> GraphDef ProtoBuf\n",
    "            input_graph_def = graph.as_graph_def()\n",
    "            if clear_devices:\n",
    "                for node in input_graph_def.node:\n",
    "                    node.device = \"\"\n",
    "            \n",
    "            print(\"\\nInput Node: {}\".format([inputs_0.op.name for inputs_0 in self.model.inputs]))\n",
    "            print(\"Output Node: {}\".format([outputs_0.op.name for outputs_0 in self.model.outputs]))\n",
    "            print('\\n---------------------------------------------------------------\\n')\n",
    "            \n",
    "            from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "            print('\\n')\n",
    "            frozen_graph = convert_variables_to_constants(sess, input_graph_def, \\\n",
    "                                                          output_names, freeze_var_names)\n",
    "        \n",
    "        print('\\n')\n",
    "        frozen_graph_dir = CHECKPOINTS_DIR \n",
    "        frozen_graph_filename = 'saved_model.pb'\n",
    "        tf.train.write_graph(frozen_graph, frozen_graph_dir, frozen_graph_filename, as_text=False )\n",
    "    \n",
    "        # Get frozen graph file size\n",
    "        frozen_graph_filepath = frozen_graph_dir + '/' + frozen_graph_filename\n",
    "        file_bytes = int(os.path.getsize(frozen_graph_filepath))\n",
    "        file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "#         print('\\nFrozen Graph File Size: {:.3f}{}'.format(file_size, units)) \n",
    "        \n",
    "        return file_bytes\n",
    "        \n",
    "    def build_cnn_1layer_model_v00(self):        \n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                self.model = tf.keras.Sequential([\n",
    "                    l.Conv1D(filters=self.cnn_n_filters,\\\n",
    "                        kernel_size=self.cnn_kernel_size, activation='relu',\\\n",
    "                        input_shape=self.input_shape,\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size,\\\n",
    "                        strides=self.stride_size, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Flatten(),\n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "            \n",
    "    def build_cnn_2layer_model_v00(self):\n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                self.model = tf.keras.Sequential([\n",
    "                    l.Conv1D(filters=self.cnn_n_filters,\\\n",
    "                        kernel_size=self.cnn_kernel_size, activation='relu',\\\n",
    "                        input_shape=self.input_shape,\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size,\\\n",
    "                        strides=self.stride_size, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Conv1D(filters=self.cnn_n_filters_2,\\\n",
    "                        kernel_size=self.cnn_kernel_size_2, activation='relu',\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size_2,\\\n",
    "                        strides=self.stride_size_2, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Flatten(),                    \n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "    \n",
    "    def build_rnn_model_v00(self):\n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                rnn_input_timesteps = self.rnn_sequence_length\n",
    "                n_input_features = 1 # Only one time sequence fed in at a time (as opposed to something like 3 dimen of velocity)\n",
    "                if self.n_rnn_stack == 1:\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.SimpleRNN(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRU(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTM(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        return rnn_layer\n",
    "                    rnn_stack = get_a_cell(self.rnn_len)\n",
    "                else: # Won't use CuDNN Kernel\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.RNNCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRUCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTMCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        return rnn_layer\n",
    "                    rnn_stacked_cells = [get_a_cell(self.rnn_len) for _ in range(self.n_rnn_stack)]           \n",
    "                    rnn_stack = l.RNN(rnn_stacked_cells,\n",
    "                                    input_shape=(rnn_input_timesteps, n_input_features))\n",
    "\n",
    "                self.model = tf.keras.models.Sequential([\n",
    "                    rnn_stack,\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "            \n",
    "    def build_cnn_to_rnn_model_v00(self):\n",
    "        with self.graph.as_default():\n",
    "            with self.session.as_default():\n",
    "                l = tf.keras.layers\n",
    "                rnn_input_timesteps = self.rnn_sequence_length\n",
    "                n_input_features = 1 # Only one time sequence fed in at a time (as opposed to something like 3 dimen of velocity)\n",
    "                if self.n_rnn_stack == 1:\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.SimpleRNN(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRU(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTM(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability,\n",
    "                                input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                        return rnn_layer\n",
    "                    rnn_stack = get_a_cell(self.rnn_len)\n",
    "                else: # Won't use CuDNN Kernel\n",
    "                    def get_a_cell(rnn_len):\n",
    "                        if(self.rnn_type == 'RNN'):\n",
    "                            rnn_layer = l.RNNCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        elif(self.rnn_type == 'GRU'):\n",
    "                            rnn_layer = l.GRUCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        else:\n",
    "                            rnn_layer = l.LSTMCell(units=rnn_len,\\\n",
    "                                dropout=self.dropout_probability,\\\n",
    "                                recurrent_dropout=self.dropout_probability)\n",
    "                        return rnn_layer\n",
    "                    rnn_stacked_cells = [get_a_cell(self.rnn_len) for _ in range(self.n_rnn_stack)]           \n",
    "                    rnn_stack = l.RNN(rnn_stacked_cells,\n",
    "                                    input_shape=(rnn_input_timesteps, n_input_features))\n",
    "                \n",
    "                self.model = tf.keras.Sequential([\n",
    "                    l.Conv1D(self.cnn_n_filters, kernel_size\\\n",
    "                        =self.cnn_kernel_size, activation='relu',\\\n",
    "                        input_shape=self.input_shape,\\\n",
    "                        data_format='channels_last'),\n",
    "                    l.MaxPooling1D(pool_size=self.cnn_pool_size,\\\n",
    "                        strides=self.stride_size, padding='same'),\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Flatten(),\n",
    "                    l.Dense(rnn_input_timesteps, activation='softmax'),                    \n",
    "                    l.Reshape((rnn_input_timesteps,n_input_features)),\n",
    "                    rnn_stack,\n",
    "                    l.Dropout(self.dropout_probability),\n",
    "                    l.Dense(self.n_classifications, activation='softmax')\n",
    "                ])\n",
    "                self.adam_optimizer = tf.keras.optimizers.Adam(\n",
    "                    lr=self.learning_rate, beta_1=0.9, beta_2=0.999,\\\n",
    "                    epsilon=1e-07, amsgrad=True)\n",
    "                self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=self.adam_optimizer,\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "                self.model.summary()\n",
    "            \n",
    "    def sample_v00(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        batch_size_test = self.batch_size #self.test_len #\n",
    "        \n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        self.test_len = np.size(y_test,0)\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            feed = {self.tf_inputs: x_batch[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                    self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                    self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "            acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "            print(\"Test Accuracy= {}%\".format(np.asscalar(np.asarray(acc_test))*100.0))\n",
    "            if acc_test < 1.0:\n",
    "                print(\"Erroroneous Samples:\")\n",
    "                incorrect_pred_idx = np.squeeze(np.where(np.squeeze(correct_pred) == False))\n",
    "#                 print(incorrect_pred_idx)\n",
    "                print(\"Prediction: {}| y_batch classification: {}| data_len: {}| data_rate: {}\"\\\n",
    "                      .format(logits_argmax_1[incorrect_pred_idx].T, y_batch[incorrect_pred_idx],\\\n",
    "                              data_len_batch[incorrect_pred_idx], data_rate_batch[incorrect_pred_idx]))          \n",
    "    def sample_v11(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "        \n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "#         for iter_idx in tnrange(n_dataset_block):                    \n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot)\n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "        self.test_len = np.size(y_test,0)\n",
    "        batch_size_test = self.test_len #\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            snr_list = []\n",
    "            accuracy_list = []            \n",
    "            for snr_level in np.arange(15, -15, -1):\n",
    "                x_batch_noise = x_batch\n",
    "                for ii in range(batch_size_test):\n",
    "                    x_batch_noise[ii] = channels.awgn(x_batch[ii], snr_level, 1.0)\n",
    "\n",
    "                feed = {self.tf_inputs: x_batch_noise[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                        self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                        self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "                acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "                snr_list.append(snr_level)\n",
    "                accuracy_list.append(np.asscalar(np.asarray(acc_test)))\n",
    "        plt.figure()\n",
    "        plt.plot(snr_list, accuracy_list, linewidth=4)                             \n",
    "        plt.title('Accuracy vs SNR')\n",
    "        plt.xlabel('SNR (dB)')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.show()\n",
    "        \n",
    "        return accuracy_list, snr_list        \n",
    "    def sample_v12(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        batch_size_test = self.batch_size #self.test_len #\n",
    "        \n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "#         for iter_idx in tnrange(n_dataset_block):                    \n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "        self.test_len = np.size(y_test,0)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            decimation_list = []\n",
    "            accuracy_list = []         \n",
    "            for dec_factor in np.arange(1, 100):\n",
    "                x_batch_decimate = x_batch\n",
    "                for ii in range(batch_size_test):\n",
    "                    x_batch_decimate[ii] = data_utils.decimate(x_batch[ii], dec_factor)\n",
    "\n",
    "                feed = {self.tf_inputs: x_batch_decimate[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                        self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                        self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "                acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "                decimation_list.append(dec_factor)\n",
    "                accuracy_list.append(np.asscalar(np.asarray(acc_test)))\n",
    "        plt.figure()\n",
    "        plt.plot(snr_list, accuracy_list, linewidth=4)                             \n",
    "        plt.title('Accuracy vs Decimation')\n",
    "        plt.xlabel('Decimation')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.show()\n",
    "    def sample_v13(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        is_verbose = True\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        self.test_len = np.size(y_test,0)\n",
    "        batch_size_test = self.test_len #self.batch_size\n",
    "        \n",
    "        unique, counts = np.unique(y_test, return_counts=True)\n",
    "        sample_counts = dict(zip(unique, counts))\n",
    "        \n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            feed = {self.tf_inputs: x_batch[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                    self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                    self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "            acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "            print(\"Test Accuracy= {}% for Batch Size {}\".format(np.asscalar(np.asarray(acc_test))*100.0, batch_size_test))\n",
    "            if acc_test < 1.0:\n",
    "                print(\"Erroroneous Samples:\")\n",
    "                incorrect_pred_idx = np.squeeze(np.where(np.squeeze(correct_pred) == False))\n",
    "#                 print(incorrect_pred_idx)\n",
    "#                 print(\"Prediction: {}| y_batch classification: {}| data_len: {}| data_rate: {}\"\\\n",
    "#                       .format(logits_argmax_1[incorrect_pred_idx].T, y_batch[incorrect_pred_idx],\\\n",
    "#                               data_len_batch[incorrect_pred_idx], data_rate_batch[incorrect_pred_idx]))\n",
    "                prediction_errors = np.squeeze(logits_argmax_1[incorrect_pred_idx]).astype(int)\n",
    "                prediction_error_labels = y_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_len = y_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_rate = data_rate_batch[incorrect_pred_idx]  \n",
    "#                 print('{},{}, {}, {}'.format(np.shape(prediction_errors),np.shape(prediction_error_labels),np.shape(prediction_error_data_len),np.shape(prediction_error_data_rate)))\n",
    "                error_map = np.zeros((n_classifications, n_classifications))\n",
    "                for ii in range(len(incorrect_pred_idx)):\n",
    "                    error_map[prediction_error_labels[ii], prediction_errors[ii]] += 1\n",
    "                \n",
    "                for ii in range(n_classifications):\n",
    "                    error_map[:, ii] = error_map[:, ii]/sample_counts[ii]\n",
    "#                 print(error_map)\n",
    "                \n",
    "                signal_list_0 = []\n",
    "                for ii in range(n_classifications):\n",
    "                    signal_list_0.append(data_utils.protocol_and_packet_name(RF_SIGNAL_LIST, ii))\n",
    "#                 print(signal_list_0)\n",
    "                ax = plt.figure(figsize=(10, 10)).gca()\n",
    "                plt.rcParams.update({'font.size': 22})\n",
    "#                 from matplotlib.ticker import MaxNLocator\n",
    "#                 ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#                 ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                plt.imshow(error_map, cmap='viridis', vmin=0.0, vmax=0.5, interpolation='nearest', aspect=\"auto\", origin = 'lower')\n",
    "#                 locs, labels = plt.xticks()\n",
    "                plt.xticks(np.arange(n_classifications), signal_list_0, rotation=90)\n",
    "                plt.yticks(np.arange(n_classifications), signal_list_0)\n",
    "#                 plt.title('Percentage of Incorrect for Each Classification')\n",
    "                plt.xlabel('Misclassification')\n",
    "                plt.ylabel('Expected Classification')\n",
    "                plt.rcParams.update({'font.size': 22})\n",
    "                plt.colorbar()    \n",
    "    def sample_v14(self, sess, n_dataset_block, threshold=0.9):\n",
    "        print(\">> Testing: \")\n",
    "        print('>> Classification: {}| # of Classifications: {}\\n'.format(self.classification_type, self.n_classifications))\n",
    "\n",
    "        is_verbose = False\n",
    "        show_plot = False\n",
    "        is_testing = True\n",
    "        file_idx = np.random.randint(n_dataset_block)\n",
    "        x_test, y_test, data_len_test, data_rate_test\\\n",
    "            = load_presegmented_dataset(is_testing, file_idx, classification_type, is_verbose, show_plot) \n",
    "        x_test, y_test, data_len_test, data_rate_test = data_utils.unison_shuffled_copies(x_test, y_test, data_len_test, data_rate_test)\n",
    "#         for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, self.test_len-batch_size_test+1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "        self.test_len = np.size(y_test,0) # self.batch_size # \n",
    "        batch_size_test = self.test_len #self.batch_size\n",
    "        \n",
    "        unique, counts = np.unique(y_test, return_counts=True)\n",
    "        sample_counts = dict(zip(unique, counts))\n",
    "        \n",
    "        data_len_unique, data_len_counts = np.unique(data_len_test, return_counts=True)\n",
    "        data_len_dict = dict(zip(data_len_unique, data_len_counts))\n",
    "        print(data_len_dict)\n",
    "        \n",
    "        data_rate_unique, data_rate_counts = np.unique(data_rate_test, return_counts=True)\n",
    "        data_rate_dict = dict(zip(data_rate_unique, data_rate_counts))\n",
    "        print(data_rate_dict)\n",
    "        for batch_num, batch_offset in enumerate(tqdm_notebook(range(0, 1, batch_size_test), desc='Batch Loop', leave = False)):\n",
    "            x_batch, y_batch = x_test[batch_offset: batch_offset + batch_size_test], y_test[batch_offset: batch_offset + batch_size_test] # fetch batch\n",
    "            data_len_batch, data_rate_batch = data_len_test[batch_offset: batch_offset + batch_size_test], data_rate_test[batch_offset: batch_offset + batch_size_test]\n",
    "            feed = {self.tf_inputs: x_batch[:,0:self.input_seq_len, np.newaxis],\\\n",
    "                    self.classifications:y_batch[:,np.newaxis], self.tf_learning_rate: self.learning_rate,\\\n",
    "                    self.tf_keep_probability: 1.0, self.tf_batch_size: batch_size_test} \n",
    "            acc_test, correct_pred, logits_argmax_1 = sess.run([self.accuracy, self.correct_pred, self.logits_argmax_1], feed_dict = feed)\n",
    "            print(\"Test Accuracy= {}% for Batch Size {}\".format(np.asscalar(np.asarray(acc_test))*100.0, batch_size_test))\n",
    "            if acc_test < 1.0:\n",
    "                print(\"Erroroneous Samples:\")\n",
    "                incorrect_pred_idx = np.squeeze(np.where(np.squeeze(correct_pred) == False))\n",
    "#                 print(incorrect_pred_idx)\n",
    "#                 print(\"Prediction: {}| y_batch classification: {}| data_len: {}| data_rate: {}\"\\\n",
    "#                       .format(logits_argmax_1[incorrect_pred_idx].T, y_batch[incorrect_pred_idx],\\\n",
    "#                               data_len_batch[incorrect_pred_idx], data_rate_batch[incorrect_pred_idx]))\n",
    "                prediction_errors = np.squeeze(logits_argmax_1[incorrect_pred_idx]).astype(int)\n",
    "                prediction_error_labels = y_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_len = data_len_batch[incorrect_pred_idx]\n",
    "                prediction_error_data_rate = data_rate_batch[incorrect_pred_idx]  \n",
    "#                 print('{},{}, {}, {}'.format(np.shape(prediction_errors),np.shape(prediction_error_labels),np.shape(prediction_error_data_len),np.shape(prediction_error_data_rate)))\n",
    "\n",
    "                error_data_len = np.zeros(len(data_len_unique))\n",
    "                for ii in range(len(incorrect_pred_idx)):\n",
    "                    error_data_len[list(data_len_unique).index(prediction_error_data_len[ii])] += 1\n",
    "                \n",
    "                for ii in range(len(data_len_unique)):\n",
    "                    error_data_len[ii] = error_data_len[ii]/data_len_counts[ii]\n",
    "                \n",
    "#                 print('data_len_unique:\\n {}'.format(data_len_unique))\n",
    "#                 print('error_data_len:\\n {}'.format(np.round(error_data_len*100)))\n",
    "                \n",
    "#                 plt.figure(figsize=(10, 10)).gca()\n",
    "# #                 plt.rcParams.update({'font.size': 22})\n",
    "#                 plt.bar(data_len_unique, error_data_len, width=0.4, color='blue')\n",
    "#                 plt.xticks(np.log10(data_len_unique), data_len_unique, rotation=90)\n",
    "#                 plt.xlabel('Data Length (Samples)')\n",
    "#                 plt.ylabel('Percent Misclassified')\n",
    "# #                 plt.rcParams.update({'font.size': 22})\n",
    "                    \n",
    "                error_data_rate = np.zeros(len(data_rate_unique))\n",
    "                for ii in range(len(incorrect_pred_idx)):\n",
    "                    error_data_rate[list(data_rate_unique).index(prediction_error_data_rate[ii])] += 1\n",
    "                \n",
    "                for ii in range(1, len(data_rate_unique)):\n",
    "                    error_data_rate[ii] = error_data_rate[ii]/data_rate_counts[ii]\n",
    "\n",
    "                print(data_rate_unique)\n",
    "                print(error_data_rate)\n",
    "                plt.figure(figsize=(10, 5)).gca()\n",
    "                plt.rcParams.update({'font.size': 22})\n",
    "#                 from matplotlib.ticker import MaxNLocator\n",
    "#                 ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#                 ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                plt.bar(data_rate_unique[1:-1], error_data_rate[1:-1], width=2.0)\n",
    "                locs, labels = plt.xticks()\n",
    "#                 plt.xticks(data_rate_unique[9:-1], data_rate_unique[9:-1], rotation=90)\n",
    "#                 plt.xticks(np.arange(n_classifications), signal_list_0, rotation=90)\n",
    "#                 plt.yticks(np.arange(n_classifications), signal_list_0)\n",
    "#                 plt.title('Percentage of Incorrect for Each Classification')                \n",
    "#                 plt.xticks(data_rate_unique[1:-1], data_rate_unique[1:-1], rotation=90)\n",
    "                plt.xlabel('Data Rate (Mbps)')\n",
    "                plt.ylabel('Percent Misclassified')\n",
    "#                 plt.rcParams.update({'font.size': 22})\n",
    "                plt.show()\n",
    "\n",
    "    \n",
    "'''\n",
    "--------------------------\n",
    "Including Instantiation and training here for ease of running\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T01:48:24.752581Z",
     "start_time": "2019-06-22T01:48:24.750292Z"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Data Block Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T00:56:06.890892Z",
     "start_time": "2020-02-27T00:56:06.730904Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: frequency| # of Classifications: 28\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/2ndSSD/802-11_datasets/sinusoid_freq/data/frequency_x_train_downsample_rate_1_block_0.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ac9e7c25318e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_len_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_rate_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n\u001b[0;32m---> 10\u001b[0;31m                             block, is_verbose, show_plot)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdataset_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5480f8fca882>\u001b[0m in \u001b[0;36mload_presegmented_dataset\u001b[0;34m(data_type, classification_type, downsample_rate, block, is_verbose, show_plot)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0;34m'_downsample_rate_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0;34m'_block_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.p'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_unshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpickle_data_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpickle_data_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclassification_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_y_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0;34m'_downsample_rate_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tflite_freq_classification/data_utils_frequency_2020-02-25-1239.py\u001b[0m in \u001b[0;36munpickle_file\u001b[0;34m(filename, is_verbose)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mUnpickle\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     '''\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/2ndSSD/802-11_datasets/sinusoid_freq/data/frequency_x_train_downsample_rate_1_block_0.p'"
     ]
    }
   ],
   "source": [
    "# Load Training and Validation data from Data Block 'file_idx'\n",
    "is_verbose = True\n",
    "show_plot = True\n",
    "downsample_rate = 1\n",
    "block = 0\n",
    "\n",
    "dataset_type = 'train'\n",
    "x_train, y_train, data_len_train, data_rate_train,\\\n",
    "= load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "                            block, is_verbose, show_plot)\n",
    "\n",
    "dataset_type = 'valid'\n",
    "x_valid, y_valid, data_len_valid, data_rate_valid,\\\n",
    "= load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "                            block, is_verbose, show_plot)\n",
    "\n",
    "# dataset_type = 'test'\n",
    "# x_test, y_test, data_len_test, data_rate_test,\\\n",
    "# = load_presegmented_dataset(dataset_type, CLASSIFICATION_TYPE, downsample_rate,\\\n",
    "#                             block, is_verbose, show_plot)\n",
    "# print('x_test shape = {}| y_test shape = {}'.format(x_test.shape, y_test.shape))\n",
    "\n",
    "x_train, y_train, data_len_train, data_rate_train =\\\n",
    "    data_utils.unison_shuffled_copies(x_train, y_train,\\\n",
    "                                      data_len_train, data_rate_train)\n",
    "print('x_train shape = {}| y_train shape = {}'.format(x_train.shape, y_train.shape))\n",
    "\n",
    "x_valid, y_valid, data_len_valid, data_rate_valid =\\\n",
    "    data_utils.unison_shuffled_copies(x_valid, y_valid,\\\n",
    "                                      data_len_valid, data_rate_valid)\n",
    "print('x_valid shape = {}| y_valid shape = {}'.format(x_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Single Set of Hyperparameters - Single Data Block Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T03:02:27.896151Z",
     "start_time": "2020-01-23T02:33:39.964838Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2019-11-22 - File Size: 22.364KB - seq len: 74| lr: 0.000911| dropout: 0.0158| batch_size: 68| logits_scaling: none| scaling_slope: 1.0\n",
    "#    cnn_n_filters: 1| cnn_kernel_size: 10| cnn_pool_size: 2    \n",
    "# 2019-11-22 - File Size: 31.990KB - seq len: 118| lr: 0.003025| dropout: 0.0066| batch_size: 66| logits_scaling: none| scaling_slope: 1.0\n",
    "#     cnn_n_filters: 1| cnn_kernel_size: 10| cnn_pool_size: 1   \n",
    "# 2019-11-22 - File Size: 64.108KB - seq len: 136| lr: 0.003213| dropout: 0.0203| batch_size: 64| logits_scaling: none| scaling_slope: 1.0\n",
    "#     cnn_n_filters: 2| cnn_kernel_size: 10| cnn_pool_size: 1  \n",
    "#  2019-11-22 -File Size: 56.476KB - seq len: 230| lr: 0.001217| dropout: 0.0704| batch_size: 62| logits_scaling: none| scaling_slope: 1.0\n",
    "# 2019-11-22 - File Size: 105.030KB - cnn_n_filters: 1| cnn_kernel_size: 9| cnn_pool_size: 2\n",
    "#     seq len: 450| lr: 0.018581| dropout: 0.0960| batch_size: 68| logits_scaling: none| scaling_slope: 1.0\n",
    "#     cnn_n_filters: 1| cnn_kernel_size: 7| cnn_pool_size: 1  \n",
    "# 2019-12-26 - Stripped Model Size: 243.383KB - ACC: .891 - seq len: 256| lr: 0.000795| dropout: 0.1163| batch_size: 64\n",
    "#     cnn_n_filters: 9| cnn_kernel_size: 9| cnn_pool_size: 10                              \n",
    "#     rnn_type: LSTM| rnn_layers: 1| rnn_len: 78      \n",
    "# 2019-12-26 - Stripped Model Size: 107.289KB - ACC: .881 - seq len: 256| lr: 0.002828| dropout: 0.4331| batch_size: 64\n",
    "#     cnn_n_filters: 2| cnn_kernel_size: 9| cnn_pool_size: 6                                 \n",
    "#     rnn_type: LSTM| rnn_layers: 1| rnn_len: 48\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LENGTH = 256 #int(n_adc_samples/4) # n_adc_samples # 2554 # int(n_adc_samples) # = np.size(x_train_unshuffled, 1)\n",
    "DROPOUT = 0.2250 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "LEARNING_RATE = 0.000375 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "NO_IMPROVEMENT_LIMIT = 10\n",
    "MAX_EPOCHS = 1000\n",
    "PRUNING_START_BATCH = 2000\n",
    "\n",
    "is_training = True\n",
    "nn_type = 'cnn_2layer' #'cnn_1layer' #cnn_2layer #rnn #cnn-rnn\n",
    "\n",
    "cnn_n_filters = 1\n",
    "cnn_kernel_size = 20\n",
    "cnn_pool_size = 5\n",
    "\n",
    "cnn_n_filters_2 = 1\n",
    "cnn_kernel_size_2 = 19\n",
    "cnn_pool_size_2 = 1\n",
    "\n",
    "rnn_type = 'LSTM' # rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "n_rnn_stack = 1 # n_layers_list = [1,3,5,7]\n",
    "rnn_len = 32 # rnn_len_list = [128, 256, 512, 1024]\n",
    "\n",
    "optimization_parameter = 'loss'\n",
    "\n",
    "nn_v19 = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH,\\\n",
    "            CLASSIFICATION_TYPE, N_CLASSIFICATIONS, DROPOUT, LEARNING_RATE,\\\n",
    "            is_training, nn_type, MAX_EPOCHS,\\\n",
    "            cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "            cnn_n_filters_2, cnn_kernel_size_2, cnn_pool_size_2,\\\n",
    "            rnn_type, n_rnn_stack, rnn_len)\n",
    "\n",
    "if nn_type == 'cnn_1layer':\n",
    "    nn_v19.build_cnn_1layer_model_v00()\n",
    "elif nn_type == 'cnn_2layer':\n",
    "    nn_v19.build_cnn_2layer_model_v00()\n",
    "elif nn_type == 'rnn':\n",
    "    nn_v19.build_rnn_model_v00()\n",
    "elif nn_type == 'cnn-rnn':\n",
    "    nn_v19.build_cnn_to_rnn_model_v00()\n",
    "else:\n",
    "   raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "\n",
    "keras_verbose = False\n",
    "batch_size_limit = int(np.size(y_train, 0))\n",
    "print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "training_history, file_bytes  =\\\n",
    "    nn_v19.train_v00(N_DATASET_BLOCKS, optimization_parameter,\\\n",
    "        NO_IMPROVEMENT_LIMIT, x_train[0:batch_size_limit, PADDING:PADDING+SEQUENCE_LENGTH],\\\n",
    "        y_train[0:batch_size_limit], data_len_train[0:batch_size_limit],\\\n",
    "        data_rate_train[0:batch_size_limit], x_valid[:, PADDING:PADDING+SEQUENCE_LENGTH],\\\n",
    "        y_valid, data_len_valid, data_rate_valid, keras_verbose)\n",
    "\n",
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "del nn_v19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Single Set of Hyperparameters - Continuous Data Block Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:46.180108Z",
     "start_time": "2019-11-05T19:09:46.172280Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2019-06-28-0116: 'loss': 0.0927734375, 'epochs': 5, 'metrics': {'accuracy': 0.90722656}}\n",
    "# # TPE best: {'BATCH_SIZE': 56.0, 'DROPOUT': 0.3841959763303179, 'LEARNING_RATE': 0.0021651506653211235, 'cnn_kernel_size': 5.0, 'cnn_n_filters': 16.0, 'cnn_pool_size': 2.0, 'seq_len': 2084.0}\n",
    "# #     CNN| Seq Len: 852| LR: 0.003010| DO: 0.2582| b_size: 48| logits_scaling: none| scaling_slope: 1.0| cnn_n_filters: 5| cnn_kernel_size: 9| cnn_pool_size: 1| cnn_n_filters_2: 5| cnn_kernel_size_2: 3| cnn_pool_size_2: 2\n",
    "# BATCH_SIZE = 64\n",
    "# SEQUENCE_LENGTH = 852 #int(n_adc_samples/4) # n_adc_samples # 2554 # int(n_adc_samples) # = np.size(x_train_unshuffled, 1)\n",
    "# NO_IMPROVEMENT_LIMIT = 5 # N_EPOCHS without improvement\n",
    "# DROPOUT = 0.2582 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "# LEARNING_RATE = 0.003010 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# logits_scaling = 'none' # logits_scaling_list['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# scaling_slope = 1.0 # scaling_slope_list[1, 2, 3, 5, 7, 11]\n",
    "# is_training = True\n",
    "# nn_type = 'cnn-rnn' #cnn_2layer #rnn #cnn-rnn\n",
    "\n",
    "# cnn_n_filters = 9\n",
    "# cnn_kernel_size = 5\n",
    "# cnn_pool_size = 2\n",
    "\n",
    "# cnn_n_filters_2 = 5\n",
    "# cnn_kernel_size_2 = 7\n",
    "# cnn_pool_size_2 = 2\n",
    "\n",
    "# rnn_type = 'LSTM' # rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "# n_rnn_stack = 1 # n_layers_list = [1,3,5,7]\n",
    "# rnn_len = 32 # rnn_len_list = [128, 256, 512, 1024]\n",
    "\n",
    "# optimization_parameter = 'loss'\n",
    "# nn_v19 = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH,\\\n",
    "#             CLASSIFICATION_TYPE, N_CLASSIFICATIONS, DROPOUT, LEARNING_RATE,\\\n",
    "#             is_training, nn_type, MAX_EPOCHS,\n",
    "#             cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "#             cnn_n_filters_2, cnn_kernel_size_2, cnn_pool_size_2,\\\n",
    "#             rnn_type, n_rnn_stack, rnn_len)\n",
    "\n",
    "# if nn_type == 'cnn_1layer':\n",
    "#     nn_v19.build_cnn_1layer_model_v00()\n",
    "# elif nn_type == 'cnn_2layer':\n",
    "#     nn_v19.build_cnn_2layer_model_v00()\n",
    "# elif nn_type == 'rnn':\n",
    "#     nn_v19.build_rnn_model_v00()\n",
    "# elif nn_type == 'cnn-rnn':\n",
    "#     nn_v19.build_cnn_to_rnn_model_v00()\n",
    "# else:\n",
    "#    raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "\n",
    "# batch_size_limit = int(np.size(y_train, 0))\n",
    "# print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "# loss, accuracy, epochs, file_bytes  = nn_v19.train_v01(N_DATASET_BLOCKS, optimization_parameter, NO_IMPROVEMENT_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt - Single Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T11:46:15.367038Z",
     "start_time": "2020-01-24T07:52:17.508027Z"
    },
    "code_folding": [
     44,
     66,
     95
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "n_adc_samples = 852 #N_ADC_SAMPLES\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LENGTH = 256 # n_adc_samples # = np.size(x_train_unshuffled, 1)\n",
    "EPOCHS = 5\n",
    "DROPOUT = 0.225 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "LEARNING_RATE = 0.000375 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "CNN_N_FILTERS  = 1\n",
    "CNN_N_FILTERS_2 = 1\n",
    "\n",
    "rnn_type = 'LSTM' # \n",
    "rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "\n",
    "is_training = True\n",
    "\n",
    "NN_TYPE = 'cnn_2layer'#'cnn-rnn' #'cnn_1layer' #cnn_2layer #rnn\n",
    "\n",
    "NO_IMPROVEMENT_LIMIT = 5\n",
    "MAX_EPOCHS = 500\n",
    "OPTIMIZATION_TARGET = 'loss'\n",
    "N_MODEL_HP_VARIATIONS_TO_TEST = 100\n",
    "\n",
    "HYPEROPT_EARLY_EXIT = True\n",
    "IS_PARAM_MAXED = False\n",
    "PREVIOUS_BEST_ACCURACY = 0\n",
    "\n",
    "# Used for plotting features vs ACC/Deployed File Size\n",
    "accuracy_list = []\n",
    "file_size_list = []\n",
    "seq_len_list = []\n",
    "dropout_list = []\n",
    "learning_rate_list = []\n",
    "cnn_n_filters_list = []                                        \n",
    "cnn_kernel_size_list = []\n",
    "cnn_pool_size_list = []\n",
    "cnn_n_filters_2_list = []\n",
    "cnn_kernel_size_2_list = []\n",
    "cnn_pool_size_2_list = []\n",
    "rnn_type_list = []\n",
    "n_rnn_stack_list = []\n",
    "rnn_len_list = []\n",
    "\n",
    "class HyperParams(object):\n",
    "    def __init__(self, seq_len=None, learning_rate=None,\\\n",
    "                 dropout=None, batch_size=None, logits_scaling=None, scaling_slope=None,\\\n",
    "                 nn_type=None, cnn_n_filters=None, cnn_kernel_size=None, cnn_pool_size=None,\n",
    "                 cnn_n_filters_2=None, cnn_kernel_size_2=None, cnn_pool_size_2=None,\n",
    "                 rnn_type=None, n_rnn_stack=None, rnn_len=None):\n",
    "        self.seq_len = SEQUENCE_LENGTH if seq_len is None else int(seq_len)\n",
    "        self.learning_rate = LEARNING_RATE if learning_rate is None else learning_rate\n",
    "        self.dropout = DROPOUT if dropout is None else dropout\n",
    "        self.batch_size = BATCH_SIZE if batch_size is None else int(batch_size)\n",
    "        self.logits_scaling = 'none' if logits_scaling is None else logits_scaling\n",
    "        self.scaling_slope = 1.0 if scaling_slope is None else scaling_slope\n",
    "        self.nn_type = NN_TYPE if nn_type is None else nn_type\n",
    "        self.cnn_n_filters = CNN_N_FILTERS if cnn_n_filters is None else int(cnn_n_filters)\n",
    "        self.cnn_kernel_size = 3 if cnn_kernel_size is None else int(cnn_kernel_size)\n",
    "        self.cnn_pool_size = 2 if cnn_pool_size is None else int(cnn_pool_size)\n",
    "        self.cnn_n_filters_2 = CNN_N_FILTERS_2 if cnn_n_filters_2 is None else int(cnn_n_filters_2)\n",
    "        self.cnn_kernel_size_2 = 3 if cnn_kernel_size_2 is None else int(cnn_kernel_size_2)\n",
    "        self.cnn_pool_size_2 = 2 if cnn_pool_size_2 is None else int(cnn_pool_size_2)\n",
    "        self.rnn_type = 'LSTM' if rnn_type is None else rnn_type\n",
    "        self.n_rnn_stack = 1 if n_rnn_stack is None else int(n_rnn_stack)\n",
    "        self.rnn_len = 32 if rnn_len is None else int(rnn_len)\n",
    "\n",
    "    def to_string(self):\n",
    "        print('    **********************************************    \\n')\n",
    "        print('Hyperopt Settings:\\n    seq len: {}| lr: {:.6f}| dropout: {:.4f}| batch_size: {}'\\\n",
    "              .format(self.seq_len, self.learning_rate, self.dropout, self.batch_size))\n",
    "        \n",
    "        if self.nn_type == 'cnn_1layer' or self.nn_type == 'cnn_2layer' or self.nn_type == 'cnn-rnn':\n",
    "            print('    cnn_n_filters: {}| cnn_kernel_size: {}| cnn_pool_size: {}'\\\n",
    "                  .format(self.cnn_n_filters, self.cnn_kernel_size, self.cnn_pool_size))\n",
    "            \n",
    "        if self.nn_type == 'cnn_2layer':\n",
    "            print('    cnn_n_filters_2: {}| cnn_kernel_size_2: {}| cnn_pool_size_2: {}'\\\n",
    "                  .format(self.cnn_n_filters_2, self.cnn_kernel_size_2, self.cnn_pool_size_2))\n",
    "        elif self.nn_type == 'rnn' or self.nn_type == 'cnn-rnn':\n",
    "            print('    rnn_type: {}| rnn_layers: {}| rnn_len: {}'\\\n",
    "                  .format(self.rnn_type, self.n_rnn_stack, self.rnn_len))\n",
    "\n",
    "def optimizer(args):\n",
    "    global x_train, y_train, data_len_train, data_rate_train\n",
    "    global x_valid, y_valid, data_len_valid, data_rate_valid\n",
    "    \n",
    "    global file_size_list, seq_len_list, dropout_list, lr_list\n",
    "    global cnn_n_filters_list, cnn_kernel_size_list, cnn_pool_size_list\n",
    "    global cnn_n_filters_2_list, cnn_kernel_size_2_list, cnn_pool_size_2_list\n",
    "    global accuracy_list\n",
    "    \n",
    "    hyper = HyperParams(**args)\n",
    "    hyper.to_string()\n",
    "    \n",
    "    nn_v19 =\\\n",
    "        nn_classification_model_v19(\n",
    "            hyper.batch_size, hyper.seq_len, CLASSIFICATION_TYPE,\\\n",
    "            N_CLASSIFICATIONS, hyper.dropout, hyper.learning_rate,\\\n",
    "            is_training, hyper.nn_type, MAX_EPOCHS,\\\n",
    "            hyper.cnn_n_filters, hyper.cnn_kernel_size, hyper.cnn_pool_size,\\\n",
    "            hyper.cnn_n_filters_2, hyper.cnn_kernel_size_2,\\\n",
    "            hyper.cnn_pool_size_2, hyper.rnn_type, hyper.n_rnn_stack,\\\n",
    "            hyper.rnn_len)\n",
    "\n",
    "    if hyper.nn_type == 'cnn_1layer':\n",
    "        nn_v19.build_cnn_1layer_model_v00()\n",
    "    elif hyper.nn_type == 'cnn_2layer':\n",
    "        nn_v19.build_cnn_2layer_model_v00()\n",
    "    elif hyper.nn_type == 'rnn':\n",
    "        nn_v19.build_rnn_model_v00()\n",
    "    elif hyper.nn_type == 'cnn-rnn':\n",
    "        nn_v19.build_cnn_to_rnn_model_v00()\n",
    "    else:\n",
    "       raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "        \n",
    "    optimization_parameter = 'loss'\n",
    "\n",
    "    # Used for plotting features vs ACC/Deployed File Size\n",
    "    seq_len_list.append(hyper.seq_len)\n",
    "    dropout_list.append(hyper.dropout)\n",
    "    learning_rate_list.append(hyper.learning_rate)\n",
    "    \n",
    "    # Appending data to appropriate lists\n",
    "    if hyper.nn_type == 'cnn_1layer' or hyper.nn_type == 'cnn_2layer' or hyper.nn_type == 'cnn-rnn':\n",
    "        cnn_n_filters_list.append(hyper.cnn_n_filters)                                       \n",
    "        cnn_kernel_size_list.append(hyper.cnn_kernel_size)\n",
    "        cnn_pool_size_list.append(hyper.cnn_pool_size)\n",
    "    if hyper.nn_type == 'cnn_2layer':\n",
    "        cnn_n_filters_2_list.append(hyper.cnn_n_filters_2)\n",
    "        cnn_kernel_size_2_list.append(hyper.cnn_kernel_size_2)\n",
    "        cnn_pool_size_2_list.append(hyper.cnn_pool_size_2)\n",
    "    elif hyper.nn_type == 'rnn'  or hyper.nn_type == 'cnn-rnn':\n",
    "        rnn_type_list.append(hyper.rnn_type)\n",
    "        n_rnn_stack_list.append(hyper.n_rnn_stack)\n",
    "        rnn_len_list.append(hyper.rnn_len)\n",
    "\n",
    "    keras_verbose = False\n",
    "    batch_size_limit = int(np.size(y_train, 0))\n",
    "    print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "    training_history, file_bytes  =\\\n",
    "        nn_v19.train_v00(N_DATASET_BLOCKS, optimization_parameter,\\\n",
    "            NO_IMPROVEMENT_LIMIT, x_train[0:batch_size_limit, PADDING:PADDING+hyper.seq_len],\\\n",
    "            y_train[0:batch_size_limit], data_len_train[0:batch_size_limit],\\\n",
    "            data_rate_train[0:batch_size_limit], x_valid[:, PADDING:PADDING+hyper.seq_len],\\\n",
    "            y_valid, data_len_valid, data_rate_valid, keras_verbose)\n",
    "    \n",
    "    final_loss = training_history.history['loss'][-1]\n",
    "    weighted_loss = final_loss*(file_bytes/(100*1024)) # Want model smaller than 100KB\n",
    "    epochs = np.size(training_history.history['loss'],0)\n",
    "    final_acc = training_history.history['sparse_categorical_accuracy'][-1]\n",
    "    \n",
    "    file_size_list.append(file_bytes)\n",
    "    accuracy_list.append(final_acc)\n",
    "    \n",
    "    if epochs >= MAX_EPOCHS-1:\n",
    "        print('!!! Still Training! Weighted Loss = {}'.format(weighted_loss))\n",
    "    \n",
    "#     if final_acc > 0.7:\n",
    "    opt_status = STATUS_OK\n",
    "#     else:\n",
    "#         opt_status = STATUS_FAIL\n",
    "#         print('\\n!!! Accuracy Fail: {:.6f} !!!\\n'.format(final_acc))\n",
    "            \n",
    "    if OPTIMIZATION_TARGET == 'loss':\n",
    "        return_struct = {\n",
    "                        'status': opt_status,\n",
    "                        'loss': weighted_loss,\n",
    "                        'epochs': epochs,\n",
    "                        'metrics': {\n",
    "                            'accuracy': final_acc\n",
    "                            }\n",
    "                        }\n",
    "    elif OPTIMIZATION_TARGET == 'accuracy':\n",
    "        return_struct = {\n",
    "                        'status': opt_status,\n",
    "                        'loss': 1-final_acc,\n",
    "                        'epochs': epochs,\n",
    "                        'metrics': {\n",
    "                            'accuracy': final_acc\n",
    "                            }\n",
    "                        }\n",
    "    # Cleaning memory\n",
    "    del nn_v19\n",
    "    sleep(5)\n",
    "    K.clear_session()   \n",
    "    sleep(10)\n",
    "    \n",
    "    return return_struct\n",
    "'''\n",
    "------------------------------------------------------------\n",
    "Hyperparameters to sweep\n",
    "'''\n",
    "if NN_TYPE == 'cnn_1layer':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 80, 2),\n",
    "        'seq_len': hp.quniform('seq_len', 32, N_ADC_SAMPLES/4, 2),\n",
    "        'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 10, 1),\n",
    "        'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 20, 1),\n",
    "        'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1)\n",
    "        }\n",
    "elif NN_TYPE == 'cnn_2layer':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "        'batch_size': hp.quniform('batch_size', 32, 128, 2),\n",
    "        'seq_len': hp.quniform('seq_len', 128, 1024, 2), # N_ADC_SAMPLES/4, 2),\n",
    "        'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 3, 1),\n",
    "        'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 100, 1),\n",
    "        'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1),\n",
    "        'cnn_n_filters_2': hp.quniform('cnn_n_filters_2', 1, 3, 1),\n",
    "        'cnn_kernel_size_2': hp.quniform('cnn_kernel_size_2', 1, 100, 1),\n",
    "        'cnn_pool_size_2': hp.quniform('cnn_pool_size_2', 1, 10, 1)\n",
    "        }\n",
    "elif NN_TYPE == 'rnn':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "        'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "#         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "        'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "        'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "        }\n",
    "elif NN_TYPE == 'cnn-rnn':\n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "        'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 128, 2500, 2),\n",
    "        'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 20, 1),\n",
    "        'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "        'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1),\n",
    "        #         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "#         'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "        'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "        }\n",
    "\n",
    "'''\n",
    "------------------------------------------------------------\n",
    "Hyperparameter Sweep\n",
    "'''\n",
    "\n",
    "t = Trials()\n",
    "best = fmin(optimizer, space, algo=tpe.suggest, max_evals=N_MODEL_HP_VARIATIONS_TO_TEST, trials=t)\n",
    "print('TPE best: {}'.format(best))\n",
    "\n",
    "for trial in t.trials:\n",
    "    print('{} --> {}'.format(trial['result'], trial['misc']['vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt - Single Data Block - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.153580Z",
     "start_time": "2019-11-05T19:09:48.124461Z"
    },
    "code_folding": [
     44,
     196,
     206
    ]
   },
   "outputs": [],
   "source": [
    "# n_adc_samples = n_adc_samples\n",
    "# BATCH_SIZE = 128\n",
    "# SEQUENCE_LENGTH = n_adc_samples # = np.size(x_train_unshuffled, 1)\n",
    "# EPOCHS = 10\n",
    "# DROPOUT = 0.0 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "# LEARNING_RATE = 0.01 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "# CNN_N_FILTERS  = 5\n",
    "# CNN_N_FILTERS_2 = 5\n",
    "\n",
    "# rnn_type = 'LSTM' # \n",
    "# rnn_type_list = ['LSTM', 'GRU', 'RNN']\n",
    "\n",
    "# logits_scaling = 'none' # logits_scaling_list = ['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# logits_scaling_list = ['none', 'decreasing linear', 'decreasing exp']\n",
    "# scaling_slope = 1.0\n",
    "# is_training = False\n",
    "\n",
    "# NN_TYPE = 'cnn-rnn' #'cnn_1layer' #cnn_2layer #rnn #cnn-rnn\n",
    "\n",
    "# NO_IMPROVEMENT_LIMIT = 3\n",
    "# OPTIMIZATION_TARGET = 'loss'\n",
    "# N_MODEL_HP_VARIATIONS_TO_TEST = 50\n",
    "\n",
    "# HYPEROPT_EARLY_EXIT = True\n",
    "# IS_PARAM_MAXED = False\n",
    "# PREVIOUS_BEST_ACCURACY = 0\n",
    "\n",
    "# # Used for plotting features vs ACC/Deployed File Size\n",
    "# accuracy_list = []\n",
    "# file_size_list = []\n",
    "# seq_len_list = []\n",
    "# dropout_list = []\n",
    "# learning_rate_list = []\n",
    "# cnn_n_filters_list = []                                        \n",
    "# cnn_kernel_size_list = []\n",
    "# cnn_pool_size_list = []\n",
    "# cnn_n_filters_2_list = []\n",
    "# cnn_kernel_size_2_list = []\n",
    "# cnn_pool_size_2_list = []\n",
    "# rnn_type_list = []\n",
    "# n_rnn_stack_list = []\n",
    "# rnn_len_list = []\n",
    "\n",
    "# class HyperParams(object):\n",
    "#     def __init__(self, seq_len=None, learning_rate=None,\\\n",
    "#                  dropout=None, batch_size=None, logits_scaling=None, scaling_slope=None,\\\n",
    "#                  nn_type=None, cnn_n_filters=None, cnn_kernel_size=None, cnn_pool_size=None,\n",
    "#                  cnn_n_filters_2=None, cnn_kernel_size_2=None, cnn_pool_size_2=None,\n",
    "#                  rnn_type=None, n_rnn_stack=None, rnn_len=None):\n",
    "#         self.seq_len = SEQUENCE_LENGTH if seq_len is None else int(seq_len)\n",
    "#         self.learning_rate = 0.001 if learning_rate is None else learning_rate\n",
    "#         self.dropout = 0.5 if dropout is None else dropout\n",
    "#         self.batch_size = 64 if batch_size is None else int(batch_size)\n",
    "#         self.logits_scaling = 'none' if logits_scaling is None else logits_scaling\n",
    "#         self.scaling_slope = 1.0 if scaling_slope is None else scaling_slope\n",
    "#         self.nn_type = NN_TYPE if nn_type is None else nn_type\n",
    "#         self.cnn_n_filters = CNN_N_FILTERS if cnn_n_filters is None else int(cnn_n_filters)\n",
    "#         self.cnn_kernel_size = 3 if cnn_kernel_size is None else int(cnn_kernel_size)\n",
    "#         self.cnn_pool_size = 2 if cnn_pool_size is None else int(cnn_pool_size)\n",
    "#         self.cnn_n_filters_2 = CNN_N_FILTERS_2 if cnn_n_filters_2 is None else int(cnn_n_filters_2)\n",
    "#         self.cnn_kernel_size_2 = 3 if cnn_kernel_size_2 is None else int(cnn_kernel_size_2)\n",
    "#         self.cnn_pool_size_2 = 2 if cnn_pool_size_2 is None else int(cnn_pool_size_2)\n",
    "#         self.rnn_type = 'LSTM' if rnn_type is None else rnn_type\n",
    "#         self.n_rnn_stack = 1 if n_rnn_stack is None else int(n_rnn_stack)\n",
    "#         self.rnn_len = 32 if rnn_len is None else int(rnn_len)\n",
    "\n",
    "#     def to_string(self):\n",
    "#         print('Hyperopt Settings:\\n    seq len: {}| lr: {:.6f}| dropout: {:.4f}| batch_size: {}| logits_scaling: {}| scaling_slope: {}'\\\n",
    "#               .format(self.seq_len, self.learning_rate, self.dropout, self.batch_size, self.logits_scaling, self.scaling_slope))\n",
    "        \n",
    "#         if self.nn_type == 'cnn_1layer' or self.nn_type == 'cnn_2layer' or self.nn_type == 'cnn-rnn':\n",
    "#             print('    cnn_n_filters: {}| cnn_kernel_size: {}| cnn_pool_size: {}'\\\n",
    "#                   .format(self.cnn_n_filters, self.cnn_kernel_size, self.cnn_pool_size))\n",
    "            \n",
    "#         if self.nn_type == 'cnn_2layer':\n",
    "#             print('    cnn_n_filters_2: {}| cnn_kernel_size_2: {}| cnn_pool_size_2: {}'\\\n",
    "#                   .format(self.cnn_n_filters_2, self.cnn_kernel_size_2, self.cnn_pool_size_2))\n",
    "#         elif self.nn_type == 'rnn' or self.nn_type == 'cnn-rnn':\n",
    "#             print('    rnn_type: {}| rnn_layers: {}| rnn_len: {}'\\\n",
    "#                   .format(self.rnn_type, self.n_rnn_stack, self.rnn_len))\n",
    "\n",
    "# def optimizer(args):    \n",
    "#     global IS_PARAM_MAXED, HYPEROPT_EARLY_EXIT, OPTIMIZATION_TARGET, PREVIOUS_BEST_ACCURACY\n",
    "#     global x_train, y_train, data_len_train, data_rate_train\n",
    "#     global x_valid, y_valid, data_len_valid, data_rate_valid\n",
    "\n",
    "#     global file_size_list, seq_len_list, dropout_list, learning_rate_list\n",
    "#     global cnn_n_filters_list, cnn_kernel_size_list, cnn_pool_size_list\n",
    "#     global cnn_n_filters_2_list, cnn_kernel_size_2_list, cnn_pool_size_2_list\n",
    "#     global accuracy_list\n",
    "\n",
    "#     hyper = HyperParams(**args)\n",
    "#     if IS_PARAM_MAXED == False:\n",
    "#         hyper.to_string()\n",
    "\n",
    "#     if IS_PARAM_MAXED == True:\n",
    "#         loss, accuracy, epochs = 128.0, 0.0, 0\n",
    "#     else:\n",
    "#         nn_v19 = nn_classification_model_v19(hyper.batch_size, hyper.seq_len, CLASSIFICATION_TYPE, N_CLASSIFICATIONS,\\\n",
    "#                                         hyper.dropout, hyper.learning_rate, hyper.logits_scaling,\\\n",
    "#                                         hyper.scaling_slope, is_training, hyper.nn_type,\\\n",
    "#                                         hyper.cnn_n_filters, hyper.cnn_kernel_size, hyper.cnn_pool_size,\\\n",
    "#                                         hyper.cnn_n_filters_2, hyper.cnn_kernel_size_2, hyper.cnn_pool_size_2,\\\n",
    "#                                         hyper.rnn_type, hyper.n_rnn_stack, hyper.rnn_len)\n",
    "#         if hyper.nn_type == 'cnn_1layer':\n",
    "#             nn_v19.build_cnn_1layer_model_v00()\n",
    "#         elif hyper.nn_type == 'cnn_2layer':\n",
    "#             nn_v19.build_cnn_2layer_model_v00()\n",
    "#         elif hyper.nn_type == 'rnn':\n",
    "#             nn_v19.build_rnn_model_v00()\n",
    "#         elif hyper.nn_type == 'cnn-rnn':\n",
    "#             nn_v19.build_cnn_to_rnn_model_v00()\n",
    "#         else:\n",
    "#            raise Exception('[{}] is not a valid network'.format(nn_type))\n",
    "\n",
    "#         optimization_parameter = 'loss'\n",
    "\n",
    "#         # Used for plotting features vs ACC/Deployed File Size\n",
    "#         seq_len_list.append(hyper.seq_len)\n",
    "#         dropout_list.append(hyper.dropout)\n",
    "#         learning_rate_list.append(hyper.learning_rate)\n",
    "\n",
    "#         if hyper.nn_type == 'cnn_1layer' or hyper.nn_type == 'cnn_2layer' or hyper.nn_type == 'cnn-rnn':\n",
    "#             cnn_n_filters_list.append(hyper.cnn_n_filters)                                       \n",
    "#             cnn_kernel_size_list.append(hyper.cnn_kernel_size)\n",
    "#             cnn_pool_size_list.append(hyper.cnn_pool_size)\n",
    "\n",
    "#         if hyper.nn_type == 'cnn_2layer':\n",
    "#             cnn_n_filters_2_list.append(hyper.cnn_n_filters_2)\n",
    "#             cnn_kernel_size_2_list.append(hyper.cnn_kernel_size_2)\n",
    "#             cnn_pool_size_2_list.append(hyper.cnn_pool_size_2)\n",
    "#         elif hyper.nn_type == 'rnn'  or hyper.nn_type == 'cnn-rnn':\n",
    "#             rnn_type_list.append(hyper.rnn_type)\n",
    "#             n_rnn_stack_list.append(hyper.n_rnn_stack)\n",
    "#             rnn_len_list.append(hyper.rnn_len)\n",
    "        \n",
    "#         batch_size_limit = int(np.size(y_train, 0))\n",
    "#         print('Training Block Batch Count: {}'.format(batch_size_limit))\n",
    "#         loss, accuracy, epochs, file_bytes  = nn_v19.train_v00(N_DATASET_BLOCKS, OPTIMIZATION_TARGET, NO_IMPROVEMENT_LIMIT,\\\n",
    "#                                                     x_train[0:batch_size_limit], y_train[0:batch_size_limit], data_len_train[0:batch_size_limit], data_rate_train[0:batch_size_limit],\\\n",
    "#                                                     x_valid, y_valid, data_len_valid, data_rate_valid)\n",
    "#         file_size_list.append(file_bytes)\n",
    "#         accuracy_list.append(accuracy)\n",
    "\n",
    "#     if HYPEROPT_EARLY_EXIT == True and IS_PARAM_MAXED == False:\n",
    "#         if PREVIOUS_BEST_ACCURACY < accuracy:\n",
    "#             PREVIOUS_BEST_ACCURACY = accuracy\n",
    "#     if accuracy >= 1.0: # if loss <= 0.001:\n",
    "#         IS_PARAM_MAXED = True\n",
    "\n",
    "#     if OPTIMIZATION_TARGET == 'loss':\n",
    "#         return_struct = {\n",
    "#                         'status': STATUS_OK,\n",
    "#                         'loss': loss,\n",
    "#                         'epochs': epochs,\n",
    "#                         'metrics': {\n",
    "#                             'accuracy': accuracy\n",
    "#                             }\n",
    "#                         }\n",
    "#     elif OPTIMIZATION_TARGET == 'accuracy':\n",
    "#         return_struct = {\n",
    "#                         'status': STATUS_OK,\n",
    "#                         'loss': 1-accuracy,\n",
    "#                         'epochs': epochs,\n",
    "#                         'metrics': {\n",
    "#                             'accuracy': accuracy\n",
    "#                             }\n",
    "#                         }\n",
    "#     return return_struct\n",
    "# '''\n",
    "# ------------------------------------------------------------\n",
    "# Hyperparameters to sweep\n",
    "# '''\n",
    "# if NN_TYPE == 'cnn_1layer':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "#         'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 5, 1),\n",
    "#         'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "#         'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1)\n",
    "#         }\n",
    "# elif NN_TYPE == 'cnn_2layer':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "#         'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 5, 1),\n",
    "#         'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "#         'cnn_pool_size': hp.quniform('cnn_pool_size_2', 1, 10, 1),\n",
    "#         'cnn_n_filters_2': hp.quniform('cnn_n_filters_2', 1, 20, 1),\n",
    "#         'cnn_kernel_size_2': hp.quniform('cnn_kernel_size_2', 1, 10, 1),\n",
    "#         'cnn_pool_size_2': hp.quniform('cnn_pool_size_2', 1, 10, 1)\n",
    "#         }\n",
    "# elif NN_TYPE == 'rnn':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 32, 2500, 2),\n",
    "# #         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "#         'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "#         'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "#         }\n",
    "# elif NN_TYPE == 'cnn-rnn':\n",
    "#     space = {\n",
    "#         'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.05)),\n",
    "#         'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "#         'batch_size': hp.quniform('batch_size', 32, 64, 2),\n",
    "#         'seq_len': hp.quniform('seq_len', 128, 2500, 2),\n",
    "#         'cnn_n_filters': hp.quniform('cnn_n_filters', 1, 20, 1),\n",
    "#         'cnn_kernel_size': hp.quniform('cnn_kernel_size', 1, 10, 1),\n",
    "#         'cnn_pool_size': hp.quniform('cnn_pool_size', 1, 10, 1),\n",
    "#         #         'rnn_type': hp.choice('rnn_type', rnn_type_list),\n",
    "#         'n_rnn_stack': hp.quniform('n_rnn_stack', 1, 10, 1),\n",
    "#         'rnn_len': hp.quniform('rnn_len', 16, 128, 2)\n",
    "#         }\n",
    "\n",
    "# '''\n",
    "# ------------------------------------------------------------\n",
    "# Hyperparameter Sweep\n",
    "# '''\n",
    "\n",
    "# for CNN_N_FILTERS in list(reversed(range(20))):\n",
    "#     divisor = 16\n",
    "#     SEQUENCE_LENGTH = n_adc_samples_max_0\n",
    "#     while divisor >= 1:\n",
    "#         PREVIOUS_BEST_ACCURACY = 1.0\n",
    "#         HYPEROPT_EARLY_EXIT = True\n",
    "#         while PREVIOUS_BEST_ACCURACY >= 0.9:\n",
    "#             PREVIOUS_BEST_ACCURACY = 0.0\n",
    "#             SEQUENCE_LENGTH = int(SEQUENCE_LENGTH/divisor - 1)\n",
    "#             IS_PARAM_MAXED = False\n",
    "#             t = Trials()\n",
    "#             best = fmin(optimizer, space, algo=tpe.suggest, max_evals=N_MODEL_HP_VARIATIONS_TO_TEST, trials=t)\n",
    "#             print('TPE best: {}'.format(best))\n",
    "#             print('Seq Len: {}| Best Accuracy: {}| Divisor: {}'.format(SEQUENCE_LENGTH, PREVIOUS_BEST_ACCURACY, divisor))\n",
    "#             print('------------------------------------------------')\n",
    "#             if PREVIOUS_BEST_ACCURACY < 0.9:\n",
    "#                 SEQUENCE_LENGTH = SEQUENCE_LENGTH*divisor\n",
    "#                 divisor = divisor/2\n",
    "\n",
    "# for trial in t.trials:\n",
    "#     print('{} --> {}'.format(trial['result'], trial['misc']['vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T20:12:10.115986Z",
     "start_time": "2019-06-28T20:12:10.113247Z"
    }
   },
   "source": [
    "## Plotting Results vs Hyperopt Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T00:57:12.714629Z",
     "start_time": "2019-11-28T00:57:10.710291Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(seq_len_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(seq_len_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(seq_len_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('seq_len')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(dropout_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(dropout_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(dropout_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('dropout_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(np.log10(learning_rate_list), accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(np.log10(learning_rate_list), np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(np.log10(learning_rate_list), np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('np.log10(learning_rate_list)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_n_filters_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_n_filters_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_n_filters_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_n_filters_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_kernel_size_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_kernel_size_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_kernel_size_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_kernel_size_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_pool_size_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_pool_size_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_pool_size_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_pool_size_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_n_filters_2_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_n_filters_2_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_n_filters_2_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_n_filters_2_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_kernel_size_2_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_kernel_size_2_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_kernel_size_2_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_kernel_size_2_list')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.scatter(cnn_pool_size_2_list, accuracy_list)    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(3,1,2)\n",
    "plt.scatter(cnn_pool_size_2_list, np.asarray(file_size_list)/1024.0) \n",
    "plt.ylabel('File Size (KB)')\n",
    "plt.subplot(3,1,3)\n",
    "plt.scatter(cnn_pool_size_2_list, np.log10(np.asarray(accuracy_list)/np.asarray(np.asarray(file_size_list)))) \n",
    "plt.ylabel('Acc/file_size (%/B)')\n",
    "plt.xlabel('cnn_pool_size_2_list')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.155543Z",
     "start_time": "2019-11-05T19:09:46.857Z"
    }
   },
   "outputs": [],
   "source": [
    "# # cnn_valid_5054935-11529 >> CNN| Seq Len: 1225| LR: 0.002223| DO: 0.0711| b_size: 126| logits_scaling: none| scaling_slope: 1.0| cnn_n_filters: 19| cnn_kernel_size: 7| cnn_pool_size: 4\n",
    "\n",
    "# logits_scaling = 'none' # logits_scaling_list['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# scaling_slope = 1.0 # scaling_slope_list[1, 2, 3, 5, 7, 11]\n",
    "# is_training = True\n",
    "\n",
    "# cnn_n_filters = 9\n",
    "# cnn_kernel_size = 5\n",
    "# cnn_pool_size = 2\n",
    "\n",
    "# cnn_n_filters_2 = 5\n",
    "# cnn_kernel_size_2 = 7\n",
    "# cnn_pool_size_2 = 2\n",
    "\n",
    "# checkpoint = \"/\" + \"cnn_valid_5083579-40525\" # 'none' cnn_valid_5083763-63219\n",
    "# # checkpoint = \"/\" + \"\" # 'decreasing exp'\n",
    "# # checkpoint = \"/\" + \"cnn_valid_3690586-454\" # 'decreasing linear'\n",
    "# meta_to_load = CHECKPOINTS_DIR + checkpoint + \".meta\"\n",
    "# checkpoint_to_load = CHECKPOINTS_DIR + checkpoint\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# optimization_parameter = 'accuracy'\n",
    "# cnn_v19_test = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, CLASSIFICATION_TYPE, N_CLASSIFICATIONS, DROPOUT,\\\n",
    "#                                       LEARNING_RATE, logits_scaling, scaling_slope,\\\n",
    "#                                       is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size,\\\n",
    "#                                       cnn_n_filters_2, cnn_kernel_size_2, cnn_pool_size_2)\n",
    "\n",
    "# saver = tf.train.import_meta_graph(meta_to_load)\n",
    "# with tf.Session() as sess:\n",
    "# #     saver.restore(sess,tf.train.latest_checkpoint(CHECKPOINTS_DIR))\n",
    "#     saver.restore(sess, checkpoint_to_load)        \n",
    "#     print('Restored from: {}'.format(checkpoint_to_load))\n",
    "#     print('Input Sequence Length: {}'.format(SEQUENCE_LENGTH))\n",
    "#     threshold=0.8\n",
    "#     accuracy_list, snr_list = cnn_v19_test.sample_v11(sess, N_DATASET_BLOCKS, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing: Determining Minimum SEQUENCE_LENGTH for 100% Accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.156226Z",
     "start_time": "2019-11-05T19:09:47.095Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # TPE best: {'BATCH_SIZE': 30.0, 'DROPOUT': 0.733620479775405, 'LEARNING_RATE': 0.004139365777138103, 'n_layers': 3.0, 'cnn_len': 78.0}\n",
    "# 6e-5 = cnn_type: LSTM| n_layers: 1| cnn_len: 32| lr: 0.001522| dropout: 0.0305| batch_size: 64| logits_scaling: decreasing exp| scaling_slope: 5.0| cnn_n_filters: 6| cnn_kernel_size: 7| cnn_pool_size: 5\n",
    "# 3e-5 = cnn_type: LSTM| n_layers: 2| cnn_len: 168| lr: 0.002650| dropout: 0.0763| batch_size: 44| logits_scaling: decreasing exp| scaling_slope: 1.0| cnn_n_filters: 5| cnn_kernel_size: 6| cnn_pool_size: 3\n",
    "# cnn_type: LSTM| n_layers: 2| cnn_len: 32| lr: 0.000754| dropout: 0.0809| batch_size: 70| logits_scaling: none| scaling_slope: 3.0| cnn_n_filters: 4| cnn_kernel_size: 8| cnn_pool_size: 3\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# SEQUENCE_LENGTH = n_adc_samples # = np.size(x_train_unshuffled, 1)\n",
    "# NO_IMPROVEMENT_LIMIT = 3 # N_EPOCHS without improvement\n",
    "# DROPOUT = 0.2 # drop_out_list = [0.0, 0.25] #[0.0, 0.1, 0.2, 0.5]\n",
    "# LEARNING_RATE = 0.0015 # learning_rate_list = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# cnn_type = 'LSTM' # cnn_type_list = ['LSTM', 'GRU', 'cnn']\n",
    "# n_layers = 1 # n_layers_list = [1,3,5,7]\n",
    "# cnn_len = 32 # cnn_len_list = [128, 256, 512, 1024]\n",
    "# logits_scaling = 'none' # logits_scaling_list['none', 'increasing linear', 'decreasing linear', 'increasing exp', 'decreasing exp']\n",
    "# scaling_slope = 5.0 # scaling_slope_list[1, 2, 3, 5, 7, 11]\n",
    "# is_training = True\n",
    "\n",
    "# cnn_n_filters = 6\n",
    "# cnn_kernel_size = 7\n",
    "# cnn_pool_size = 5\n",
    "\n",
    "# optimization_parameter = 'loss'\n",
    "\n",
    "# cnn_v19 = cnn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, N_PACKET_TYPES, cnn_len, n_layers, DROPOUT, LEARNING_RATE, cnn_type, logits_scaling, scaling_slope, is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size)\n",
    "\n",
    "# sequence_length_list = []\n",
    "# accuracy_list = []\n",
    "# threshold = 0.9\n",
    "# while SEQUENCE_LENGTH > 7: # Breaks at SEQUENCE_LENGTH == 7 for some reason\n",
    "#     print('Input Sequence Length: {}'.format(SEQUENCE_LENGTH))\n",
    "#     cnn_v19 = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, N_PACKET_TYPES, cnn_len, n_layers, DROPOUT, LEARNING_RATE, cnn_type, logits_scaling, scaling_slope, is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size)\n",
    "#     loss, accuracy, epochs  = cnn_v19.train(x_train_unshuffled[:,0:SEQUENCE_LENGTH], y_train_unshuffled, x_valid_unshuffled[:,0:SEQUENCE_LENGTH], y_valid_unshuffled, optimization_parameter, NO_IMPROVEMENT_LIMIT)\n",
    "#     sequence_length_list.append(SEQUENCE_LENGTH)\n",
    "#     accuracy_list.append(accuracy)\n",
    "#     SEQUENCE_LENGTH = SEQUENCE_LENGTH - 1\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(sequence_length_list, accuracy_list, linewidth=4)                             \n",
    "# plt.title('Minimizing Input Sequence\\nNecessary for Training')\n",
    "# plt.xlabel('Trained Sequence Length')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.xlim([76, 86])\n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tested Network Node and Parameter Count</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.156947Z",
     "start_time": "2019-11-05T19:09:47.266Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint = \"/\" + \"cnn_valid_4155106-2768\" # 'decreasing exp'\n",
    "# meta_to_load = CHECKPOINTS_DIR + checkpoint + \".meta\"\n",
    "# checkpoint_to_load = CHECKPOINTS_DIR + checkpoint\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# cnn_v19_test = nn_classification_model_v19(BATCH_SIZE, SEQUENCE_LENGTH, N_PACKET_TYPES, DROPOUT, LEARNING_RATE, logits_scaling, scaling_slope, is_training, cnn_n_filters, cnn_kernel_size, cnn_pool_size)\n",
    "# # graph = tf.get_default_graph()\n",
    "# # with graph.as_default():\n",
    "# saver = tf.train.import_meta_graph(meta_to_load)\n",
    "# with tf.Session() as sess:\n",
    "# #     saver.restore(sess,tf.train.latest_checkpoint(CHECKPOINTS_DIR))\n",
    "#     saver.restore(sess, checkpoint_to_load)        \n",
    "#     print('Restored from: {}'.format(checkpoint_to_load))\n",
    "# #     threshold=0.8\n",
    "# #     cnn_v19_test.sample_v02(sess, x_test_unshuffled, y_test_unshuffled, threshold, logits_scaling)\n",
    "# #     print('Finished')\n",
    "    \n",
    "# print('\\n# Nodes in Checkpoint: {}\\n'.format(np.size(tf.get_default_graph().as_graph_def().node)))\n",
    "\n",
    "# # Open TensorFlow ckpt\n",
    "# reader = tf.train.NewCheckpointReader(checkpoint_to_load)\n",
    "\n",
    "# param_map = reader.get_variable_to_shape_map()\n",
    "# total_count = 0\n",
    "# for k, v in param_map.items():\n",
    "#     if 'Momentum' not in k and 'global_step' not in k:\n",
    "#         temp = np.prod(v)\n",
    "#         total_count += temp\n",
    "#         print('%s: %s => %d' % (k, str(v), temp))\n",
    "\n",
    "# print('\\nTotal Param Count in Checkpoint: %d' % total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Freezing Graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.157596Z",
     "start_time": "2019-11-05T19:09:47.605Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_loaded_graph(model_save_file=MODEL_FINAL_SAVE_PATH, keep_var_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "\n",
    "    K.set_learning_phase(0)#tf.keras.backend.set_learning_phase(0) # 0 testing, 1 training mode        \n",
    "    sess = K.get_session()   \n",
    "    \n",
    "    from keras.models import load_model\n",
    "    loaded_model = load_model(model_save_file) #tf.keras.models.load_model(MODEL_FINAL_SAVE_PATH)\n",
    "    output_names=[out.op.name for out in loaded_model.outputs]\n",
    "    input_names=[inputs_0.op.name for inputs_0 in loaded_model.inputs]\n",
    "\n",
    "    graph = sess.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        print(\"Loaded Input Node: {}\".format([inputs_0.op.name for inputs_0 in loaded_model.inputs]))\n",
    "        print(\"Loaded Output Node: {}\".format([outputs_0.op.name for outputs_0 in loaded_model.outputs]))\n",
    "#         print(\"Loaded Freeze Vals: {}\".format(freeze_var_names))\n",
    "        print('\\n---------------------------------------------------------------\\n')\n",
    "    \n",
    "        from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "        frozen_graph = convert_variables_to_constants(sess, input_graph_def, \\\n",
    "                                                      output_names, freeze_var_names)\n",
    "    return frozen_graph\n",
    "\n",
    "frozen_graph = freeze_loaded_graph(MODEL_FINAL_SAVE_PATH) # Save the frozen graph\n",
    "\n",
    "frozen_graph_dir = CHECKPOINTS_DIR \n",
    "frozen_graph_filename = 'saved_model.pb'\n",
    "tf.train.write_graph(frozen_graph, frozen_graph_dir, frozen_graph_filename, as_text=False )\n",
    "\n",
    "# Get frozen graph file size\n",
    "frozen_graph_filepath = frozen_graph_dir + '/' + frozen_graph_filename\n",
    "file_bytes = int(os.path.getsize(frozen_graph_filepath))\n",
    "file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "K.clear_session()\n",
    "\n",
    "print('File Size: {:.3f}{}'.format(file_size, units))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Frozen Graph and Counting the Nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T19:09:48.158297Z",
     "start_time": "2019-11-05T19:09:47.774Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def load_frozen_graph(graph_dir):\n",
    "    with tf.gfile.FastGFile(graph_dir, \"rb\") as file:\n",
    "        print(\"Graph restored from:\\n{}\".format(graph_dir))\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(file.read())            \n",
    "        print(\"\\n# of Nodes in GraphDef: {}\".format(np.size(graph_def.node)))\n",
    "\n",
    "frozen_graph_dir = CHECKPOINTS_DIR \n",
    "frozen_graph_filename = 'saved_model.pb'\n",
    "frozen_graph_filepath = frozen_graph_dir + '/' + frozen_graph_filename\n",
    "load_frozen_graph(frozen_graph_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TFLite Conversion:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T03:14:29.160439Z",
     "start_time": "2019-12-11T03:14:26.743981Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(MODEL_FINAL_SAVE_PATH)\n",
    "\n",
    "# TFLite Version\n",
    "tflite_model = converter.convert()    \n",
    "with open(MODEL_TFLITE_PATH, 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "file_bytes = int(os.path.getsize(MODEL_TFLITE_PATH))\n",
    "file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "print('\\nTFLite File Size: {:.3f}{}'.format(file_size, units)) \n",
    "\n",
    "# Quantized TFLite Version\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "with open(MODEL_TFLITE_QUANT_PATH, 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n",
    "\n",
    "file_bytes = int(os.path.getsize(MODEL_TFLITE_QUANT_PATH))\n",
    "file_size, units = data_utils.convert_bytes(file_bytes)\n",
    "print('\\nTFLite Quantized File Size: {:.3f}{}'.format(file_size, units))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "TF_K_SKL_P_GPU_191124",
   "language": "python",
   "name": "tf_k_skl_p_gpu_191124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 538,
   "position": {
    "height": "560px",
    "left": "545px",
    "right": "20px",
    "top": "159.984px",
    "width": "660px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
